--- original/./test_simple_fix.py
+++ fixed/./test_simple_fix.py
@@ -62,7 +62,8 @@
 
         # Check if ROOT_PATH exists
         if hasattr(optimizer, "ROOT_PATH"):
-            print(f"✅ SUCCESS: ROOT_PATH attribute exists: {optimizer.ROOT_PATH}")
+            print(
+                f"✅ SUCCESS: ROOT_PATH attribute exists: {optimizer.ROOT_PATH}")
             return True
         else:
             print("❌ FAILED: ROOT_PATH attribute not found")
--- original/./etl/pipeline.py
+++ fixed/./etl/pipeline.py
@@ -162,8 +162,10 @@
 
         # ---------- 0. PRE-PIPELINE CLEANUP -------------------------------
         # Clean downloads and staging folders for fresh data
-        cleanup_downloads = self.global_cfg.get("cleanup_downloads_before_run", True)
-        cleanup_staging = self.global_cfg.get("cleanup_staging_before_run", True)
+        cleanup_downloads = self.global_cfg.get(
+            "cleanup_downloads_before_run", True)
+        cleanup_staging = self.global_cfg.get(
+            "cleanup_staging_before_run", True)
 
         if cleanup_downloads or cleanup_staging:
             lg_sum.info("🧹 Starting pre-pipeline cleanup...")
@@ -171,7 +173,8 @@
 
         # ---------- 1. DOWNLOAD & STAGING ---------------------------------
         sources = list(Source.load_all(self.sources_yaml_path))
-        self.logger.info("📋 Found sources to process", source_count=len(sources))
+        self.logger.info("📋 Found sources to process",
+                         source_count=len(sources))
 
         # Create SDE loader for proper source-to-dataset mapping
         from .models import AppConfig, SdeLoader
@@ -232,7 +235,8 @@
                             "source": src.name,
                             "type": src.type,
                             "concurrent": str(
-                                self.global_cfg.get("enable_concurrent_downloads", True)
+                                self.global_cfg.get(
+                                    "enable_concurrent_downloads", True)
                             ),
                         },
                     )
@@ -302,7 +306,8 @@
                 reset_gdb(paths.GDB)
             self.logger.info("✅ Staging GDB reset complete")
         except (ImportError, arcpy.ExecuteError, OSError) as reset_exc:
-            self.logger.warning("⚠️ Failed to reset staging GDB", error=reset_exc)
+            self.logger.warning(
+                "⚠️ Failed to reset staging GDB", error=reset_exc)
             if not self.global_cfg.get("continue_on_failure", True):
                 self.monitor.end_run("failed")
                 raise
@@ -425,9 +430,11 @@
             )
         )
         if not aoi_boundary.exists():
-            self.logger.error("❌ AOI boundary not found", aoi_path=str(aoi_boundary))
+            self.logger.error("❌ AOI boundary not found",
+                              aoi_path=str(aoi_boundary))
             if not self.global_cfg.get("continue_on_failure", True):
-                raise FileNotFoundError(f"AOI boundary not found: {aoi_boundary}")
+                raise FileNotFoundError(
+                    f"AOI boundary not found: {aoi_boundary}")
             return
 
         try:
@@ -443,7 +450,8 @@
                 staging_gdb=paths.GDB,
                 aoi_fc=aoi_boundary,
                 target_srid=geoprocessing_config.get("target_srid", 3006),
-                pp_factor=geoprocessing_config.get("parallel_processing_factor", "100"),
+                pp_factor=geoprocessing_config.get(
+                    "parallel_processing_factor", "100"),
             )
 
             geoprocessing_duration = time.time() - start_time
@@ -467,7 +475,8 @@
         """🚚 Step 4: Load processed GDB to production SDE with parallel processing"""
 
         if not source_gdb.exists():
-            self.logger.error("❌ Source GDB not found", gdb_path=str(source_gdb))
+            self.logger.error("❌ Source GDB not found",
+                              gdb_path=str(source_gdb))
             return
 
         # Get SDE connection from config and validate
@@ -488,7 +497,8 @@
 
         all_feature_classes = self._discover_feature_classes(source_gdb)
         if not all_feature_classes:
-            self.logger.warning("⚠️ No feature classes found", gdb_path=str(source_gdb))
+            self.logger.warning("⚠️ No feature classes found",
+                                gdb_path=str(source_gdb))
             return
 
         self.logger.info(
@@ -511,7 +521,8 @@
 
     def _validate_sde_connection_file(self, path: Path) -> bool:
         if not path.exists():
-            self.logger.error("❌ SDE connection file not found", sde_path=str(path))
+            self.logger.error(
+                "❌ SDE connection file not found", sde_path=str(path))
             return False
         return True
 
@@ -529,7 +540,8 @@
                     all_fcs.append((fc_full_path, fc))
             datasets = arcpy.ListDatasets(feature_type="Feature")
             if datasets:
-                self.logger.debug("📁 Found feature datasets", count=len(datasets))
+                self.logger.debug("📁 Found feature datasets",
+                                  count=len(datasets))
                 for ds in datasets:
                     ds_fcs = arcpy.ListFeatureClasses(feature_dataset=ds)
                     if ds_fcs:
@@ -562,7 +574,8 @@
         )
 
         # Get load strategy from config (default: truncate_and_load)
-        load_strategy = self.global_cfg.get("sde_load_strategy", "truncate_and_load")
+        load_strategy = self.global_cfg.get(
+            "sde_load_strategy", "truncate_and_load")
 
         try:
             # Check if target dataset exists in SDE
@@ -655,13 +668,16 @@
 
         if arcpy.Exists(target_path):
             if load_strategy == "truncate_and_load":
-                lg_sum.info("🗑️ Truncating existing FC: %s\\%s", dataset, sde_fc_name)
+                lg_sum.info("🗑️ Truncating existing FC: %s\\%s",
+                            dataset, sde_fc_name)
                 arcpy.management.TruncateTable(target_path)
-                lg_sum.info("📄 Loading fresh data to: %s\\%s", dataset, sde_fc_name)
+                lg_sum.info("📄 Loading fresh data to: %s\\%s",
+                            dataset, sde_fc_name)
                 arcpy.management.Append(
                     inputs=source_fc_path, target=target_path, schema_type="NO_TEST"
                 )
-                lg_sum.info("🚚→  %s\\%s (truncated + loaded)", dataset, sde_fc_name)
+                lg_sum.info("🚚→  %s\\%s (truncated + loaded)",
+                            dataset, sde_fc_name)
             elif load_strategy == "replace":
                 self.logger.info(
                     "🗑️ Deleting existing FC", dataset=dataset, fc=sde_fc_name
@@ -680,7 +696,8 @@
                 )
 
                 duration = time.time() - start_time
-                self.metrics.record_timing("sde.replace.duration_ms", duration * 1000)
+                self.metrics.record_timing(
+                    "sde.replace.duration_ms", duration * 1000)
                 self.logger.info(
                     "🚚→ Replaced",
                     dataset=dataset,
@@ -716,7 +733,8 @@
             )
 
             duration = time.time() - start_time
-            self.metrics.record_timing("sde.create.duration_ms", duration * 1000)
+            self.metrics.record_timing(
+                "sde.create.duration_ms", duration * 1000)
             self.logger.info(
                 "🚚→ Created",
                 dataset=dataset,
--- original/./etl/utils/regression_detector.py
+++ fixed/./etl/utils/regression_detector.py
@@ -384,7 +384,8 @@
             "total_operations_monitored": len(self.performance_data),
             "baselines_established": len(self.baselines),
             "regressions_by_severity": {
-                severity: len([r for r in recent_regressions if r.severity == severity])
+                severity: len(
+                    [r for r in recent_regressions if r.severity == severity])
                 for severity in ["minor", "moderate", "major", "critical"]
             },
             "operation_analysis": operation_analysis,
--- original/./etl/utils/http_session.py
+++ fixed/./etl/utils/http_session.py
@@ -106,7 +106,8 @@
                     session.close()
                     log.debug("Closed HTTP session for: %s", session_key)
                 except Exception as e:
-                    log.warning("Failed to close session %s: %s", session_key, e)
+                    log.warning("Failed to close session %s: %s",
+                                session_key, e)
             self._sessions.clear()
 
     def __del__(self):
@@ -173,7 +174,8 @@
     def session(self) -> requests.Session:
         """Get the HTTP session for this handler."""
         if self._session is None:
-            self._session = get_http_session(self.base_url, **self.session_config)
+            self._session = get_http_session(
+                self.base_url, **self.session_config)
         return self._session
 
     def close_session(self):
--- original/./etl/utils/performance_monitor.py
+++ fixed/./etl/utils/performance_monitor.py
@@ -239,7 +239,8 @@
     def remove_alert_rule(self, rule_name: str) -> bool:
         """Remove alert rule by name."""
         original_count = len(self.alert_rules)
-        self.alert_rules = [rule for rule in self.alert_rules if rule.name != rule_name]
+        self.alert_rules = [
+            rule for rule in self.alert_rules if rule.name != rule_name]
 
         if len(self.alert_rules) < original_count:
             log.info("Removed alert rule: %s", rule_name)
@@ -261,7 +262,8 @@
         # Summary for all operations
         summary = {}
         for op_name, history in self.performance_history.items():
-            summary[op_name] = self._summarize_operation_metrics(op_name, list(history))
+            summary[op_name] = self._summarize_operation_metrics(
+                op_name, list(history))
 
         return summary
 
@@ -306,7 +308,8 @@
         # Collect operation metrics
         operations = {}
         for op_name, history_deque in self.performance_history.items():
-            history_list = list(history_deque)  # Convert deque to list explicitly
+            # Convert deque to list explicitly
+            history_list = list(history_deque)
             relevant_metrics = [
                 m for m in history if start_time <= m.start_time <= end_time
             ]
@@ -402,7 +405,8 @@
     def _get_system_resources(self) -> SystemResources:
         """Get current system resources."""
         memory = psutil.virtual_memory()
-        disk = psutil.disk_usage(getattr(self, "_ROOT_PATH", Path.cwd().anchor))
+        disk = psutil.disk_usage(
+            getattr(self, "_ROOT_PATH", Path.cwd().anchor))
 
         return SystemResources(
             cpu_percent=psutil.cpu_percent(interval=0.1),
@@ -624,7 +628,8 @@
                 history.popleft()
 
         # Clean up alerts
-        self.alerts = [alert for alert in self.alerts if alert.timestamp > cutoff_time]
+        self.alerts = [
+            alert for alert in self.alerts if alert.timestamp > cutoff_time]
 
         # Clean up active alerts that are resolved
         resolved_alerts = [
--- original/./etl/utils/concurrent.py
+++ fixed/./etl/utils/concurrent.py
@@ -147,7 +147,8 @@
                         self.stats.update(result)
 
                     if fail_fast and not result.success:
-                        log.warning("Fail-fast enabled, cancelling remaining tasks")
+                        log.warning(
+                            "Fail-fast enabled, cancelling remaining tasks")
                         # Cancel remaining futures
                         for remaining_future in future_to_task:
                             if not remaining_future.done():
@@ -201,7 +202,8 @@
 
         except Exception as e:
             duration = time.time() - start_time
-            log.debug("Task '%s' failed after %.2fs: %s", task_name, duration, e)
+            log.debug("Task '%s' failed after %.2fs: %s",
+                      task_name, duration, e)
 
             return ConcurrentResult(
                 success=False,
@@ -290,7 +292,8 @@
             task = (handler._fetch_collection, (collection,), {})
             tasks.append(task)
 
-        log.info("Starting concurrent download of %d collections", len(collections))
+        log.info("Starting concurrent download of %d collections",
+                 len(collections))
         return self.manager.execute_concurrent(tasks, task_names, fail_fast)
 
 
--- original/./etl/utils/adaptive_tuning.py
+++ fixed/./etl/utils/adaptive_tuning.py
@@ -43,8 +43,8 @@
             current_metrics: PerformanceMetrics,
             threshold: float = 0.2) -> bool:
         """Check if current performance is degraded compared to baseline."""
-        duration_increase = (current_metrics.duration - \
-                             self.avg_duration) / self.avg_duration # type: ignore
+        duration_increase = (current_metrics.duration -
+                             self.avg_duration) / self.avg_duration  # type: ignore
         throughput_decrease = (
             self.avg_throughput - current_metrics.throughput_items_per_sec) / self.avg_throughput
 
--- original/./etl/handlers/file.py
+++ fixed/./etl/handlers/file.py
@@ -133,7 +133,8 @@
         for result in results:
             if not result.success:
                 file_name = result.metadata.get("task_name", "unknown")
-                log.error("❌ File download failed: %s - %s", file_name, result.error)
+                log.error("❌ File download failed: %s - %s",
+                          file_name, result.error)
 
     def _download_single_file_stem(self, included_filename_stem: str) -> None:
         """Download a single file stem (extracted from original loop)."""
@@ -165,7 +166,8 @@
             self.src.url,
         )
 
-        true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(self.src.url)
+        true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(
+            self.src.url)
         consistent_local_stem = sanitize_for_filename(self.src.name)
         final_extension = true_ext_from_web
 
@@ -347,7 +349,8 @@
 
             if explicit_local_file_ext.lower() == ".zip":
                 try:
-                    extract_zip(downloaded_file_path, final_staging_destination_dir)
+                    extract_zip(downloaded_file_path,
+                                final_staging_destination_dir)
                     log.info(
                         "➕ Extracted and staged archive %s to %s",
                         downloaded_file_path.name,
--- original/./etl/handlers/rest_api.py
+++ fixed/./etl/handlers/rest_api.py
@@ -356,7 +356,7 @@
             if "id" in lyr
         }
 
-            if configured_layer_ids_from_yaml:
+           if configured_layer_ids_from_yaml:
                 log.info(
                     "Found explicit layer_ids in config: %s for source '%s'. Processing only these.",
                     configured_layer_ids_from_yaml,
@@ -370,11 +370,12 @@
                 lid_str = str(lid_val)
                 layer_detail = metadata_layers_details.get(lid_str)
 
-                    if layer_detail:
+                   if layer_detail:
                         layer_name = layer_detail.get(
                             "name", f"layer_{lid_str}")
                         layers_to_iterate_final.append(
-                            {"id": lid_str, "name": layer_name, "metadata": layer_detail}
+                            {"id": lid_str, "name": layer_name,
+                                "metadata": layer_detail}
                         )
                     else:
                         log.warning(
@@ -427,7 +428,8 @@
                 fs_layer_name = service_meta.get(
                     "name", f"feature_layer_{fs_layer_id_str}")
                 layers_to_iterate_final.append(
-                    {"id": fs_layer_id_str, "name": fs_layer_name, "metadata": service_meta}
+                    {"id": fs_layer_id_str, "name": fs_layer_name,
+                        "metadata": service_meta}
                 )
 
             if not layers_to_iterate_final:
@@ -454,7 +456,8 @@
                 for layer_info_to_query in layers_to_iterate_final:
                     self._fetch_layer_data(
                         layer_info=layer_info_to_query,
-                        layer_metadata_from_service=layer_info_to_query.get("metadata"),
+                        layer_metadata_from_service=layer_info_to_query.get(
+                            "metadata"),
                         rollback_mgr=rollback_mgr,
                     )
 
@@ -713,7 +716,8 @@
         params = self._prepare_query_params()
 
         source_name_sanitized = sanitize_for_filename(self.src.name)
-        staging_dir = Path(paths.STAGING) / self.src.authority / source_name_sanitized
+        staging_dir = Path(paths.STAGING) / \
+                           self.src.authority / source_name_sanitized
         staging_dir.mkdir(parents=True, exist_ok=True)
 
         output_filename = f"{layer_name_sanitized}.{params['f']}"
