--- original/./run_etl.py
+++ fixed/./run_etl.py
@@ -18,12 +18,13 @@
 
     # 2) determine YAML paths (allow overrides via CLI args)
     default_sources = Path("config/sources.yaml")
-    default_config  = Path("config/config.yaml")
+    default_config = Path("config/config.yaml")
     default_mappings = Path("config/mappings.yaml")
 
     sources_path = Path(sys.argv[1]) if len(sys.argv) > 1 else default_sources
-    config_path  = Path(sys.argv[2]) if len(sys.argv) > 2 else default_config
-    mappings_path = Path(sys.argv[3]) if len(sys.argv) > 3 else (default_mappings if default_mappings.exists() else None)
+    config_path = Path(sys.argv[2]) if len(sys.argv) > 2 else default_config
+    mappings_path = Path(sys.argv[3]) if len(sys.argv) > 3 else (
+        default_mappings if default_mappings.exists() else None)
 
     # 3) run the pipeline
     Pipeline(
--- original/./test_fixes.py
+++ fixed/./test_fixes.py
@@ -12,153 +12,162 @@
 # Add the ETL package to path
 sys.path.insert(0, str(Path(__file__).parent))
 
+
 def test_thread_safety_fix():
     """
     Test the thread safety fix for concurrent downloaders.
-    
+
     The issue was that code was doing:
     downloader = get_file_downloader()
     downloader.manager.max_workers = max_workers  # This mutates the singleton!
-    
+
     The fix is to pass max_workers as a parameter:
     downloader.download_files_concurrent(..., max_workers=max_workers)
     """
     print("üîß Testing thread safety fix for concurrent downloads...")
-    
+
     # Mock objects to simulate the fix without arcpy dependency
     class MockFileHandler:
         def __init__(self, name):
             self.name = name
-            
+
         def _download_single_file_stem(self, file_stem):
             print(f"  üì• Downloading {file_stem} (handler: {self.name})")
             time.sleep(0.1)  # Simulate download time
             return f"result_{file_stem}"
-    
+
     class MockConcurrentResult:
         def __init__(self, success=True, metadata=None):
             self.success = success
             self.metadata = metadata or {}
             self.error = None
-    
+
     class MockFileDownloader:
         def __init__(self):
             self.manager = MockDownloadManager()
-            
+
         def download_files_concurrent(self, handler, file_stems, fail_fast=False, max_workers=None):
             effective_workers = max_workers or self.manager.max_workers
-            print(f"  üöÄ Using {effective_workers} workers (passed as parameter, not mutating singleton)")
-            
+            print(
+                f"  üöÄ Using {effective_workers} workers (passed as parameter, not mutating singleton)")
+
             results = []
             for stem in file_stems:
                 handler._download_single_file_stem(stem)
-                results.append(MockConcurrentResult(metadata={"task_name": f"file_{stem}"}))
+                results.append(MockConcurrentResult(
+                    metadata={"task_name": f"file_{stem}"}))
             return results
-    
+
     class MockDownloadManager:
         def __init__(self):
             self.max_workers = 4  # Default
-    
+
     # Simulate the OLD way (thread unsafe - mutating singleton)
     print("\n‚ùå OLD WAY (thread unsafe):")
     downloader = MockFileDownloader()
     print(f"Original max_workers: {downloader.manager.max_workers}")
-    
+
     # This was the problematic code:
     downloader.manager.max_workers = 8  # MUTATES THE SINGLETON!
     print(f"After mutation: {downloader.manager.max_workers}")
     print("‚ö†Ô∏è  This could cause race conditions in concurrent access!")
-    
+
     # Simulate the NEW way (thread safe - passing parameter)
     print("\n‚úÖ NEW WAY (thread safe):")
     downloader = MockFileDownloader()
     print(f"Singleton max_workers unchanged: {downloader.manager.max_workers}")
-    
+
     # This is the fixed approach:
     handler = MockFileHandler("test_handler")
     file_stems = ["file1", "file2", "file3"]
     max_workers = 8  # Configuration value
-    
+
     results = downloader.download_files_concurrent(
         handler=handler,
         file_stems=file_stems,
         max_workers=max_workers  # PASSED AS PARAMETER, no mutation!
     )
-    
-    print(f"Singleton max_workers still unchanged: {downloader.manager.max_workers}")
+
+    print(
+        f"Singleton max_workers still unchanged: {downloader.manager.max_workers}")
     print(f"Downloaded {len(results)} files successfully")
     print("‚úÖ No singleton mutation, thread-safe!")
-    
+
     return True
+
 
 def test_http_timeout_fix():
     """Test the HTTP session timeout fix."""
     print("\nüîß Testing HTTP session timeout fix...")
-    
+
     class MockSession:
         def __init__(self):
             self._etl_timeout = 30
             self._etl_request_override = False
-            
+
         def request(self, method, url, **kwargs):
             # This simulates the fix where timeout is always passed
             if 'timeout' not in kwargs and hasattr(self, '_etl_timeout'):
                 kwargs['timeout'] = self._etl_timeout
                 print(f"  ‚úÖ Timeout automatically set to {kwargs['timeout']}s")
             else:
-                print(f"  ‚ö†Ô∏è  Timeout already specified: {kwargs.get('timeout')}s")
+                print(
+                    f"  ‚ö†Ô∏è  Timeout already specified: {kwargs.get('timeout')}s")
             return f"Response for {method} {url}"
-    
+
     session = MockSession()
-    
+
     print("‚ùå OLD WAY: session.timeout = 60  # Had no effect!")
-    
+
     print("‚úÖ NEW WAY: Override request method to ensure timeout is passed")
-    
+
     # Test requests without explicit timeout
     print("Request without timeout:")
     session.request("GET", "http://example.com")
-    
+
     print("Request with explicit timeout:")
     session.request("GET", "http://example.com", timeout=120)
-    
+
     return True
+
 
 def test_threading_import_fix():
     """Test the threading import fix."""
     print("\nüîß Testing threading import fix...")
-    
+
     print("‚ùå OLD WAY: import threading at end of file")
     print("   def some_function():")
     print("       self._lock = threading.Lock()  # NameError: threading not defined!")
-    
+
     print("‚úÖ NEW WAY: import threading at top of file")
     print("   import threading  # At top of file")
     print("   ...")
     print("   def some_function():")
     print("       self._lock = threading.Lock()  # Works correctly!")
-    
+
     return True
+
 
 if __name__ == "__main__":
     print("üß™ Testing critical fixes from PR review")
     print("=" * 50)
-    
+
     try:
         test_thread_safety_fix()
-        test_http_timeout_fix() 
+        test_http_timeout_fix()
         test_threading_import_fix()
-        
+
         print("\n" + "=" * 50)
         print("‚úÖ All critical fixes working correctly!")
         print("\nSummary of fixes:")
         print("1. ‚úÖ Thread safety: Pass max_workers as parameter instead of mutating singleton")
-        print("2. ‚úÖ HTTP timeout: Override session.request to ensure timeout is always passed") 
+        print(
+            "2. ‚úÖ HTTP timeout: Override session.request to ensure timeout is always passed")
         print("3. ‚úÖ Threading import: Move import to top of file")
         print("4. ‚úÖ Context manager: FileDownloadHandler already has proper __enter__/__exit__")
-        
+
     except Exception as e:
         print(f"\n‚ùå Test failed: {e}")
         import traceback
         traceback.print_exc()
-        sys.exit(1)
\ No newline at end of file
+        sys.exit(1)
--- original/./etl/__init__.py
+++ fixed/./etl/__init__.py
@@ -11,4 +11,4 @@
     Pipeline(sources_yaml=Path(sources), **kwargs).run()
 
 
-__all__ = ["run", "Pipeline"]
\ No newline at end of file
+__all__ = ["run", "Pipeline"]
--- original/./etl/exceptions.py
+++ fixed/./etl/exceptions.py
@@ -69,7 +69,7 @@
 # The new system provides 8 core exception types with enhanced functionality:
 #
 # 1. NetworkError - HTTP, connection, timeout, rate limit issues
-# 2. DataError - Data format, quality, validation, geospatial issues  
+# 2. DataError - Data format, quality, validation, geospatial issues
 # 3. SystemError - Storage, resources, permissions, disk space
 # 4. ConfigurationError - Configuration and setup problems
 # 5. SourceError - Source availability, authentication, access
@@ -78,4 +78,4 @@
 # 8. ConcurrentError - Concurrent operation failures
 #
 # Legacy exception names are preserved for backwards compatibility
-# All exceptions now have enhanced error context and structured logging support
\ No newline at end of file
+# All exceptions now have enhanced error context and structured logging support
--- original/./etl/config.py
+++ fixed/./etl/config.py
@@ -28,14 +28,16 @@
     format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
     max_file_size_mb: int = 10
     backup_count: int = 5
-    
+
     def __post_init__(self):
         """Validate logging configuration."""
         valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
         if self.level.upper() not in valid_levels:
-            raise ValidationError(f"Invalid log level: {self.level}. Must be one of {valid_levels}")
+            raise ValidationError(
+                f"Invalid log level: {self.level}. Must be one of {valid_levels}")
         if self.console_level.upper() not in valid_levels:
-            raise ValidationError(f"Invalid console log level: {self.console_level}. Must be one of {valid_levels}")
+            raise ValidationError(
+                f"Invalid console log level: {self.console_level}. Must be one of {valid_levels}")
 
 
 @dataclass
@@ -48,7 +50,7 @@
     timeout: int = 30
     circuit_breaker_threshold: int = 5
     circuit_breaker_timeout: float = 60.0
-    
+
     def __post_init__(self):
         """Validate retry configuration."""
         if self.max_attempts < 1:
@@ -69,7 +71,7 @@
     output: str = "output"
     temp: str = "temp"
     logs: str = "logs"
-    
+
     def __post_init__(self):
         """Validate and normalize paths."""
         # Convert relative paths to absolute
@@ -87,7 +89,7 @@
     memory_limit_mb: int = 1024
     enable_caching: bool = True
     cache_ttl_hours: int = 24
-    
+
     def __post_init__(self):
         """Validate processing configuration."""
         if self.chunk_size < 1:
@@ -107,7 +109,7 @@
     attribute_validation: bool = True
     coordinate_system_validation: bool = True
     max_validation_errors: int = 100
-    
+
     def __post_init__(self):
         """Validate validation configuration."""
         if self.max_validation_errors < 1:
@@ -123,14 +125,14 @@
     allowed_file_types: List[str] = field(default_factory=lambda: [
         ".zip", ".gpkg", ".shp", ".geojson", ".json", ".gdb"
     ])
-    
+
     def __post_init__(self):
         """Validate security configuration."""
         if self.max_file_size_mb < 1:
             raise ValidationError("max_file_size_mb must be at least 1MB")
 
 
-@dataclass 
+@dataclass
 class DatabaseConfig:
     """Configuration for database connections."""
     connection_string: Optional[str] = None
@@ -138,7 +140,7 @@
     max_overflow: int = 10
     pool_timeout: int = 30
     echo_sql: bool = False
-    
+
     def __post_init__(self):
         """Validate database configuration."""
         if self.pool_size < 1:
@@ -157,16 +159,17 @@
     validation: ValidationConfig = field(default_factory=ValidationConfig)
     security: SecurityConfig = field(default_factory=SecurityConfig)
     database: DatabaseConfig = field(default_factory=DatabaseConfig)
-    
+
     # Environment-specific settings
     environment: str = "development"
     debug: bool = False
-    
+
     def __post_init__(self):
         """Validate global configuration."""
         valid_environments = ["development", "staging", "production"]
         if self.environment not in valid_environments:
-            raise ValidationError(f"Invalid environment: {self.environment}. Must be one of {valid_environments}")
+            raise ValidationError(
+                f"Invalid environment: {self.environment}. Must be one of {valid_environments}")
 
 
 @dataclass
@@ -180,59 +183,62 @@
     download_format: Optional[str] = None
     staged_data_type: Optional[str] = None
     include: List[str] = field(default_factory=list)
-    
+
     # Additional validation fields
     timeout: Optional[int] = None
     retry_attempts: Optional[int] = None
     priority: int = 10
     tags: List[str] = field(default_factory=list)
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def __post_init__(self):
         """Validate source configuration."""
         if not self.name.strip():
             raise ValidationError("Source name cannot be empty")
         if not self.authority.strip():
             raise ValidationError("Source authority cannot be empty")
-        
+
         valid_types = ["file", "rest_api", "ogc_api", "atom_feed", "database"]
         if self.type not in valid_types:
-            raise ValidationError(f"Invalid source type: {self.type}. Must be one of {valid_types}")
-        
+            raise ValidationError(
+                f"Invalid source type: {self.type}. Must be one of {valid_types}")
+
         if self.enabled and not self.url.strip():
-            raise ValidationError(f"Enabled source '{self.name}' must have a URL")
-        
+            raise ValidationError(
+                f"Enabled source '{self.name}' must have a URL")
+
         if self.priority < 1 or self.priority > 100:
             raise ValidationError("Source priority must be between 1 and 100")
-        
+
         if self.timeout is not None and self.timeout < 1:
             raise ValidationError("Source timeout must be at least 1 second")
-        
+
         if self.retry_attempts is not None and self.retry_attempts < 0:
             raise ValidationError("Source retry_attempts must be non-negative")
 
 
 class ConfigManager:
     """Manages configuration loading, validation, and environment-specific settings."""
-    
+
     def __init__(self, environment: Optional[str] = None):
-        self.environment = environment or os.getenv("ETL_ENVIRONMENT", "development")
+        self.environment = environment or os.getenv(
+            "ETL_ENVIRONMENT", "development")
         self._config_cache: Dict[str, Any] = {}
-    
+
     def load_global_config(self, config_path: Optional[Path] = None) -> GlobalConfig:
         """Load and validate global configuration."""
         if config_path is None:
             config_path = self._find_config_file("config.yaml")
-        
+
         try:
             config_dict = self._load_yaml_file(config_path)
-            
+
             # Apply environment-specific overrides
             config_dict = self._apply_environment_overrides(config_dict)
-            
+
             # Create configuration object with validation
             return self._create_global_config(config_dict)
-            
+
         except Exception as e:
             if isinstance(e, (ConfigurationError, ValidationError)):
                 raise
@@ -240,21 +246,21 @@
                 f"Failed to load configuration from {config_path}: {e}",
                 config_file=str(config_path)
             ) from e
-    
+
     def load_sources_config(self, sources_path: Optional[Path] = None) -> List[SourceConfig]:
         """Load and validate sources configuration."""
         if sources_path is None:
             sources_path = self._find_config_file("sources.yaml")
-        
+
         try:
             sources_dict = self._load_yaml_file(sources_path)
-            
+
             if "sources" not in sources_dict:
                 raise ConfigurationError(
                     f"Configuration file {sources_path} missing 'sources' key",
                     config_file=str(sources_path)
                 )
-            
+
             sources = []
             for i, source_data in enumerate(sources_dict["sources"]):
                 try:
@@ -265,10 +271,10 @@
                         f"Source {i + 1} validation failed: {e}",
                         field_name=f"sources[{i}]"
                     ) from e
-            
+
             log.info("‚úÖ Loaded %d source configurations", len(sources))
             return sources
-            
+
         except Exception as e:
             if isinstance(e, (ConfigurationError, ValidationError)):
                 raise
@@ -276,36 +282,41 @@
                 f"Failed to load sources from {sources_path}: {e}",
                 config_file=str(sources_path)
             ) from e
-    
+
     def validate_configuration(self, config: GlobalConfig) -> List[str]:
         """Validate configuration and return list of warnings."""
         warnings = []
-        
+
         # Check path accessibility
         for path_name in ["download", "staging", "output", "temp", "logs"]:
             path_value = getattr(config.paths, path_name)
             path_obj = Path(path_value)
-            
+
             if not path_obj.parent.exists():
-                warnings.append(f"Parent directory for {path_name} path does not exist: {path_obj.parent}")
-        
+                warnings.append(
+                    f"Parent directory for {path_name} path does not exist: {path_obj.parent}")
+
         # Check memory settings
         if config.processing.memory_limit_mb > 8192:  # 8GB
-            warnings.append("Memory limit is set very high (>8GB). Consider reducing for better stability.")
-        
+            warnings.append(
+                "Memory limit is set very high (>8GB). Consider reducing for better stability.")
+
         # Check retry settings
         if config.retry.max_attempts > 10:
-            warnings.append("Max retry attempts is very high (>10). This may cause long delays.")
-        
+            warnings.append(
+                "Max retry attempts is very high (>10). This may cause long delays.")
+
         # Environment-specific warnings
         if config.environment == "production":
             if config.debug:
-                warnings.append("Debug mode is enabled in production environment")
+                warnings.append(
+                    "Debug mode is enabled in production environment")
             if config.logging.level == "DEBUG":
-                warnings.append("Debug logging is enabled in production environment")
-        
+                warnings.append(
+                    "Debug logging is enabled in production environment")
+
         return warnings
-    
+
     def _find_config_file(self, filename: str) -> Path:
         """Find configuration file in standard locations."""
         """Configuration management and validation for ETL pipeline.
@@ -330,59 +341,62 @@
 
         # ... rest of the code remains unchanged ...
 
-            def _find_config_file(self, filename: str) -> Path:
-                """Find configuration file in standard locations."""
-                search_paths = [
-                    Path.cwd() / "config" / filename,
-                    Path.cwd() / filename,
-                    Path.home() / ".etl" / filename,
-                ]
-                
-                # Add Unix-specific path only on non-Windows systems
-                if platform.system() != "Windows":
+        def _find_config_file(self, filename: str) -> Path:
+             """Find configuration file in standard locations."""
+              search_paths = [
+                   Path.cwd() / "config" / filename,
+                   Path.cwd() / filename,
+                   Path.home() / ".etl" / filename,
+                   ]
+
+               # Add Unix-specific path only on non-Windows systems
+               if platform.system() != "Windows":
                     search_paths.append(Path("/etc/etl") / filename)
-                
+
                 for path in search_paths:
                     if path.exists():
                         return path
-                
-                raise ConfigurationError(f"Configuration file '{filename}' not found in any of: {search_paths}")
+
+                raise ConfigurationError(
+                    f"Configuration file '{filename}' not found in any of: {search_paths}")
         search_paths = [
             Path.cwd() / "config" / filename,
             Path.cwd() / filename,
             Path.home() / ".etl" / filename,
         ]
-        
+
         # Add Unix-specific path only on non-Windows systems
         if platform.system() != "Windows":
             search_paths.append(Path("/etc/etl") / filename)
-        
+
         for path in search_paths:
             if path.exists():
                 return path
-        
-        raise ConfigurationError(f"Configuration file '{filename}' not found in any of: {search_paths}")
-    
+
+        raise ConfigurationError(
+            f"Configuration file '{filename}' not found in any of: {search_paths}")
+
     def _load_yaml_file(self, path: Path) -> Dict[str, Any]:
         """Load YAML file with error handling."""
         if not path.exists():
             raise ConfigurationError(f"Configuration file not found: {path}")
-        
+
         try:
             with path.open("r", encoding="utf-8") as f:
                 content = yaml.safe_load(f)
-            
+
             if content is None:
                 return {}
-            
+
             if not isinstance(content, dict):
-                raise ConfigurationError(f"Configuration file must contain a YAML dictionary: {path}")
-            
+                raise ConfigurationError(
+                    f"Configuration file must contain a YAML dictionary: {path}")
+
             return content
-            
+
         except yaml.YAMLError as e:
             raise ConfigurationError(f"Invalid YAML in {path}: {e}") from e
-    
+
     def _apply_environment_overrides(self, config_dict: Dict[str, Any]) -> Dict[str, Any]:
         """Apply environment-specific configuration overrides."""
         # Look for environment-specific section
@@ -390,15 +404,15 @@
         if env_section in config_dict:
             env_overrides = config_dict[env_section]
             config_dict = self._deep_merge(config_dict, env_overrides)
-        
+
         # Apply environment variables
         config_dict = self._apply_environment_variables(config_dict)
-        
+
         # Set environment in config
         config_dict["environment"] = self.environment
-        
+
         return config_dict
-    
+
     def _apply_environment_variables(self, config_dict: Dict[str, Any]) -> Dict[str, Any]:
         """Apply environment variable overrides."""
         env_mappings = {
@@ -411,7 +425,7 @@
             "ETL_MAX_WORKERS": ("processing", "parallel_workers"),
             "ETL_MEMORY_LIMIT": ("processing", "memory_limit_mb"),
         }
-        
+
         for env_var, config_path in env_mappings.items():
             env_value = os.getenv(env_var)
             if env_value is not None:
@@ -419,93 +433,94 @@
                 current = config_dict
                 for key in config_path[:-1]:
                     current = current.setdefault(key, {})
-                
+
                 # Set the value with appropriate type conversion
                 key = config_path[-1]
                 if key in ["debug"]:
-                    current[key] = env_value.lower() in ("true", "1", "yes", "on")
+                    current[key] = env_value.lower() in (
+                        "true", "1", "yes", "on")
                 elif key in ["parallel_workers", "memory_limit_mb"]:
                     current[key] = int(env_value)
                 else:
                     current[key] = env_value
-        
+
         return config_dict
-    
+
     def _deep_merge(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
         """Deep merge two dictionaries."""
         result = base.copy()
-        
+
         for key, value in override.items():
             if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                 result[key] = self._deep_merge(result[key], value)
             else:
                 result[key] = value
-        
+
         return result
-    
+
     def _create_global_config(self, config_dict: Dict[str, Any]) -> GlobalConfig:
         """Create GlobalConfig from dictionary with validation."""
         # Create nested config objects
         config_sections = {}
-        
+
         if "logging" in config_dict:
             config_sections["logging"] = self._create_dataclass_from_dict(
                 LoggingConfig, config_dict["logging"]
             )
-        
+
         if "retry" in config_dict:
             config_sections["retry"] = self._create_dataclass_from_dict(
                 RetryConfig, config_dict["retry"]
             )
-        
+
         if "paths" in config_dict:
             config_sections["paths"] = self._create_dataclass_from_dict(
                 PathsConfig, config_dict["paths"]
             )
-        
+
         if "processing" in config_dict:
             config_sections["processing"] = self._create_dataclass_from_dict(
                 ProcessingConfig, config_dict["processing"]
             )
-        
+
         if "validation" in config_dict:
             config_sections["validation"] = self._create_dataclass_from_dict(
                 ValidationConfig, config_dict["validation"]
             )
-        
+
         if "security" in config_dict:
             config_sections["security"] = self._create_dataclass_from_dict(
                 SecurityConfig, config_dict["security"]
             )
-        
+
         if "database" in config_dict:
             config_sections["database"] = self._create_dataclass_from_dict(
                 DatabaseConfig, config_dict["database"]
             )
-        
+
         # Add top-level fields
         if "environment" in config_dict:
             config_sections["environment"] = config_dict["environment"]
         if "debug" in config_dict:
             config_sections["debug"] = config_dict["debug"]
-        
+
         return GlobalConfig(**config_sections)
-    
+
     def _create_source_config(self, source_dict: Dict[str, Any]) -> SourceConfig:
         """Create SourceConfig from dictionary with validation."""
         return self._create_dataclass_from_dict(SourceConfig, source_dict)
-    
+
     def _create_dataclass_from_dict(self, cls: Type, data: Dict[str, Any]):
         """Create dataclass instance from dictionary, handling type conversion."""
         field_types = get_type_hints(cls)
         kwargs = {}
-        
+
         for field_info in fields(cls):
             field_name = field_info.name
             if field_name in data:
                 field_type = field_types.get(field_name)
                 value = data[field_name]
-                
+
                 # Handle type conversion
                 if field_type and hasattr(field_type, '__origin__'):
                     # Handle List types
@@ -513,10 +528,11 @@
                         if not isinstance(value, list):
                             if isinstance(value, str):
                                 # Split string into list
-                                value = [item.strip() for item in value.split(',') if item.strip()]
+                                value = [item.strip() for item in value.split(
+                                    ',') if item.strip()]
                             else:
                                 value = [value]
-                
+
                 kwargs[field_name] = value
-        
-        return cls(**kwargs)
\ No newline at end of file
+
+        return cls(**kwargs)
--- original/./etl/mapping.py
+++ fixed/./etl/mapping.py
@@ -28,7 +28,7 @@
     description: Optional[str] = None
     enabled: bool = True
     schema: Optional[str] = None
-    
+
     def __post_init__(self):
         """Validate mapping configuration."""
         if not self.staging_fc.strip():
@@ -52,98 +52,105 @@
 
 class MappingManager:
     """Manages output mappings between staging and production."""
-    
+
     def __init__(self, mappings_file: Optional[Path] = None):
         self.mappings_file = mappings_file
         self.mappings: Dict[str, OutputMapping] = {}
         self.settings = MappingSettings()
-        
+
         if mappings_file:
             self.load_mappings(mappings_file)
-        
-        log.info("üó∫Ô∏è  Mapping manager initialized with %d mappings", len(self.mappings))
-    
+
+        log.info("üó∫Ô∏è  Mapping manager initialized with %d mappings",
+                 len(self.mappings))
+
     def load_mappings(self, mappings_file: Path) -> None:
         """üîÑ Load mappings from YAML file.
-        
+
         Args:
             mappings_file: Path to the mappings YAML file.
-            
+
         Raises:
             ConfigurationError: If the file cannot be loaded or parsed.
         """
         if not mappings_file.exists():
-            log.info("üìã No mappings file found at %s, using defaults only", mappings_file)
+            log.info(
+                "üìã No mappings file found at %s, using defaults only", mappings_file)
             return
-            
+
         try:
             with mappings_file.open('r', encoding='utf-8') as f:
                 content = yaml.safe_load(f)
-                
+
             # Handle empty file or missing mappings section
             if not content:
                 log.info("üìã Empty mappings file, using defaults only")
                 return
-                
+
             # Load settings if present
             if 'settings' in content and content['settings']:
                 self.settings = MappingSettings(**content['settings'])
                 log.info("‚öôÔ∏è  Loaded mapping settings: %s", self.settings)
-            
+
             # Load individual mappings if present
             mappings_data = content.get('mappings', [])
             if not mappings_data:
                 log.info("üìã No mappings defined, using defaults only")
                 return
-                
+
             for mapping_data in mappings_data:
                 try:
                     mapping = OutputMapping(**mapping_data)
                     self.mappings[mapping.staging_fc] = mapping
-                    log.debug("üìå Loaded mapping: %s ‚Üí %s.%s", 
-                             mapping.staging_fc, mapping.sde_dataset, mapping.sde_fc)
+                    log.debug("üìå Loaded mapping: %s ‚Üí %s.%s",
+                              mapping.staging_fc, mapping.sde_dataset, mapping.sde_fc)
                 except TypeError as e:
-                    log.warning("‚ö†Ô∏è  Skipping invalid mapping %s: %s", mapping_data, e)
+                    log.warning(
+                        "‚ö†Ô∏è  Skipping invalid mapping %s: %s", mapping_data, e)
                     continue
-                
-            log.info("‚úÖ Loaded %d mappings from %s", len(self.mappings), mappings_file)
-            
+
+            log.info("‚úÖ Loaded %d mappings from %s",
+                     len(self.mappings), mappings_file)
+
         except yaml.YAMLError as e:
-            raise ConfigurationError(f"Invalid YAML in mappings file {mappings_file}: {e}") from e
+            raise ConfigurationError(
+                f"Invalid YAML in mappings file {mappings_file}: {e}") from e
         except Exception as e:
-            raise ConfigurationError(f"Failed to load mappings from {mappings_file}: {e}") from e
-    
+            raise ConfigurationError(
+                f"Failed to load mappings from {mappings_file}: {e}") from e
+
     def get_output_mapping(self, source: Source, staging_fc_name: str) -> OutputMapping:
         """üîç Get output mapping for a staging feature class.
-    
+
         Args:
             source: The data source.
             staging_fc_name: Name of the staging feature class.
-        
+
         Returns:
             OutputMapping with target SDE details.
         """
-        log.info("üîç Looking for mapping: staging_fc='%s', available mappings: %s", 
+        log.info("üîç Looking for mapping: staging_fc='%s', available mappings: %s",
                  staging_fc_name, list(self.mappings.keys()))
-    
+
         # First check for exact custom mapping
         if staging_fc_name in self.mappings:
             mapping = self.mappings[staging_fc_name]
-            log.info("üìå Found exact mapping: %s ‚Üí %s.%s", 
-                    staging_fc_name, mapping.sde_dataset, mapping.sde_fc)
+            log.info("üìå Found exact mapping: %s ‚Üí %s.%s",
+                     staging_fc_name, mapping.sde_dataset, mapping.sde_fc)
             return mapping
-    
+
         # Check for partial matches (in case of naming variations)
         for mapping_key, mapping in self.mappings.items():
             if staging_fc_name.lower() in mapping_key.lower() or mapping_key.lower() in staging_fc_name.lower():
-                log.info("üìå Found partial mapping: %s ‚âà %s ‚Üí %s.%s", 
-                        staging_fc_name, mapping_key, mapping.sde_dataset, mapping.sde_fc)
+                log.info("üìå Found partial mapping: %s ‚âà %s ‚Üí %s.%s",
+                         staging_fc_name, mapping_key, mapping.sde_dataset, mapping.sde_fc)
                 return mapping
-    
+
         # No custom mapping found, create default
-        log.info("üîß No mapping found for '%s', using default pattern", staging_fc_name)
+        log.info("üîß No mapping found for '%s', using default pattern",
+                 staging_fc_name)
         return self._create_default_mapping(source, staging_fc_name)
-    
+
     def _create_default_mapping(self, source: Source, staging_fc_name: str) -> OutputMapping:
         """Create default mapping using naming patterns."""
         # Generate default SDE dataset name
@@ -151,21 +158,21 @@
             authority=source.authority,
             source_name=sanitize_for_arcgis_name(source.name)
         )
-        
+
         # Generate default SDE feature class name
         sde_fc = self.settings.default_fc_pattern.format(
             authority=source.authority,
             source_name=sanitize_for_arcgis_name(source.name),
             staging_fc=staging_fc_name
         )
-        
+
         # Ensure ArcGIS naming compatibility
         sde_dataset = sanitize_for_arcgis_name(sde_dataset)
         sde_fc = sanitize_for_arcgis_name(sde_fc)
-        
-        log.debug("üîß Generated default mapping for %s -> %s.%s", 
-                 staging_fc_name, sde_dataset, sde_fc)
-        
+
+        log.debug("üîß Generated default mapping for %s -> %s.%s",
+                  staging_fc_name, sde_dataset, sde_fc)
+
         return OutputMapping(
             staging_fc=staging_fc_name,
             sde_fc=sde_fc,
@@ -173,26 +180,26 @@
             description=f"Auto-generated mapping for {source.name}",
             schema=self.settings.default_schema
         )
-    
+
     def get_full_sde_path(self, mapping: OutputMapping, sde_connection: str) -> str:
         """Get full SDE path for a mapping."""
         if mapping.schema:
             return f"{sde_connection}\\{mapping.schema}.{mapping.sde_dataset}\\{mapping.sde_fc}"
         else:
             return f"{sde_connection}\\{mapping.sde_dataset}\\{mapping.sde_fc}"
-    
+
     def get_dataset_path(self, mapping: OutputMapping, sde_connection: str) -> str:
         """Get SDE dataset path for a mapping."""
         if mapping.schema:
             return f"{sde_connection}\\{mapping.schema}.{mapping.sde_dataset}"
         else:
             return f"{sde_connection}\\{mapping.sde_dataset}"
-    
+
     def get_mappings_for_dataset(self, dataset_name: str) -> List[OutputMapping]:
         """Get all mappings that target a specific dataset."""
-        return [mapping for mapping in self.mappings.values() 
+        return [mapping for mapping in self.mappings.values()
                 if mapping.sde_dataset == dataset_name and mapping.enabled]
-    
+
     def get_all_target_datasets(self) -> List[str]:
         """Get list of all target SDE datasets from mappings."""
         datasets = set()
@@ -203,49 +210,55 @@
                 else:
                     datasets.add(mapping.sde_dataset)
         return list(datasets)
-    
+
     def validate_mapping(self, mapping: OutputMapping) -> List[str]:
         """Validate a mapping configuration and return any issues."""
         issues = []
-        
+
         # Check naming conventions
         if not mapping.staging_fc.replace("_", "").replace("-", "").isalnum():
-            issues.append(f"Staging FC name '{mapping.staging_fc}' contains invalid characters")
-        
+            issues.append(
+                f"Staging FC name '{mapping.staging_fc}' contains invalid characters")
+
         if not mapping.sde_fc.replace("_", "").isalnum():
-            issues.append(f"SDE FC name '{mapping.sde_fc}' contains invalid characters")
-        
+            issues.append(
+                f"SDE FC name '{mapping.sde_fc}' contains invalid characters")
+
         if not mapping.sde_dataset.replace("_", "").isalnum():
-            issues.append(f"SDE dataset name '{mapping.sde_dataset}' contains invalid characters")
-        
+            issues.append(
+                f"SDE dataset name '{mapping.sde_dataset}' contains invalid characters")
+
         # Check length limits (ArcGIS SDE limits)
         if len(mapping.sde_fc) > 128:
-            issues.append(f"SDE FC name '{mapping.sde_fc}' exceeds 128 character limit")
-        
+            issues.append(
+                f"SDE FC name '{mapping.sde_fc}' exceeds 128 character limit")
+
         if len(mapping.sde_dataset) > 128:
-            issues.append(f"SDE dataset name '{mapping.sde_dataset}' exceeds 128 character limit")
-        
+            issues.append(
+                f"SDE dataset name '{mapping.sde_dataset}' exceeds 128 character limit")
+
         return issues
-    
+
     def validate_all_mappings(self) -> Dict[str, List[str]]:
         """Validate all mappings and return issues by staging FC name."""
         all_issues = {}
-        
+
         for staging_fc, mapping in self.mappings.items():
             issues = self.validate_mapping(mapping)
             if issues:
                 all_issues[staging_fc] = issues
-        
+
         return all_issues
-    
+
     def get_mapping_statistics(self) -> Dict[str, Any]:
         """Get statistics about the current mappings."""
         enabled_mappings = [m for m in self.mappings.values() if m.enabled]
-        disabled_mappings = [m for m in self.mappings.values() if not m.enabled]
-        
+        disabled_mappings = [
+            m for m in self.mappings.values() if not m.enabled]
+
         datasets = set(m.sde_dataset for m in enabled_mappings)
         schemas = set(m.schema for m in enabled_mappings if m.schema)
-        
+
         return {
             'total_mappings': len(self.mappings),
             'enabled_mappings': len(enabled_mappings),
@@ -255,17 +268,18 @@
             'datasets': list(datasets),
             'schemas': list(schemas)
         }
-    
+
     def add_mapping(self, mapping: OutputMapping) -> None:
         """Add a new mapping."""
         validation_issues = self.validate_mapping(mapping)
         if validation_issues:
-            raise ValidationError(f"Invalid mapping: {'; '.join(validation_issues)}")
-        
+            raise ValidationError(
+                f"Invalid mapping: {'; '.join(validation_issues)}")
+
         self.mappings[mapping.staging_fc] = mapping
-        log.info("‚ûï Added mapping: %s -> %s.%s", 
-                mapping.staging_fc, mapping.sde_dataset, mapping.sde_fc)
-    
+        log.info("‚ûï Added mapping: %s -> %s.%s",
+                 mapping.staging_fc, mapping.sde_dataset, mapping.sde_fc)
+
     def remove_mapping(self, staging_fc: str) -> bool:
         """Remove a mapping by staging FC name."""
         if staging_fc in self.mappings:
@@ -273,14 +287,15 @@
             log.info("‚ûñ Removed mapping for: %s", staging_fc)
             return True
         return False
-    
+
     def save_mappings(self, output_file: Optional[Path] = None) -> None:
         """Save current mappings to file."""
         output_file = output_file or self.mappings_file
-        
+
         if not output_file:
-            raise ConfigurationError("No output file specified for saving mappings")
-        
+            raise ConfigurationError(
+                "No output file specified for saving mappings")
+
         # Prepare data for serialization
         mappings_data = []
         for mapping in self.mappings.values():
@@ -289,16 +304,16 @@
                 'sde_fc': mapping.sde_fc,
                 'sde_dataset': mapping.sde_dataset
             }
-            
+
             if mapping.description:
                 mapping_dict['description'] = mapping.description
             if not mapping.enabled:
                 mapping_dict['enabled'] = mapping.enabled
             if mapping.schema != self.settings.default_schema:
                 mapping_dict['schema'] = mapping.schema
-            
+
             mappings_data.append(mapping_dict)
-        
+
         # Prepare settings data
         settings_data = {
             'default_schema': self.settings.default_schema,
@@ -308,46 +323,50 @@
             'create_missing_datasets': self.settings.create_missing_datasets,
             'skip_unmappable_sources': self.settings.skip_unmappable_sources
         }
-        
+
         output_data = {
             'settings': settings_data,
             'mappings': mappings_data
         }
-        
+
         try:
             with output_file.open('w', encoding='utf-8') as f:
-                yaml.dump(output_data, f, default_flow_style=False, sort_keys=False)
-            
-            log.info("üíæ Saved %d mappings to %s", len(self.mappings), output_file)
-            
+                yaml.dump(output_data, f, default_flow_style=False,
+                          sort_keys=False)
+
+            log.info("üíæ Saved %d mappings to %s",
+                     len(self.mappings), output_file)
+
         except Exception as e:
-            raise ConfigurationError(f"Failed to save mappings to {output_file}: {e}") from e
-    
+            raise ConfigurationError(
+                f"Failed to save mappings to {output_file}: {e}") from e
+
     def get_explicit_mapping(self, staging_fc_name: str) -> Optional[OutputMapping]:
         """üîç Get explicit mapping for a staging feature class (no fallback).
-    
+
         Args:
             staging_fc_name: Name of the staging feature class.
-        
+
         Returns:
             OutputMapping if found, None if no explicit mapping exists.
         """
         # First check for exact custom mapping
         if staging_fc_name in self.mappings:
             mapping = self.mappings[staging_fc_name]
-            log.info("üìå Found exact mapping: %s ‚Üí %s.%s", 
-                    staging_fc_name, mapping.sde_dataset, mapping.sde_fc)
+            log.info("üìå Found exact mapping: %s ‚Üí %s.%s",
+                     staging_fc_name, mapping.sde_dataset, mapping.sde_fc)
             return mapping
-    
+
         # Check for partial matches (in case of naming variations)
         for mapping_key, mapping in self.mappings.items():
             if staging_fc_name.lower() in mapping_key.lower() or mapping_key.lower() in staging_fc_name.lower():
-                log.info("üìå Found partial mapping: %s ‚âà %s ‚Üí %s.%s", 
-                        staging_fc_name, mapping_key, mapping.sde_dataset, mapping.sde_fc)
+                log.info("üìå Found partial mapping: %s ‚âà %s ‚Üí %s.%s",
+                         staging_fc_name, mapping_key, mapping.sde_dataset, mapping.sde_fc)
                 return mapping
-    
+
         # No explicit mapping found
         return None
+
 
 # Global mapping manager instance
 _mapping_manager: Optional[MappingManager] = None
@@ -356,7 +375,7 @@
 def get_mapping_manager(mappings_file: Optional[Path] = None) -> MappingManager:
     """Get global mapping manager instance."""
     global _mapping_manager
-    
+
     if _mapping_manager is None:
         # Try to find mappings file if not provided
         if mappings_file is None:
@@ -365,22 +384,22 @@
                 Path("mappings.yaml"),
                 Path.cwd() / "config" / "mappings.yaml"
             ]
-            
+
             for path in default_paths:
                 if path.exists():
                     mappings_file = path
                     break
-        
+
         _mapping_manager = MappingManager(mappings_file)
-    
+
     return _mapping_manager
 
 
 def load_mappings_from_config(config: Dict[str, Any]) -> MappingManager:
     """Load mappings based on configuration."""
     mappings_file = None
-    
+
     if 'mappings_file' in config:
         mappings_file = Path(config['mappings_file'])
-    
-    return MappingManager(mappings_file)
\ No newline at end of file
+
+    return MappingManager(mappings_file)
--- original/./etl/pipeline.py
+++ fixed/./etl/pipeline.py
@@ -90,7 +90,7 @@
         self.recovery_manager = get_global_recovery_manager()
         self.rollback_manager = get_global_rollback_manager()
         self.degradation_config = GracefulDegradationConfig()
-        
+
         # Configure recovery strategies based on config
         self._setup_pipeline_recovery_strategies()
 
@@ -101,14 +101,14 @@
         # Configure degradation thresholds based on config
         max_concurrent = self.global_cfg.get("concurrent_download_workers", 5)
         timeout = self.global_cfg.get("timeout", 30)
-        
+
         # Update degradation config
         self.degradation_config.max_concurrent_downloads = max_concurrent
         self.degradation_config.timeout_seconds = timeout
-        
+
         # Register pipeline-specific recovery strategies
         from .exceptions import NetworkError, SourceError, SystemError
-        
+
         # Network recovery with config-aware degradation
         def degrade_network_config():
             level = self.recovery_manager.get_degradation_level()
@@ -122,7 +122,7 @@
             self.global_cfg["concurrent_download_workers"] = degraded["concurrent_downloads"]
             self.global_cfg["timeout"] = degraded["timeout"]
             return degraded
-        
+
         self.recovery_manager.register_recovery_strategy(
             NetworkError,
             RecoveryStrategy.DEGRADE,
@@ -130,7 +130,7 @@
             description="Reduce concurrent downloads and increase timeout",
             priority=4
         )
-        
+
         # Source recovery with cached data fallback
         def use_cached_data():
             # Look for previous successful downloads
@@ -139,7 +139,7 @@
                 self.logger.info("üîÑ Looking for cached data in %s", cache_dir)
                 return list(cache_dir.glob("*.json"))
             return []
-        
+
         self.recovery_manager.register_recovery_strategy(
             SourceError,
             RecoveryStrategy.FALLBACK,
@@ -165,8 +165,10 @@
 
         # ---------- 0. PRE-PIPELINE CLEANUP -------------------------------
         # Clean downloads and staging folders for fresh data
-        cleanup_downloads = self.global_cfg.get("cleanup_downloads_before_run", True)
-        cleanup_staging = self.global_cfg.get("cleanup_staging_before_run", True)
+        cleanup_downloads = self.global_cfg.get(
+            "cleanup_downloads_before_run", True)
+        cleanup_staging = self.global_cfg.get(
+            "cleanup_staging_before_run", True)
 
         if cleanup_downloads or cleanup_staging:
             lg_sum.info("üßπ Starting pre-pipeline cleanup...")
@@ -174,21 +176,26 @@
 
         # ---------- 1. DOWNLOAD & STAGING ---------------------------------
         sources = list(Source.load_all(self.sources_yaml_path))
-        self.logger.info("üìã Found sources to process", source_count=len(sources))
+        self.logger.info("üìã Found sources to process",
+                         source_count=len(sources))
 
         # Create SDE loader for proper source-to-dataset mapping
         from .models import SdeLoader, AppConfig
-        app_config = AppConfig(sde_dataset_pattern=self.global_cfg.get("sde_dataset_pattern", "Underlag_{authority}"))
+        app_config = AppConfig(sde_dataset_pattern=self.global_cfg.get(
+            "sde_dataset_pattern", "Underlag_{authority}"))
         self.sde_loader = SdeLoader(app_config, sources)
 
         # Log concurrent download configuration
         if self.global_cfg.get("enable_concurrent_downloads", True):
             self.logger.info("üöÄ Concurrent downloads enabled: REST=%d, OGC=%d, Files=%d workers",
-                           self.global_cfg.get("concurrent_download_workers", 5),
-                           self.global_cfg.get("concurrent_collection_workers", 3),
-                           self.global_cfg.get("concurrent_file_workers", 4))
+                             self.global_cfg.get(
+                                 "concurrent_download_workers", 5),
+                             self.global_cfg.get(
+                                 "concurrent_collection_workers", 3),
+                             self.global_cfg.get("concurrent_file_workers", 4))
         else:
-            self.logger.info("‚ö†Ô∏è Concurrent downloads disabled - using sequential processing")
+            self.logger.info(
+                "‚ö†Ô∏è Concurrent downloads disabled - using sequential processing")
 
         for src in sources:
             if not src.enabled:
@@ -220,7 +227,8 @@
                     self.metrics.record_timing(
                         "download.duration_ms",
                         download_duration * 1000,
-                        tags={"source": src.name, "type": src.type, "concurrent": str(self.global_cfg.get("enable_concurrent_downloads", True))},
+                        tags={"source": src.name, "type": src.type, "concurrent": str(
+                            self.global_cfg.get("enable_concurrent_downloads", True))},
                     )
                     self.metrics.increment_counter(
                         "download.success", tags={"source": src.name}
@@ -228,7 +236,8 @@
 
                     # Log performance improvement hint
                     if download_duration > 60 and src.type in ["rest_api", "ogc_api"] and not self.global_cfg.get("enable_concurrent_downloads", True):
-                        self.logger.info("üí° Performance hint: Enable concurrent downloads for faster processing of %s sources", src.type)
+                        self.logger.info(
+                            "üí° Performance hint: Enable concurrent downloads for faster processing of %s sources", src.type)
 
                     self.summary.log_download("done")
                     self.monitor.record_source_processed(success=True)
@@ -239,25 +248,30 @@
                         error=exc,
                         operation_context=f"download_source_{src.name}"
                     )
-                    
+
                     if recovery_result.success:
-                        self.logger.info("‚úÖ Recovered from download error for %s", src.name)
+                        self.logger.info(
+                            "‚úÖ Recovered from download error for %s", src.name)
                         self.summary.log_download("recovered")
-                        self.monitor.record_source_processed(success=True, error=f"Recovered: {exc}")
+                        self.monitor.record_source_processed(
+                            success=True, error=f"Recovered: {exc}")
                     else:
                         self.summary.log_download("error")
                         self.summary.log_error(src.name, str(exc))
-                        self.logger.error("‚ùå Download failed and recovery failed", source_name=src.name, error=exc)
+                        self.logger.error(
+                            "‚ùå Download failed and recovery failed", source_name=src.name, error=exc)
 
                         self.metrics.increment_counter(
                             "download.error", tags={"source": src.name}
                         )
-                        self.monitor.record_source_processed(success=False, error=str(exc))
+                        self.monitor.record_source_processed(
+                            success=False, error=str(exc))
 
                         if not self.global_cfg.get("continue_on_failure", True):
                             self.monitor.end_run("failed")
                             # Execute pipeline rollback before raising
-                            execute_pipeline_rollback(f"Source download failed: {src.name}")
+                            execute_pipeline_rollback(
+                                f"Source download failed: {src.name}")
                             raise  # ---------- 2. STAGE ‚Üí staging.gdb --------------------------------
         self.logger.info("üì¶ Starting staging phase")
 
@@ -270,13 +284,14 @@
                 reset_gdb(paths.GDB)
             self.logger.info("‚úÖ Staging GDB reset complete")
         except (ImportError, arcpy.ExecuteError, OSError) as reset_exc:
-            self.logger.warning("‚ö†Ô∏è Failed to reset staging GDB", error=reset_exc)
+            self.logger.warning(
+                "‚ö†Ô∏è Failed to reset staging GDB", error=reset_exc)
             if not self.global_cfg.get("continue_on_failure", True):
                 self.monitor.end_run("failed")
                 raise
 
         staging_success = True
-        
+
         # Wrap staging in graceful degradation
         with graceful_degradation("staging_phase", self.recovery_manager):
             try:
@@ -289,7 +304,8 @@
                 loader.run()
 
                 staging_duration = time.time() - start_time
-                self.metrics.record_timing("staging.duration_ms", staging_duration * 1000)
+                self.metrics.record_timing(
+                    "staging.duration_ms", staging_duration * 1000)
                 self.metrics.increment_counter("staging.success")
 
                 self.logger.info(
@@ -302,7 +318,7 @@
                     error=exc,
                     operation_context="staging_phase"
                 )
-                
+
                 if recovery_result.success:
                     self.logger.info("‚úÖ Recovered from staging error")
                     self.summary.log_staging("recovered")
@@ -311,7 +327,8 @@
                     self.summary.log_staging("error")
                     self.summary.log_error("GDB loader", str(exc))
 
-                    self.logger.error("‚ùå GDB load failed and recovery failed", error=exc)
+                    self.logger.error(
+                        "‚ùå GDB load failed and recovery failed", error=exc)
                     self.metrics.increment_counter("staging.error")
 
                     if not self.global_cfg.get("continue_on_failure", True):
@@ -348,7 +365,7 @@
         # Log final metrics and recovery statistics
         pipeline_stats = self.monitor.get_current_run()
         recovery_stats = self.recovery_manager.get_recovery_stats()
-        
+
         if pipeline_stats:
             self.logger.info(
                 "üèÅ Pipeline completed successfully",
@@ -357,7 +374,7 @@
                 success_rate=pipeline_stats.success_rate,
                 degradation_level=self.recovery_manager.get_degradation_level()
             )
-        
+
         # Log recovery statistics
         if recovery_stats:
             self.logger.info("üìä Recovery Statistics:")
@@ -368,7 +385,7 @@
                     stats["attempts"],
                     stats["success_rate"]
                 )
-        
+
         # Reset degradation level for next run
         self.recovery_manager.reset_degradation_level()
 
@@ -391,9 +408,11 @@
             )
         )
         if not aoi_boundary.exists():
-            self.logger.error("‚ùå AOI boundary not found", aoi_path=str(aoi_boundary))
+            self.logger.error("‚ùå AOI boundary not found",
+                              aoi_path=str(aoi_boundary))
             if not self.global_cfg.get("continue_on_failure", True):
-                raise FileNotFoundError(f"AOI boundary not found: {aoi_boundary}")
+                raise FileNotFoundError(
+                    f"AOI boundary not found: {aoi_boundary}")
             return
 
         try:
@@ -409,7 +428,8 @@
                 staging_gdb=paths.GDB,
                 aoi_fc=aoi_boundary,
                 target_srid=geoprocessing_config.get("target_srid", 3006),
-                pp_factor=geoprocessing_config.get("parallel_processing_factor", "100"),
+                pp_factor=geoprocessing_config.get(
+                    "parallel_processing_factor", "100"),
             )
 
             geoprocessing_duration = time.time() - start_time
@@ -433,7 +453,8 @@
         """üöö Step 4: Load processed GDB to production SDE with parallel processing"""
 
         if not source_gdb.exists():
-            self.logger.error("‚ùå Source GDB not found", gdb_path=str(source_gdb))
+            self.logger.error("‚ùå Source GDB not found",
+                              gdb_path=str(source_gdb))
             return
 
         # Get SDE connection from config and validate
@@ -454,7 +475,8 @@
 
         all_feature_classes = self._discover_feature_classes(source_gdb)
         if not all_feature_classes:
-            self.logger.warning("‚ö†Ô∏è No feature classes found", gdb_path=str(source_gdb))
+            self.logger.warning("‚ö†Ô∏è No feature classes found",
+                                gdb_path=str(source_gdb))
             return
 
         self.logger.info(
@@ -477,7 +499,8 @@
 
     def _validate_sde_connection_file(self, path: Path) -> bool:
         if not path.exists():
-            self.logger.error("‚ùå SDE connection file not found", sde_path=str(path))
+            self.logger.error(
+                "‚ùå SDE connection file not found", sde_path=str(path))
             return False
         return True
 
@@ -495,7 +518,8 @@
                     all_fcs.append((fc_full_path, fc))
             datasets = arcpy.ListDatasets(feature_type="Feature")
             if datasets:
-                self.logger.debug("üìÅ Found feature datasets", count=len(datasets))
+                self.logger.debug("üìÅ Found feature datasets",
+                                  count=len(datasets))
                 for ds in datasets:
                     ds_fcs = arcpy.ListFeatureClasses(feature_dataset=ds)
                     if ds_fcs:
@@ -527,7 +551,8 @@
         )
 
         # Get load strategy from config (default: truncate_and_load)
-        load_strategy = self.global_cfg.get("sde_load_strategy", "truncate_and_load")
+        load_strategy = self.global_cfg.get(
+            "sde_load_strategy", "truncate_and_load")
 
         try:
             # Check if target dataset exists in SDE
@@ -620,13 +645,16 @@
 
         if arcpy.Exists(target_path):
             if load_strategy == "truncate_and_load":
-                lg_sum.info("üóëÔ∏è Truncating existing FC: %s\\%s", dataset, sde_fc_name)
+                lg_sum.info("üóëÔ∏è Truncating existing FC: %s\\%s",
+                            dataset, sde_fc_name)
                 arcpy.management.TruncateTable(target_path)
-                lg_sum.info("üìÑ Loading fresh data to: %s\\%s", dataset, sde_fc_name)
+                lg_sum.info("üìÑ Loading fresh data to: %s\\%s",
+                            dataset, sde_fc_name)
                 arcpy.management.Append(
                     inputs=source_fc_path, target=target_path, schema_type="NO_TEST"
                 )
-                lg_sum.info("üöö‚Üí  %s\\%s (truncated + loaded)", dataset, sde_fc_name)
+                lg_sum.info("üöö‚Üí  %s\\%s (truncated + loaded)",
+                            dataset, sde_fc_name)
             elif load_strategy == "replace":
                 self.logger.info(
                     "üóëÔ∏è Deleting existing FC", dataset=dataset, fc=sde_fc_name
@@ -645,7 +673,8 @@
                 )
 
                 duration = time.time() - start_time
-                self.metrics.record_timing("sde.replace.duration_ms", duration * 1000)
+                self.metrics.record_timing(
+                    "sde.replace.duration_ms", duration * 1000)
                 self.logger.info(
                     "üöö‚Üí Replaced",
                     dataset=dataset,
@@ -681,7 +710,8 @@
             )
 
             duration = time.time() - start_time
-            self.metrics.record_timing("sde.create.duration_ms", duration * 1000)
+            self.metrics.record_timing(
+                "sde.create.duration_ms", duration * 1000)
             self.logger.info(
                 "üöö‚Üí Created",
                 dataset=dataset,
--- original/./etl/models.py
+++ fixed/./etl/models.py
@@ -18,23 +18,23 @@
     """Parse include field from YAML into a list of strings."""
     if include_value is None:
         return []
-    
+
     if isinstance(include_value, list):
         return include_value
-    
+
     if isinstance(include_value, str):
         # Handle semicolon-separated strings
         if ";" in include_value:
             return [item.strip() for item in include_value.split(";") if item.strip()]
         return [include_value]
-    
+
     return []
 
 
 @dataclass(slots=True, frozen=True)
 class AppConfig:
     """Application configuration settings."""
-    
+
     sde_dataset_pattern: str
 
 
@@ -63,7 +63,7 @@
             List of Source objects loaded from the file.
         """
         yaml_path = Path(yaml_path)
-        
+
         if not yaml_path.exists():
             log.warning("‚ö†Ô∏è Sources YAML file not found: %s", yaml_path)
             return []
@@ -84,7 +84,8 @@
 
         sources_data = data.get("sources", [])
         if not isinstance(sources_data, list):
-            log.warning("‚ö†Ô∏è 'sources' key is not a list in YAML file: %s", yaml_path)
+            log.warning(
+                "‚ö†Ô∏è 'sources' key is not a list in YAML file: %s", yaml_path)
             return []
 
         sources: List[Source] = []
@@ -92,7 +93,7 @@
             try:
                 # Parse the include field properly
                 include_parsed = _parse_include(source_data.get("include"))
-                
+
                 source = cls(
                     name=source_data["name"],
                     authority=source_data["authority"],
@@ -105,7 +106,7 @@
                     include=include_parsed,
                 )
                 sources.append(source)
-                
+
             except KeyError as exc:
                 log.warning(
                     "‚ö†Ô∏è Skipping source with missing field: %s. Source data: %s",
@@ -141,8 +142,9 @@
     def _find_source_for_fc(self, staging_fc_name: str) -> Optional[Source]:
         """Find the original Source object for a staging feature class."""
         log.info("üîç Looking for source for FC: %s", staging_fc_name)
-        log.info("üîç Available sources: %d enabled sources", len([s for s in self.sources if s.enabled]))
-        
+        log.info("üîç Available sources: %d enabled sources",
+                 len([s for s in self.sources if s.enabled]))
+
         for source in self.sources:
             if not source.enabled:
                 continue
@@ -161,7 +163,7 @@
                 sanitized_source_name,
                 staging_fc_name,
             )
-            
+
             if sanitized_source_name in staging_fc_name:
                 log.info(
                     "üîç ‚úÖ Source matched by name: '%s' ('%s' in '%s')",
@@ -171,7 +173,7 @@
                 )
                 return source
 
-            # Method 2: For REST API sources with multiple layers, the feature class name 
+            # Method 2: For REST API sources with multiple layers, the feature class name
             # might be "{sanitized_source_name}_{layer_suffix}_{geometry_type}"
             if source.type in ("rest_api", "ogc_api") and source.raw.get("layer_ids"):
                 # Check if the fc name starts with the sanitized source name
@@ -211,7 +213,7 @@
                         return source
 
         log.warning("‚ö†Ô∏è No source found for staging fc: %s", staging_fc_name)
-        
+
         # Debug: Log all available sources for troubleshooting
         log.info("üîç Available sources for debugging:")
         for source in self.sources:
@@ -224,18 +226,20 @@
                     sanitized,
                     source.type,
                 )
-        
+
         return None
 
     def _map_to_sde(self, staging_fc_name: str) -> Optional[Tuple[str, str]]:
         """Map a staging feature class to its target SDE dataset and feature class name."""
         source = self._find_source_for_fc(staging_fc_name)
         if not source:
-            log.warning("‚ö†Ô∏è Could not find a source for staging fc: %s", staging_fc_name)
+            log.warning(
+                "‚ö†Ô∏è Could not find a source for staging fc: %s", staging_fc_name)
             # Fallback: try to extract authority from the beginning of the fc name
             # This assumes the pattern is "{authority}_{rest_of_name}"
             parts = staging_fc_name.split("_", 1)
-            if len(parts) >= 2 and len(parts[0]) <= 5:  # Authorities are typically short
+            # Authorities are typically short
+            if len(parts) >= 2 and len(parts[0]) <= 5:
                 authority = parts[0].upper()
             else:
                 authority = "UNKNOWN"
@@ -243,7 +247,8 @@
             return dataset_name, sanitize_for_arcgis_name(staging_fc_name)
 
         # Use the source's authority to create the dataset name
-        dataset_name = self.sde_dataset_pattern.format(authority=source.authority)
+        dataset_name = self.sde_dataset_pattern.format(
+            authority=source.authority)
 
         # The final feature class name is the sanitized staging name
         final_fc_name = sanitize_for_arcgis_name(staging_fc_name)
@@ -263,4 +268,4 @@
         # This is a placeholder for the method that would orchestrate the loading.
         # It would list feature classes in staging_gdb and call _map_to_sde for each.
         log.info("Starting SDE load from: %s", staging_gdb)
-        # ... implementation needed here ...
\ No newline at end of file
+        # ... implementation needed here ...
--- original/./etl/network_context.py
+++ fixed/./etl/network_context.py
@@ -18,13 +18,13 @@
 @dataclass
 class NetworkContext:
     """Mutable context for network configuration and degradation.
-    
+
     This class provides a way to manage dynamic network settings without
     modifying the global configuration directly. It allows for graceful
     degradation of network parameters in response to failures while
     maintaining clear separation between static configuration and runtime
     adjustments.
-    
+
     Attributes:
         timeout: Current timeout value in seconds
         max_retries: Current maximum retry attempts
@@ -35,30 +35,30 @@
         degraded: Whether the context is in degraded mode
         degradation_history: History of degradation events
     """
-    
+
     # Network timing settings
     timeout: float = 30.0
     max_retries: int = 3
     backoff_factor: float = 2.0
     max_delay: float = 300.0
-    
+
     # Circuit breaker settings
     circuit_breaker_threshold: int = 5
     circuit_breaker_timeout: float = 60.0
-    
+
     # Rate limiting
     rate_limit_delay: float = 0.0
-    
+
     # Degradation tracking
     degraded: bool = False
     degradation_level: int = 0
     degradation_history: list[Dict[str, Any]] = field(default_factory=list)
-    
+
     # Context metadata
     source_name: Optional[str] = None
     handler_type: Optional[str] = None
     created_at: float = field(default_factory=time.time)
-    
+
     @classmethod
     def from_global_config(
         cls,
@@ -67,18 +67,18 @@
         handler_type: Optional[str] = None
     ) -> NetworkContext:
         """Create a NetworkContext from global configuration.
-        
+
         Args:
             global_config: Global configuration dictionary
             source_name: Name of the source using this context
             handler_type: Type of handler using this context
-            
+
         Returns:
             NetworkContext initialized with global config values
         """
         retry_config = global_config.get("retry", {})
         circuit_config = global_config.get("circuit_breaker", {})
-        
+
         return cls(
             timeout=global_config.get("timeout", 30.0),
             max_retries=retry_config.get("max_attempts", 3),
@@ -89,7 +89,7 @@
             source_name=source_name,
             handler_type=handler_type
         )
-    
+
     def degrade_network_config(
         self,
         reason: str,
@@ -97,12 +97,12 @@
         severity: str = "moderate"
     ) -> None:
         """Apply network degradation to improve resilience.
-        
+
         This method implements graceful degradation by adjusting network
         parameters to be more conservative when failures occur. Unlike
         modifying global_cfg directly, this approach keeps the degradation
         isolated to this context instance.
-        
+
         Args:
             reason: Human-readable reason for degradation
             error: Optional exception that triggered degradation
@@ -110,7 +110,7 @@
         """
         self.degradation_level += 1
         self.degraded = True
-        
+
         # Record degradation event
         degradation_event = {
             "timestamp": time.time(),
@@ -121,7 +121,7 @@
             "error_message": str(error) if error else None
         }
         self.degradation_history.append(degradation_event)
-        
+
         # Apply degradation based on severity
         if severity == "mild":
             self._apply_mild_degradation()
@@ -130,30 +130,32 @@
         elif severity == "severe":
             self._apply_severe_degradation()
         else:
-            log.warning("Unknown degradation severity: %s, applying moderate", severity)
+            log.warning(
+                "Unknown degradation severity: %s, applying moderate", severity)
             self._apply_moderate_degradation()
-        
+
         log.warning(
             "üîª Network degradation applied - Source: %s, Reason: %s, Level: %d",
             self.source_name or "unknown",
             reason,
             self.degradation_level
         )
-    
+
     def _apply_mild_degradation(self) -> None:
         """Apply mild network degradation."""
         self.timeout = min(self.timeout * 1.2, 60.0)
         self.max_retries = min(self.max_retries + 1, 5)
         self.rate_limit_delay = max(self.rate_limit_delay, 0.5)
-    
+
     def _apply_moderate_degradation(self) -> None:
         """Apply moderate network degradation."""
         self.timeout = min(self.timeout * 1.5, 90.0)
         self.max_retries = min(self.max_retries + 2, 7)
         self.backoff_factor = min(self.backoff_factor * 1.5, 4.0)
         self.rate_limit_delay = max(self.rate_limit_delay, 1.0)
-        self.circuit_breaker_threshold = max(self.circuit_breaker_threshold - 1, 2)
-    
+        self.circuit_breaker_threshold = max(
+            self.circuit_breaker_threshold - 1, 2)
+
     def _apply_severe_degradation(self) -> None:
         """Apply severe network degradation."""
         self.timeout = min(self.timeout * 2.0, 120.0)
@@ -161,11 +163,12 @@
         self.backoff_factor = min(self.backoff_factor * 2.0, 5.0)
         self.max_delay = min(self.max_delay * 1.5, 600.0)
         self.rate_limit_delay = max(self.rate_limit_delay, 2.0)
-        self.circuit_breaker_threshold = max(self.circuit_breaker_threshold - 2, 1)
-    
+        self.circuit_breaker_threshold = max(
+            self.circuit_breaker_threshold - 2, 1)
+
     def reset_degradation(self, global_config: Dict[str, Any]) -> None:
         """Reset the context to original configuration values.
-        
+
         Args:
             global_config: Global configuration to reset to
         """
@@ -174,7 +177,7 @@
             self.source_name,
             self.handler_type
         )
-        
+
         # Reset network settings to original values
         self.timeout = original_context.timeout
         self.max_retries = original_context.max_retries
@@ -183,20 +186,20 @@
         self.circuit_breaker_threshold = original_context.circuit_breaker_threshold
         self.circuit_breaker_timeout = original_context.circuit_breaker_timeout
         self.rate_limit_delay = 0.0
-        
+
         # Reset degradation state
         self.degraded = False
         self.degradation_level = 0
-        
+
         log.info(
             "üîÑ Network context reset - Source: %s",
             self.source_name or "unknown"
         )
-    
+
     def should_apply_rate_limit(self) -> bool:
         """Check if rate limiting should be applied."""
         return self.rate_limit_delay > 0
-    
+
     def apply_rate_limit(self) -> None:
         """Apply rate limiting delay if configured."""
         if self.should_apply_rate_limit():
@@ -206,10 +209,10 @@
                 self.source_name or "unknown"
             )
             time.sleep(self.rate_limit_delay)
-    
+
     def get_retry_config_dict(self) -> Dict[str, Any]:
         """Get current configuration as a dictionary for retry mechanisms.
-        
+
         Returns:
             Dictionary compatible with RetryConfig
         """
@@ -219,10 +222,10 @@
             "backoff_factor": self.backoff_factor,
             "max_delay": self.max_delay
         }
-    
+
     def get_circuit_breaker_config_dict(self) -> Dict[str, Any]:
         """Get current circuit breaker configuration as a dictionary.
-        
+
         Returns:
             Dictionary with circuit breaker settings
         """
@@ -230,10 +233,10 @@
             "failure_threshold": self.circuit_breaker_threshold,
             "recovery_timeout": self.circuit_breaker_timeout
         }
-    
+
     def get_status_summary(self) -> Dict[str, Any]:
         """Get a summary of the current context status.
-        
+
         Returns:
             Dictionary with context status information
         """
@@ -249,7 +252,7 @@
             "degradation_events": len(self.degradation_history),
             "created_at": self.created_at
         }
-    
+
     def __str__(self) -> str:
         """String representation of the context."""
         status = "degraded" if self.degraded else "normal"
@@ -257,4 +260,4 @@
             f"NetworkContext(source={self.source_name}, "
             f"status={status}, level={self.degradation_level}, "
             f"timeout={self.timeout}s, retries={self.max_retries})"
-        )
\ No newline at end of file
+        )
--- original/./etl/monitoring.py
+++ fixed/./etl/monitoring.py
@@ -19,9 +19,11 @@
 from .exceptions import ETLError, format_error_context
 
 # Structured logging formatter
+
+
 class StructuredFormatter(logging.Formatter):
     """JSON formatter for structured logging."""
-    
+
     def format(self, record: logging.LogRecord) -> str:
         """Format log record as structured JSON."""
         log_data = {
@@ -33,7 +35,7 @@
             'function': record.funcName,
             'line': record.lineno
         }
-        
+
         # Add exception information if present
         if record.exc_info:
             log_data['exception'] = {
@@ -41,15 +43,15 @@
                 'message': str(record.exc_info[1]),
                 'traceback': self.formatException(record.exc_info)
             }
-        
+
         # Add extra fields from record
         for key, value in record.__dict__.items():
             if key not in ('name', 'msg', 'args', 'levelname', 'levelno', 'pathname',
-                          'filename', 'module', 'lineno', 'funcName', 'created',
-                          'msecs', 'relativeCreated', 'thread', 'threadName',
-                          'processName', 'process', 'exc_info', 'exc_text', 'stack_info'):
+                           'filename', 'module', 'lineno', 'funcName', 'created',
+                           'msecs', 'relativeCreated', 'thread', 'threadName',
+                           'processName', 'process', 'exc_info', 'exc_text', 'stack_info'):
                 log_data[key] = value
-        
+
         return json.dumps(log_data, default=str)
 
 
@@ -87,13 +89,13 @@
     total_records: int = 0
     total_bytes: int = 0
     errors: List[str] = field(default_factory=list)
-    
+
     @property
     def duration(self) -> float:
         """Get run duration in seconds."""
         end = self.end_time or time.time()
         return end - self.start_time
-    
+
     @property
     def success_rate(self) -> float:
         """Get success rate as percentage."""
@@ -104,22 +106,24 @@
 
 class MetricsCollector:
     """Collects and manages performance metrics."""
-    
+
     def __init__(self, max_points: int = 10000):
         self.max_points = max_points
-        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_points))
+        self._metrics: Dict[str, deque] = defaultdict(
+            lambda: deque(maxlen=max_points))
         self._lock = threading.RLock()
-        
+
         # Built-in metrics
         self._counters: Dict[str, float] = defaultdict(float)
         self._gauges: Dict[str, float] = defaultdict(float)
-        
-        logging.getLogger(__name__).info("üìä Metrics collector initialized (max_points=%d)", max_points)
-    
+
+        logging.getLogger(__name__).info(
+            "üìä Metrics collector initialized (max_points=%d)", max_points)
+
     def record_metric(
-        self, 
-        name: str, 
-        value: Union[int, float], 
+        self,
+        name: str,
+        value: Union[int, float],
         tags: Optional[Dict[str, str]] = None,
         metric_type: str = "gauge"
     ):
@@ -132,38 +136,38 @@
             tags=tags or {},
             metric_type=metric_type
         )
-        
+
         with self._lock:
             self._metrics[name].append(point)
-            
+
             # Update internal counters/gauges
             if metric_type == "counter":
                 self._counters[name] += value
             elif metric_type == "gauge":
                 self._gauges[name] = value
-    
+
     def increment_counter(self, name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None):
         """Increment a counter metric."""
         self.record_metric(name, value, tags, "counter")
-    
+
     def set_gauge(self, name: str, value: Union[int, float], tags: Optional[Dict[str, str]] = None):
         """Set a gauge metric value."""
         self.record_metric(name, value, tags, "gauge")
-    
+
     def record_timing(self, name: str, duration_ms: float, tags: Optional[Dict[str, str]] = None):
         """Record a timing metric."""
         self.record_metric(name, duration_ms, tags, "histogram")
-    
+
     def get_metric_history(self, name: str, since: Optional[float] = None) -> List[MetricPoint]:
         """Get metric history for a given metric name."""
         with self._lock:
             points = list(self._metrics.get(name, []))
-            
+
             if since is not None:
                 points = [p for p in points if p.timestamp >= since]
-            
+
             return points
-    
+
     def get_current_value(self, name: str) -> Optional[float]:
         """Get current value for a metric."""
         with self._lock:
@@ -175,17 +179,17 @@
                 points = self._metrics[name]
                 return points[-1].value if points else None
             return None
-    
+
     def get_metric_summary(self, name: str, time_window: Optional[float] = None) -> Dict[str, Any]:
         """Get summary statistics for a metric."""
         since = time.time() - time_window if time_window else None
         points = self.get_metric_history(name, since)
-        
+
         if not points:
             return {"count": 0}
-        
+
         values = [p.value for p in points]
-        
+
         return {
             "count": len(values),
             "min": min(values),
@@ -195,7 +199,7 @@
             "first_timestamp": points[0].timestamp,
             "last_timestamp": points[-1].timestamp
         }
-    
+
     def get_all_metrics_summary(self) -> Dict[str, Dict[str, Any]]:
         """Get summary for all metrics."""
         with self._lock:
@@ -203,7 +207,7 @@
             for name in self._metrics:
                 summaries[name] = self.get_metric_summary(name)
             return summaries
-    
+
     def clear_metrics(self, older_than: Optional[float] = None):
         """Clear metrics older than specified timestamp."""
         with self._lock:
@@ -221,29 +225,29 @@
 
 class HealthMonitor:
     """Monitors system health and performs health checks."""
-    
+
     def __init__(self):
         self._health_checks: Dict[str, HealthCheck] = {}
         self._check_functions: Dict[str, callable] = {}
         self._lock = threading.RLock()
-        
+
         # Register default health checks
         self._register_default_checks()
-        
+
         logging.getLogger(__name__).info("ü©∫ Health monitor initialized")
-    
+
     def _register_default_checks(self):
         """Register default health checks."""
         self.register_check("system_time", self._check_system_time)
         self.register_check("memory_usage", self._check_memory_usage)
         self.register_check("disk_space", self._check_disk_space)
-    
+
     def register_check(self, name: str, check_function: callable):
         """Register a health check function."""
         with self._lock:
             self._check_functions[name] = check_function
         logging.getLogger(__name__).debug("Registered health check: %s", name)
-    
+
     def run_check(self, name: str) -> HealthCheck:
         """Run a specific health check."""
         if name not in self._check_functions:
@@ -253,12 +257,12 @@
                 message=f"Unknown health check: {name}",
                 timestamp=time.time()
             )
-        
+
         start_time = time.time()
         try:
             check_function = self._check_functions[name]
             result = check_function()
-            
+
             if isinstance(result, HealthCheck):
                 result.duration_ms = (time.time() - start_time) * 1000
                 return result
@@ -281,33 +285,36 @@
                 duration_ms=(time.time() - start_time) * 1000,
                 details={"error": str(e)}
             )
-    
+
     def run_all_checks(self) -> Dict[str, HealthCheck]:
         """Run all registered health checks."""
         results = {}
-        
+
         for name in self._check_functions:
             results[name] = self.run_check(name)
-        
+
         with self._lock:
             self._health_checks.update(results)
-        
+
         return results
-    
+
     def get_health_status(self) -> Dict[str, Any]:
         """Get overall health status."""
         checks = self.run_all_checks()
-        
-        healthy_count = sum(1 for check in checks.values() if check.status == "healthy")
-        warning_count = sum(1 for check in checks.values() if check.status == "warning")
-        unhealthy_count = sum(1 for check in checks.values() if check.status == "unhealthy")
-        
+
+        healthy_count = sum(1 for check in checks.values()
+                            if check.status == "healthy")
+        warning_count = sum(1 for check in checks.values()
+                            if check.status == "warning")
+        unhealthy_count = sum(1 for check in checks.values()
+                              if check.status == "unhealthy")
+
         overall_status = "healthy"
         if unhealthy_count > 0:
             overall_status = "unhealthy"
         elif warning_count > 0:
             overall_status = "warning"
-        
+
         return {
             "status": overall_status,
             "timestamp": time.time(),
@@ -319,12 +326,12 @@
                 "unhealthy": unhealthy_count
             }
         }
-    
+
     def _check_system_time(self) -> HealthCheck:
         """Check system time is reasonable."""
         current_time = time.time()
         year = datetime.fromtimestamp(current_time).year
-        
+
         if year < 2020 or year > 2030:
             return HealthCheck(
                 name="system_time",
@@ -332,30 +339,30 @@
                 message=f"System time appears incorrect: {datetime.fromtimestamp(current_time)}",
                 timestamp=current_time
             )
-        
+
         return HealthCheck(
             name="system_time",
             status="healthy",
             message="System time is reasonable",
             timestamp=current_time
         )
-    
+
     def _check_memory_usage(self) -> HealthCheck:
         """Check memory usage."""
         import psutil
         memory = psutil.virtual_memory()
         usage_percent = memory.percent
-        
+
         status = "healthy"
         message = f"Memory usage: {usage_percent:.1f}%"
-        
+
         if usage_percent > 90:
             status = "unhealthy"
             message = f"High memory usage: {usage_percent:.1f}%"
         elif usage_percent > 75:
             status = "warning"
             message = f"Elevated memory usage: {usage_percent:.1f}%"
-        
+
         return HealthCheck(
             name="memory_usage",
             status=status,
@@ -367,25 +374,26 @@
                 "total_gb": memory.total / (1024**3)
             }
         )
-    
+
     def _check_disk_space(self) -> HealthCheck:
         """Check disk space."""
         try:
             import shutil
             import os
-            total, used, free = shutil.disk_usage(getattr(self, "ROOT_PATH", Path.cwd().anchor))
+            total, used, free = shutil.disk_usage(
+                getattr(self, "ROOT_PATH", Path.cwd().anchor))
             free_percent = (free / total) * 100
-            
+
             status = "healthy"
             message = f"Disk space: {free_percent:.1f}% free"
-            
+
             if free_percent < 5:
                 status = "unhealthy"
                 message = f"Low disk space: {free_percent:.1f}% free"
             elif free_percent < 15:
                 status = "warning"
                 message = f"Limited disk space: {free_percent:.1f}% free"
-            
+
             return HealthCheck(
                 name="disk_space",
                 status=status,
@@ -408,15 +416,15 @@
 
 class PipelineMonitor:
     """Monitors pipeline execution and tracks runs."""
-    
+
     def __init__(self):
         self._current_run: Optional[PipelineRun] = None
         self._run_history: List[PipelineRun] = []
         self._max_history = 100
         self._lock = threading.RLock()
-        
+
         logging.getLogger(__name__).info("üîç Pipeline monitor initialized")
-    
+
     def start_run(self, run_id: str) -> PipelineRun:
         """Start monitoring a new pipeline run."""
         with self._lock:
@@ -425,38 +433,39 @@
                 self._current_run.status = "failed"
                 self._current_run.end_time = time.time()
                 self._current_run.errors.append("Run interrupted by new run")
-            
+
             self._current_run = PipelineRun(
                 run_id=run_id,
                 start_time=time.time()
             )
-            
-            logging.getLogger(__name__).info("üöÄ Started pipeline run: %s", run_id)
+
+            logging.getLogger(__name__).info(
+                "üöÄ Started pipeline run: %s", run_id)
             return self._current_run
-    
+
     def end_run(self, status: str = "completed"):
         """End the current pipeline run."""
         with self._lock:
             if self._current_run:
                 self._current_run.end_time = time.time()
                 self._current_run.status = status
-                
+
                 # Add to history
                 self._run_history.append(self._current_run)
-                
+
                 # Trim history
                 if len(self._run_history) > self._max_history:
                     self._run_history = self._run_history[-self._max_history:]
-                
+
                 logging.getLogger(__name__).info(
                     "üèÅ Ended pipeline run: %s (status=%s, duration=%.2fs)",
                     self._current_run.run_id,
                     status,
                     self._current_run.duration
                 )
-                
+
                 self._current_run = None
-    
+
     def record_source_processed(self, success: bool, records: int = 0, bytes_processed: int = 0, error: Optional[str] = None):
         """Record processing of a source."""
         with self._lock:
@@ -464,19 +473,19 @@
                 self._current_run.sources_processed += 1
                 self._current_run.total_records += records
                 self._current_run.total_bytes += bytes_processed
-                
+
                 if success:
                     self._current_run.sources_successful += 1
                 else:
                     self._current_run.sources_failed += 1
                     if error:
                         self._current_run.errors.append(error)
-    
+
     def get_current_run(self) -> Optional[PipelineRun]:
         """Get current pipeline run."""
         with self._lock:
             return self._current_run
-    
+
     def get_run_history(self, limit: Optional[int] = None) -> List[PipelineRun]:
         """Get pipeline run history."""
         with self._lock:
@@ -484,27 +493,31 @@
             if limit:
                 history = history[-limit:]
             return history
-    
+
     def get_run_statistics(self, days: int = 7) -> Dict[str, Any]:
         """Get pipeline run statistics for the last N days."""
         cutoff_time = time.time() - (days * 24 * 3600)
-        
-        with self._lock:
-            recent_runs = [run for run in self._run_history if run.start_time >= cutoff_time]
-        
+
+        with self._lock:
+            recent_runs = [
+                run for run in self._run_history if run.start_time >= cutoff_time]
+
         if not recent_runs:
             return {"total_runs": 0, "period_days": days}
-        
-        completed_runs = [run for run in recent_runs if run.status == "completed"]
+
+        completed_runs = [
+            run for run in recent_runs if run.status == "completed"]
         failed_runs = [run for run in recent_runs if run.status == "failed"]
-        
+
         total_records = sum(run.total_records for run in recent_runs)
         total_bytes = sum(run.total_bytes for run in recent_runs)
-        total_duration = sum(run.duration for run in recent_runs if run.end_time)
-        
+        total_duration = sum(
+            run.duration for run in recent_runs if run.end_time)
+
         avg_duration = total_duration / len(recent_runs) if recent_runs else 0
-        success_rate = (len(completed_runs) / len(recent_runs)) * 100 if recent_runs else 0
-        
+        success_rate = (len(completed_runs) / len(recent_runs)
+                        ) * 100 if recent_runs else 0
+
         return {
             "period_days": days,
             "total_runs": len(recent_runs),
@@ -521,7 +534,7 @@
 
 class StructuredLogger:
     """Enhanced logger with structured logging and metrics integration."""
-    
+
     def __init__(
         self,
         name: str,
@@ -531,51 +544,55 @@
         self.logger = logging.getLogger(name)
         self.metrics = metrics_collector
         self.monitor = pipeline_monitor
-    
+
     def info(self, message: str, **kwargs):
         """Log info message with structured data."""
         self._log_with_metrics(logging.INFO, message, **kwargs)
-    
+
     def warning(self, message: str, **kwargs):
         """Log warning message with structured data."""
         self._log_with_metrics(logging.WARNING, message, **kwargs)
         if self.metrics:
             self.metrics.increment_counter("log.warnings")
-    
+
     def error(self, message: str, error: Optional[Exception] = None, **kwargs):
         """Log error message with structured data."""
         if error:
             kwargs["error_type"] = type(error).__name__
             kwargs["error_message"] = str(error)
-            
+
             if isinstance(error, ETLError):
                 kwargs["error_context"] = error.context
                 kwargs["error_recoverable"] = error.recoverable
                 kwargs["error_source"] = error.source_name
-        
-        self._log_with_metrics(logging.ERROR, message, exc_info=error, **kwargs)
-        
+
+        self._log_with_metrics(logging.ERROR, message,
+                               exc_info=error, **kwargs)
+
         if self.metrics:
             self.metrics.increment_counter("log.errors")
-        
+
         if self.monitor and error:
-            self.monitor.record_source_processed(success=False, error=str(error))
-    
+            self.monitor.record_source_processed(
+                success=False, error=str(error))
+
     def debug(self, message: str, **kwargs):
         """Log debug message with structured data."""
         self._log_with_metrics(logging.DEBUG, message, **kwargs)
-    
+
     def _log_with_metrics(self, level: int, message: str, **kwargs):
         """Log message and record metrics."""
         # Add structured data as extra fields
         extra = {k: v for k, v in kwargs.items() if not k.startswith('exc_')}
-        
+
         # Log the message
-        self.logger.log(level, message, extra=extra, **{k: v for k, v in kwargs.items() if k.startswith('exc_')})
-        
+        self.logger.log(level, message, extra=extra, **
+                        {k: v for k, v in kwargs.items() if k.startswith('exc_')})
+
         # Record metrics
         if self.metrics:
-            self.metrics.increment_counter(f"log.{logging.getLevelName(level).lower()}")
+            self.metrics.increment_counter(
+                f"log.{logging.getLevelName(level).lower()}")
 
 
 def setup_structured_logging(
@@ -588,13 +605,13 @@
     """Set up structured logging configuration."""
     log_dir = Path(log_dir)
     log_dir.mkdir(parents=True, exist_ok=True)
-    
+
     # Create formatters
     structured_formatter = StructuredFormatter()
     console_formatter = logging.Formatter(
         '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
     )
-    
+
     # Create handlers
     # File handler for structured JSON logs
     json_log_file = log_dir / f"{app_name}.json"
@@ -604,7 +621,7 @@
         backupCount=backup_count
     )
     json_handler.setFormatter(structured_formatter)
-    
+
     # File handler for human-readable logs
     text_log_file = log_dir / f"{app_name}.log"
     text_handler = logging.handlers.RotatingFileHandler(
@@ -613,23 +630,23 @@
         backupCount=backup_count
     )
     text_handler.setFormatter(console_formatter)
-    
+
     # Console handler
     console_handler = logging.StreamHandler()
     console_handler.setFormatter(console_formatter)
-    
+
     # Configure root logger
     root_logger = logging.getLogger()
     root_logger.setLevel(getattr(logging, level.upper()))
-    
+
     # Clear existing handlers
     root_logger.handlers.clear()
-    
+
     # Add handlers
     root_logger.addHandler(json_handler)
     root_logger.addHandler(text_handler)
     root_logger.addHandler(console_handler)
-    
+
     logging.getLogger(__name__).info("üìù Structured logging configured")
     return root_logger
 
@@ -670,4 +687,4 @@
         name,
         metrics_collector=get_metrics_collector(),
         pipeline_monitor=get_pipeline_monitor()
-    )
\ No newline at end of file
+    )
--- original/./etl/loaders/__init__.py
+++ fixed/./etl/loaders/__init__.py
@@ -2,4 +2,4 @@
 
 from .filegdb import ArcPyFileGDBLoader  # noqa: F401
 
-__all__ = ["ArcPyFileGDBLoader"]
\ No newline at end of file
+__all__ = ["ArcPyFileGDBLoader"]
--- original/./etl/loaders/gpkg_loader.py
+++ fixed/./etl/loaders/gpkg_loader.py
@@ -21,29 +21,31 @@
 _MAIN_RE: Final = re.compile(r"^main\.", re.IGNORECASE)
 
 
-
 @safe_arcpy_operation
 def copy_gpkg_feature_class(input_fc_name: str, target_name: str, gdb_path: Path, summary: Summary) -> None:
     """üì¶ Copy a single feature class from GPKG to GDB with comprehensive retry logic."""
     lg_sum = logging.getLogger("summary")
-    
+
     # Strategy 1: Try with original listed name
     if _attempt_copy_with_name(input_fc_name, target_name, gdb_path):
-        log.info("‚úÖ SUCCESS: Copied GPKG FC '%s' to '%s'", input_fc_name, target_name)
+        log.info("‚úÖ SUCCESS: Copied GPKG FC '%s' to '%s'",
+                 input_fc_name, target_name)
         lg_sum.info("   üìÑ GPKG ‚ûú staged : %s", target_name)
         summary.log_staging("done")
         return
-    
+
     # Strategy 2: Try with stripped name (remove 'main.' prefix)
     stripped_name = _MAIN_RE.sub("", input_fc_name)
     if stripped_name != input_fc_name:
-        log.info("üîÑ Retrying with stripped name: '%s' ‚Üí '%s'", input_fc_name, stripped_name)
+        log.info("üîÑ Retrying with stripped name: '%s' ‚Üí '%s'",
+                 input_fc_name, stripped_name)
         if _attempt_copy_with_name(stripped_name, target_name, gdb_path):
-            log.info("‚úÖ SUCCESS (retry): Copied GPKG FC '%s' to '%s'", stripped_name, target_name)
+            log.info("‚úÖ SUCCESS (retry): Copied GPKG FC '%s' to '%s'",
+                     stripped_name, target_name)
             lg_sum.info("   üìÑ GPKG ‚ûú staged : %s", target_name)
             summary.log_staging("done")
             return
-    
+
     # Strategy 3: Try with full workspace path
     current_workspace = arcpy.env.workspace  # type: ignore[attr-defined]
     if current_workspace:
@@ -51,11 +53,12 @@
             full_path = f"{current_workspace}\\{candidate_name}"
             log.info("üîÑ Trying full path: '%s'", full_path)
             if _attempt_copy_with_name(full_path, target_name, gdb_path):
-                log.info("‚úÖ SUCCESS (full path): Copied GPKG FC '%s' to '%s'", full_path, target_name)
+                log.info(
+                    "‚úÖ SUCCESS (full path): Copied GPKG FC '%s' to '%s'", full_path, target_name)
                 lg_sum.info("   üìÑ GPKG ‚ûú staged : %s", target_name)
                 summary.log_staging("done")
                 return
-    
+
     # All strategies failed
     log.error("‚ùå All copy strategies failed for GPKG FC '%s'", input_fc_name)
     summary.log_staging("error")
@@ -68,10 +71,12 @@
     try:
         # First validate that the source exists
         if not arcpy.Exists(source_name):
-            log.debug("üîç Source '%s' does not exist, skipping attempt", source_name)
+            log.debug(
+                "üîç Source '%s' does not exist, skipping attempt", source_name)
             return False
-        
-        log.info("üì• Copying GPKG FC ('%s') ‚Üí GDB:/'%s'", source_name, target_name)
+
+        log.info("üì• Copying GPKG FC ('%s') ‚Üí GDB:/'%s'",
+                 source_name, target_name)
         with arcpy.EnvManager(overwriteOutput=True):
             arcpy.conversion.FeatureClassToFeatureClass(
                 in_features=source_name,
@@ -79,16 +84,16 @@
                 out_name=target_name
             )
         return True
-        
+
     except arcpy.ExecuteError as arc_error:
         arcpy_messages: str = arcpy.GetMessages(2)
-        log.debug("üîÑ Copy attempt failed for '%s': %s. ArcPy Messages: %s", 
-                 source_name, arc_error, arcpy_messages)
+        log.debug("üîÑ Copy attempt failed for '%s': %s. ArcPy Messages: %s",
+                  source_name, arc_error, arcpy_messages)
         return False
-        
+
     except Exception as generic_error:
-        log.debug("üîÑ Copy attempt failed for '%s' with unexpected error: %s", 
-                 source_name, generic_error)
+        log.debug("üîÑ Copy attempt failed for '%s' with unexpected error: %s",
+                  source_name, generic_error)
         return False
 
 
@@ -102,7 +107,7 @@
 ) -> None:
     """üì¶ Process a single feature class from GPKG."""
     stem_for_output_naming: str = _MAIN_RE.sub("", fc_name_listed_by_arcpy)
-    
+
     if normalized_include_filter:
         comparable_fc_name: str = stem_for_output_naming.lower()
         if comparable_fc_name not in normalized_include_filter:
@@ -112,15 +117,16 @@
         else:
             log.info("‚úÖ GPKG FC '%s' (normalized: '%s') is in include filter. Proceeding.",
                      fc_name_listed_by_arcpy, comparable_fc_name)
-    
+
     if stem_for_output_naming != fc_name_listed_by_arcpy:
         log.info("Stripped 'main.' from '%s' ‚Üí '%s' for output naming",
                  fc_name_listed_by_arcpy, stem_for_output_naming)
-    
+
     base_name: str = generate_fc_name(authority, stem_for_output_naming)
     tgt_name: str = ensure_unique_name(base_name, used_names_set)
-    
-    copy_gpkg_feature_class(fc_name_listed_by_arcpy, tgt_name, gdb_path, summary)
+
+    copy_gpkg_feature_class(fc_name_listed_by_arcpy,
+                            tgt_name, gdb_path, summary)
 
 
 def process_gpkg_contents(
@@ -134,28 +140,31 @@
     """üì¶ Copy GPKG contents to staging GDB with comprehensive error handling."""
     log.info("üì¶ Processing GeoPackage: %s (Authority: '%s', Include Filter: %s)",
              gpkg_file_path.name, authority, include_filter or "None")
-    
+
     if not gpkg_file_path.exists():
         log.error("‚ùå GeoPackage file does not exist: %s", gpkg_file_path)
         return
-    
+
     # Use context manager for safe workspace management
     with arcpy_workspace(gpkg_file_path, overwrite_output=True):
         try:
             feature_classes_in_gpkg: List[str] = arcpy.ListFeatureClasses()
             if not feature_classes_in_gpkg:
-                log.info("‚ÑπÔ∏è No feature classes found in GeoPackage: %s", gpkg_file_path.name)
-                return
-            
+                log.info("‚ÑπÔ∏è No feature classes found in GeoPackage: %s",
+                         gpkg_file_path.name)
+                return
+
             log.info("Found %d feature classes in %s: %s",
                      len(feature_classes_in_gpkg), gpkg_file_path.name, feature_classes_in_gpkg)
-            
+
             # Normalize include filter for comparison BEFORE validation
             normalized_include_filter: Optional[Set[str]] = None
             if include_filter:
-                normalized_include_filter = {_MAIN_RE.sub("", item).lower() for item in include_filter if item}
-                log.info("Normalized include filter for %s: %s", gpkg_file_path.name, normalized_include_filter)
-                
+                normalized_include_filter = {_MAIN_RE.sub(
+                    "", item).lower() for item in include_filter if item}
+                log.info("Normalized include filter for %s: %s",
+                         gpkg_file_path.name, normalized_include_filter)
+
                 # Pre-filter feature classes based on include filter
                 if normalized_include_filter:
                     filtered_feature_classes = []
@@ -163,17 +172,21 @@
                         stem_for_comparison = _MAIN_RE.sub("", fc_name).lower()
                         if stem_for_comparison in normalized_include_filter:
                             filtered_feature_classes.append(fc_name)
-                            log.info("‚úÖ Including GPKG FC '%s' (matches filter)", fc_name)
+                            log.info(
+                                "‚úÖ Including GPKG FC '%s' (matches filter)", fc_name)
                         else:
-                            log.info("‚è≠Ô∏è Excluding GPKG FC '%s' (not in include filter)", fc_name)
-                    
+                            log.info(
+                                "‚è≠Ô∏è Excluding GPKG FC '%s' (not in include filter)", fc_name)
+
                     feature_classes_in_gpkg = filtered_feature_classes
-                    log.info("üì¶ After applying include filter: %d feature classes remain", len(feature_classes_in_gpkg))
-            
+                    log.info("üì¶ After applying include filter: %d feature classes remain", len(
+                        feature_classes_in_gpkg))
+
             if not feature_classes_in_gpkg:
-                log.info("‚ÑπÔ∏è No feature classes match the include filter in GeoPackage: %s", gpkg_file_path.name)
-                return
-            
+                log.info(
+                    "‚ÑπÔ∏è No feature classes match the include filter in GeoPackage: %s", gpkg_file_path.name)
+                return
+
             # Validate that feature classes actually exist and can be accessed
             valid_feature_classes: List[str] = []
             for fc_name in feature_classes_in_gpkg:
@@ -181,21 +194,23 @@
                     valid_feature_classes.append(fc_name)
                     log.debug("‚úÖ Validated GPKG FC: %s", fc_name)
                 else:
-                    log.warning("‚ö†Ô∏è Skipping inaccessible GPKG FC: %s", fc_name)
-            
+                    log.warning(
+                        "‚ö†Ô∏è Skipping inaccessible GPKG FC: %s", fc_name)
+
             if not valid_feature_classes:
-                log.warning("‚ö†Ô∏è No accessible feature classes found in GeoPackage: %s", gpkg_file_path.name)
-                return
-                
-            log.info("üì¶ Processing %d valid feature classes from %s", 
+                log.warning(
+                    "‚ö†Ô∏è No accessible feature classes found in GeoPackage: %s", gpkg_file_path.name)
+                return
+
+            log.info("üì¶ Processing %d valid feature classes from %s",
                      len(valid_feature_classes), gpkg_file_path.name)
-            
+
             for fc_name_listed_by_arcpy in valid_feature_classes:
                 process_gpkg_feature_class(
                     fc_name_listed_by_arcpy, authority, gdb_path, used_names_set,
                     summary, normalized_include_filter
                 )
-                
+
         except Exception as gpkg_processing_error:
             log.error("‚ùå Failed to list or process feature classes in GeoPackage '%s': %s",
                       gpkg_file_path.name, gpkg_processing_error, exc_info=True)
@@ -205,30 +220,33 @@
     """üîç Validate that a GPKG feature class can be accessed."""
     # Try multiple approaches to validate GPKG feature class existence
     candidates = [fc_name]
-    
+
     # Also try stripped name if it has 'main.' prefix
     stripped_name = _MAIN_RE.sub("", fc_name)
     if stripped_name != fc_name:
         candidates.append(stripped_name)
-    
+
     # Try with full workspace path
     current_workspace = arcpy.env.workspace  # type: ignore[attr-defined]
     if current_workspace:
         for candidate in [fc_name, stripped_name]:
             candidates.append(f"{current_workspace}\\{candidate}")
-    
+
     for candidate in candidates:
         try:
             if arcpy.Exists(candidate):
-                log.debug("üîç GPKG FC validation SUCCESS: '%s' accessible as '%s'", fc_name, candidate)
+                log.debug(
+                    "üîç GPKG FC validation SUCCESS: '%s' accessible as '%s'", fc_name, candidate)
                 return True
             # Also try Describe as a secondary check
             desc = arcpy.Describe(candidate)
             if desc:
-                log.debug("üîç GPKG FC validation SUCCESS via Describe: '%s' accessible as '%s'", fc_name, candidate)
+                log.debug(
+                    "üîç GPKG FC validation SUCCESS via Describe: '%s' accessible as '%s'", fc_name, candidate)
                 return True
         except Exception:
             continue
-    
-    log.debug("üîç GPKG FC validation FAILED: '%s' not accessible via any method", fc_name)
-    return False
\ No newline at end of file
+
+    log.debug(
+        "üîç GPKG FC validation FAILED: '%s' not accessible via any method", fc_name)
+    return False
--- original/./etl/loaders/shapefile_loader.py
+++ fixed/./etl/loaders/shapefile_loader.py
@@ -29,7 +29,7 @@
 
     temp_dir = paths.TEMP / f"shp_{uuid.uuid4().hex[:8]}"
     temp_dir.mkdir(parents=True, exist_ok=True)
-    
+
     # Track temporary directory for cleanup
     track_temp_path(temp_dir)
 
@@ -57,7 +57,8 @@
 
         # Calculate the staging directory path for this source
         source_staging_dir = (
-            paths.STAGING / self.src.authority / sanitize_for_filename(self.src.name)
+            paths.STAGING / self.src.authority /
+            sanitize_for_filename(self.src.name)
         )
 
         if not source_staging_dir.exists():
@@ -118,7 +119,8 @@
             )
             # Track temporary directory for cleanup
             track_temp_path(temp_unzip_dir)
-            log.info("üì¶ Unzipping '%s' to '%s'", item_path.name, temp_unzip_dir)
+            log.info("üì¶ Unzipping '%s' to '%s'",
+                     item_path.name, temp_unzip_dir)
             with zipfile.ZipFile(item_path, "r") as zip_ref:
                 zip_ref.extractall(temp_unzip_dir)
             item_dir = temp_unzip_dir
@@ -134,7 +136,8 @@
             return
 
         log.info(
-            "üìê Found %d shapefile(s) in item dir '%s'.", len(shapefiles), item_dir.name
+            "üìê Found %d shapefile(s) in item dir '%s'.", len(
+                shapefiles), item_dir.name
         )
         for shp_file in shapefiles:
             self.process_shapefile(shp_file, used_names)
@@ -158,7 +161,8 @@
                     shp_file_path, self.src.authority
                 )
 
-            fc_name_base = generate_fc_name(self.src.authority, working_path.stem)
+            fc_name_base = generate_fc_name(
+                self.src.authority, working_path.stem)
             target_fc_name = ensure_unique_name(
                 base_name=fc_name_base, used_names=used_names, max_length=60
             )
@@ -196,7 +200,8 @@
                 try:
                     untrack_temp_path(temp_copy_dir)
                     shutil.rmtree(temp_copy_dir, ignore_errors=True)
-                    log.debug("Cleaned up temporary directory: %s", temp_copy_dir)
+                    log.debug("Cleaned up temporary directory: %s",
+                              temp_copy_dir)
                 except Exception as cleanup_error:
-                    log.warning("Failed to cleanup temporary directory %s: %s", 
-                               temp_copy_dir, cleanup_error)
+                    log.warning("Failed to cleanup temporary directory %s: %s",
+                                temp_copy_dir, cleanup_error)
--- original/./etl/loaders/geojson_loader.py
+++ fixed/./etl/loaders/geojson_loader.py
@@ -32,7 +32,8 @@
             features = data.get("features", [])
             log.debug("üîç Found %d features in FeatureCollection", len(features))
 
-            for i, feature in enumerate(features[:10]):  # Sample first 10 features
+            # Sample first 10 features
+            for i, feature in enumerate(features[:10]):
                 geom = feature.get("geometry", {})
                 geom_type = geom.get("type")
                 if geom_type:
@@ -173,7 +174,8 @@
             except Exception as verify_error:
                 log.warning("‚ö†Ô∏è Could not verify output FC: %s", verify_error)
         else:
-            log.error("‚ùå Output feature class was not created: %s", out_fc_full_path)
+            log.error("‚ùå Output feature class was not created: %s",
+                      out_fc_full_path)
 
         lg_sum.info("   üìÑ JSON ‚ûú staged : %s", tgt_name)
         summary.log_staging("done")
@@ -189,7 +191,8 @@
             exc_info=True,
         )
         summary.log_staging("error")
-        summary.log_error(json_file_path.name, f"JSONToFeatures failed: {arc_error}")
+        summary.log_error(json_file_path.name,
+                          f"JSONToFeatures failed: {arc_error}")
     except Exception as generic_error:
         log.error(
             "‚ùå Unexpected error processing JSON/GeoJSON %s: %s",
@@ -198,4 +201,5 @@
             exc_info=True,
         )
         summary.log_staging("error")
-        summary.log_error(json_file_path.name, f"Unexpected error: {generic_error}")
+        summary.log_error(json_file_path.name,
+                          f"Unexpected error: {generic_error}")
--- original/./etl/loaders/filegdb.py
+++ fixed/./etl/loaders/filegdb.py
@@ -40,16 +40,19 @@
         if sources_yaml_path:
             self._load_sources_configuration(sources_yaml_path)
         else:
-            log.warning("‚ö†Ô∏è sources_yaml_path not provided. Using fallback globbing.")
+            log.warning(
+                "‚ö†Ô∏è sources_yaml_path not provided. Using fallback globbing.")
 
     def _load_sources_configuration(self, sources_yaml_path: Path) -> None:
         """üìã Load and validate sources configuration from YAML file."""
         try:
             self.sources = Source.load_all(sources_yaml_path)
             if not self.sources and sources_yaml_path.exists():
-                log.warning("‚ö†Ô∏è Source.load_all returned empty list. Check YAML format.")
+                log.warning(
+                    "‚ö†Ô∏è Source.load_all returned empty list. Check YAML format.")
             elif not sources_yaml_path.exists():
-                log.warning("‚ö†Ô∏è Sources YAML file does not exist: %s", sources_yaml_path)
+                log.warning(
+                    "‚ö†Ô∏è Sources YAML file does not exist: %s", sources_yaml_path)
         except Exception as exc:
             log.error(
                 "‚ùå Failed to load sources from %s: %s",
@@ -106,7 +109,8 @@
                     source.name,
                     source.authority,
                 )
-                self._process_single_source(source, staging_root, used_names_set)
+                self._process_single_source(
+                    source, staging_root, used_names_set)
             except Exception as exc:
                 log.error(
                     "‚ùå Failed to process source '%s': %s",
@@ -120,9 +124,11 @@
     ) -> None:
         """üîÑ Process a single source based on its type."""
         source_staging_dir = (
-            staging_root / source.authority / sanitize_for_filename(source.name)
+            staging_root / source.authority /
+            sanitize_for_filename(source.name)
         )
-        normalized_data_type = self._normalize_staged_data_type(source.staged_data_type)
+        normalized_data_type = self._normalize_staged_data_type(
+            source.staged_data_type)
 
         log.info(
             "üîç Processing source '%s': staged_data_type='%s', normalized='%s'",
@@ -137,9 +143,11 @@
                 "üì¶ Source '%s' configured for GPKG - using GPKG loader exclusively",
                 source.name,
             )
-            self._handle_gpkg_source(source, source_staging_dir, used_names_set)
+            self._handle_gpkg_source(
+                source, source_staging_dir, used_names_set)
         elif normalized_data_type in ("geojson", "json"):
-            self._handle_geojson_source(source, source_staging_dir, used_names_set)
+            self._handle_geojson_source(
+                source, source_staging_dir, used_names_set)
         elif normalized_data_type == "shapefile_collection":
             log.info("üìê Handling shapefile collection source '%s'", source.name)
             loader = ShapefileLoader(src=source)
@@ -277,7 +285,8 @@
         self, source: Source, staging_root: Path, used_names_set: Set[str]
     ) -> None:
         """üìê Process multi-part shapefile collections based on include list."""
-        log.info("üìê Processing multi-part shapefile collection for '%s'.", source.name)
+        log.info(
+            "üìê Processing multi-part shapefile collection for '%s'.", source.name)
 
         for included_item_stem in source.include or []:
             sanitized_item_stem = sanitize_for_filename(included_item_stem)
@@ -324,7 +333,8 @@
         self, staging_root: Path, used_names_set: Set[str]
     ) -> None:
         """üîç Perform fallback globbing when no source configuration is available."""
-        log.warning("‚ö†Ô∏è No Source configurations loaded. Using fallback globbing.")
+        log.warning(
+            "‚ö†Ô∏è No Source configurations loaded. Using fallback globbing.")
 
         self._glob_and_load_shapefiles(staging_root, used_names_set)
         self._glob_and_load_geopackages(staging_root, used_names_set)
@@ -336,7 +346,8 @@
         """üìê Fallback: glob and load all shapefiles."""
         shp_files = list(staging_root.rglob("*.shp"))
         if shp_files:
-            log.info("üîç Fallback: Found %d shapefile(s) to process.", len(shp_files))
+            log.info("üîç Fallback: Found %d shapefile(s) to process.",
+                     len(shp_files))
             for shp_file_path in shp_files:
                 derived_authority = derive_authority_from_path(
                     shp_file_path, staging_root
@@ -345,7 +356,8 @@
                 # Fallback globbing is complex with the new model.
                 # For now, we'll assume a source-based approach.
                 # A proper fix would require creating temporary Source objects.
-                log.warning("Fallback globbing for shapefiles is not fully supported with the new loader structure.")
+                log.warning(
+                    "Fallback globbing for shapefiles is not fully supported with the new loader structure.")
 
     def _glob_and_load_geopackages(
         self, staging_root: Path, used_names_set: Set[str]
@@ -353,7 +365,8 @@
         """üì¶ Fallback: glob and load all geopackages."""
         gpkg_files = list(staging_root.rglob("*.gpkg"))
         if gpkg_files:
-            log.info("üîç Fallback: Found %d GeoPackage(s) to process.", len(gpkg_files))
+            log.info("üîç Fallback: Found %d GeoPackage(s) to process.",
+                     len(gpkg_files))
             for gpkg_file_path in gpkg_files:
                 derived_authority = derive_authority_from_path(
                     gpkg_file_path, staging_root
@@ -398,7 +411,8 @@
 
     def load_from_staging(self, staging_root: Path) -> None:
         """üîÑ Compatibility method that mimics the old interface."""
-        log.info("üîÑ Starting FileGDB loading from staging directory: %s", staging_root)
+        log.info(
+            "üîÑ Starting FileGDB loading from staging directory: %s", staging_root)
 
         # Reset GDB to start fresh
         reset_gdb(self.gdb_path)
--- original/./etl/exceptions/compat.py
+++ fixed/./etl/exceptions/compat.py
@@ -134,4 +134,4 @@
         context=ErrorContext(
             operation='configuration'
         )
-    )
\ No newline at end of file
+    )
--- original/./etl/exceptions/core.py
+++ fixed/./etl/exceptions/core.py
@@ -12,7 +12,7 @@
     LOW = "low"           # Warnings, can continue
     MEDIUM = "medium"     # Errors, but recoverable
     HIGH = "high"        # Critical errors, stop current operation
-    CRITICAL = "critical" # Pipeline-breaking errors
+    CRITICAL = "critical"  # Pipeline-breaking errors
 
 
 class ErrorCategory(Enum):
@@ -33,7 +33,7 @@
     timestamp: float = field(default_factory=time.time)
     retry_count: int = 0
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary for logging."""
         return {
@@ -49,7 +49,7 @@
 
 class ETLError(Exception):
     """Base exception for all ETL pipeline errors with enhanced functionality."""
-    
+
     def __init__(
         self,
         message: str,
@@ -69,26 +69,26 @@
         self.recoverable = recoverable
         self.retry_after = retry_after
         self.cause = cause
-        
+
         # Set the cause for proper exception chaining
         if cause:
             self.__cause__ = cause
-    
+
     def __str__(self) -> str:
         """Enhanced string representation."""
         parts = [self.message]
-        
+
         if self.context.source_name:
             parts.append(f"[source: {self.context.source_name}]")
-        
+
         if self.context.operation:
             parts.append(f"[operation: {self.context.operation}]")
-        
+
         if self.context.retry_count > 0:
             parts.append(f"[retry: {self.context.retry_count}]")
-        
+
         return " ".join(parts)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert error to dictionary for structured logging."""
         return {
@@ -106,7 +106,7 @@
 # 1. NETWORK ERRORS (replaces HTTPError, NetworkError, ConnectionError, TimeoutError, RateLimitError)
 class NetworkError(ETLError):
     """Network-related errors including HTTP, connection, and timeout issues."""
-    
+
     def __init__(
         self,
         message: str,
@@ -123,12 +123,12 @@
             context.metadata['timeout'] = timeout
         if status_code:
             context.metadata['status_code'] = status_code
-        
+
         # Determine severity and recoverability based on status code
         severity = ErrorSeverity.MEDIUM
         recoverable = True
         retry_after = None
-        
+
         if status_code:
             if status_code == 429:  # Rate limit
                 retry_after = 60.0
@@ -139,7 +139,7 @@
             elif 400 <= status_code < 500:  # Client errors (except 429)
                 recoverable = False
                 severity = ErrorSeverity.HIGH
-        
+
         super().__init__(
             message,
             severity=severity,
@@ -149,7 +149,7 @@
             retry_after=retry_after,
             **kwargs
         )
-        
+
         self.status_code = status_code
         self.url = url
         self.timeout = timeout
@@ -158,7 +158,7 @@
 # 2. DATA ERRORS (replaces DataError, DataFormatError, DataQualityError, ValidationError, GeospatialError)
 class DataError(ETLError):
     """Data-related errors including format, quality, and validation issues."""
-    
+
     def __init__(
         self,
         message: str,
@@ -175,7 +175,7 @@
             context.metadata['data_type'] = data_type
         if field_name:
             context.metadata['field_name'] = field_name
-        
+
         super().__init__(
             message,
             severity=ErrorSeverity.MEDIUM,
@@ -184,7 +184,7 @@
             recoverable=False,  # Data errors usually aren't recoverable
             **kwargs
         )
-        
+
         self.data_type = data_type
         self.file_path = file_path
         self.field_name = field_name
@@ -193,7 +193,7 @@
 # 3. SYSTEM ERRORS (replaces StorageError, ResourceError, PermissionError, DiskSpaceError)
 class SystemError(ETLError):
     """System-related errors including storage, resources, and permissions."""
-    
+
     def __init__(
         self,
         message: str,
@@ -211,17 +211,17 @@
             context.metadata['available'] = available
         if required is not None:
             context.metadata['required'] = required
-        
+
         # Determine severity based on resource type
         severity = ErrorSeverity.HIGH
         recoverable = False
-        
+
         if resource_type == "disk_space":
             severity = ErrorSeverity.CRITICAL
         elif resource_type == "memory":
             severity = ErrorSeverity.HIGH
             recoverable = True
-            
+
         super().__init__(
             message,
             severity=severity,
@@ -230,7 +230,7 @@
             recoverable=recoverable,
             **kwargs
         )
-        
+
         self.resource_type = resource_type
         self.available = available
         self.required = required
@@ -239,7 +239,7 @@
 # 4. CONFIGURATION ERRORS (replaces ConfigurationError)
 class ConfigurationError(ETLError):
     """Configuration-related errors."""
-    
+
     def __init__(
         self,
         message: str,
@@ -253,7 +253,7 @@
         context.file_path = config_file
         if config_key:
             context.metadata['config_key'] = config_key
-        
+
         super().__init__(
             message,
             severity=ErrorSeverity.CRITICAL,
@@ -262,7 +262,7 @@
             recoverable=False,
             **kwargs
         )
-        
+
         self.config_file = config_file
         self.config_key = config_key
 
@@ -270,7 +270,7 @@
 # 5. SOURCE ERRORS (replaces SourceError, SourceUnavailableError, SourceNotFoundError, AuthenticationError)
 class SourceError(ETLError):
     """Source-related errors including availability, authentication, and access."""
-    
+
     def __init__(
         self,
         message: str,
@@ -286,18 +286,18 @@
             context.metadata['source_type'] = source_type
         context.metadata['available'] = available
         context.metadata['authenticated'] = authenticated
-        
+
         # Determine severity and recoverability
         severity = ErrorSeverity.MEDIUM
         recoverable = True
         retry_after = None
-        
+
         if not available:
             retry_after = 300.0  # 5 minutes for unavailable sources
         elif not authenticated:
             recoverable = False
             severity = ErrorSeverity.HIGH
-            
+
         super().__init__(
             message,
             severity=severity,
@@ -307,7 +307,7 @@
             retry_after=retry_after,
             **kwargs
         )
-        
+
         self.source_type = source_type
         self.available = available
         self.authenticated = authenticated
@@ -316,7 +316,7 @@
 # 6. PROCESSING ERRORS (replaces TransformationError, GeoprocessingError, LoadError)
 class ProcessingError(ETLError):
     """Processing-related errors including transformation, geoprocessing, and loading."""
-    
+
     def __init__(
         self,
         message: str,
@@ -331,7 +331,7 @@
             context.metadata['process_type'] = process_type
         if stage:
             context.metadata['stage'] = stage
-        
+
         super().__init__(
             message,
             severity=ErrorSeverity.MEDIUM,
@@ -340,7 +340,7 @@
             recoverable=True,
             **kwargs
         )
-        
+
         self.process_type = process_type
         self.stage = stage
 
@@ -348,7 +348,7 @@
 # 7. PIPELINE ERRORS (replaces PipelineError, DependencyError, CircuitBreakerError)
 class PipelineError(ETLError):
     """Pipeline-level errors including dependencies and circuit breakers."""
-    
+
     def __init__(
         self,
         message: str,
@@ -363,7 +363,7 @@
             context.metadata['pipeline_stage'] = pipeline_stage
         if dependency:
             context.metadata['dependency'] = dependency
-        
+
         super().__init__(
             message,
             severity=ErrorSeverity.HIGH,
@@ -373,7 +373,7 @@
             retry_after=300.0,  # 5 minutes for pipeline errors
             **kwargs
         )
-        
+
         self.pipeline_stage = pipeline_stage
         self.dependency = dependency
 
@@ -381,7 +381,7 @@
 # 8. CONCURRENT ERRORS (new - for concurrent operations)
 class ConcurrentError(ETLError):
     """Concurrent operation errors including thread pool and task failures."""
-    
+
     def __init__(
         self,
         message: str,
@@ -399,7 +399,7 @@
             context.metadata['worker_count'] = worker_count
         if failed_tasks:
             context.metadata['failed_tasks'] = failed_tasks
-        
+
         super().__init__(
             message,
             severity=ErrorSeverity.MEDIUM,
@@ -408,7 +408,7 @@
             recoverable=True,
             **kwargs
         )
-        
+
         self.task_name = task_name
         self.worker_count = worker_count
         self.failed_tasks = failed_tasks
@@ -419,7 +419,7 @@
     """Classify a standard exception into our error hierarchy."""
     if isinstance(exc, ETLError):
         return exc
-    
+
     # Network-related exceptions
     if isinstance(exc, (ConnectionError, TimeoutError)):
         return NetworkError(
@@ -427,7 +427,7 @@
             cause=exc,
             context=ErrorContext(operation="network_request")
         )
-    
+
     # File system exceptions
     if isinstance(exc, (FileNotFoundError, PermissionError, OSError)):
         return SystemError(
@@ -436,7 +436,7 @@
             cause=exc,
             context=ErrorContext(operation="file_system")
         )
-    
+
     # Data format exceptions
     if isinstance(exc, (ValueError, TypeError)) and "json" in str(exc).lower():
         return DataError(
@@ -445,7 +445,7 @@
             cause=exc,
             context=ErrorContext(operation="data_parsing")
         )
-    
+
     # Generic system error
     return SystemError(
         f"Unexpected error: {exc}",
@@ -458,11 +458,11 @@
     """Check if an error is recoverable and should be retried."""
     if isinstance(error, ETLError):
         return error.recoverable
-    
+
     # Standard exceptions that are usually recoverable
     if isinstance(error, (ConnectionError, TimeoutError)):
         return True
-    
+
     return False
 
 
@@ -470,13 +470,13 @@
     """Get suggested retry delay for an error."""
     if isinstance(error, ETLError):
         return error.retry_after
-    
+
     # Default delays for standard exceptions
     if isinstance(error, ConnectionError):
         return 30.0
     elif isinstance(error, TimeoutError):
         return 60.0
-    
+
     return None
 
 
@@ -484,7 +484,7 @@
     """Format error for structured logging."""
     if isinstance(error, ETLError):
         return error.to_dict()
-    
+
     # Format standard exceptions
     return {
         "error_type": error.__class__.__name__,
@@ -494,4 +494,4 @@
         "recoverable": is_recoverable_error(error),
         "retry_after": get_retry_delay(error),
         "context": {"operation": "unknown"}
-    }
\ No newline at end of file
+    }
--- original/./etl/utils/io.py
+++ fixed/./etl/utils/io.py
@@ -32,8 +32,10 @@
     # Get content length if available for size estimate
     try:
         with requests.head(url, timeout=10) as head_resp:
-            content_length: Optional[str] = head_resp.headers.get("content-length")
-            total_size: Optional[int] = int(content_length) if content_length else None
+            content_length: Optional[str] = head_resp.headers.get(
+                "content-length")
+            total_size: Optional[int] = int(
+                content_length) if content_length else None
     except:
         total_size = None
 
--- original/./etl/utils/performance.py
+++ fixed/./etl/utils/performance.py
@@ -35,26 +35,26 @@
     cache_hits: int = 0
     cache_misses: int = 0
     errors: int = 0
-    
+
     @property
     def duration(self) -> float:
         """Get operation duration in seconds."""
         if self.end_time is None:
             return time.time() - self.start_time
         return self.end_time - self.start_time
-    
+
     @property
     def throughput_ops_per_sec(self) -> float:
         """Get operations per second."""
         duration = self.duration
         return self.operation_count / duration if duration > 0 else 0.0
-    
+
     @property
     def throughput_bytes_per_sec(self) -> float:
         """Get bytes processed per second."""
         duration = self.duration
         return self.bytes_processed / duration if duration > 0 else 0.0
-    
+
     @property
     def cache_hit_rate(self) -> float:
         """Get cache hit rate as percentage."""
@@ -64,7 +64,7 @@
 
 class ConnectionPool:
     """HTTP connection pool with session management and retry logic."""
-    
+
     def __init__(
         self,
         pool_connections: int = 10,
@@ -78,29 +78,30 @@
         self.max_retries = max_retries
         self.backoff_factor = backoff_factor
         self.pool_block = pool_block
-        
+
         # Thread-local storage for sessions
         self._local = threading.local()
-        
+
         # Weak references to track sessions for cleanup
         self._sessions = weakref.WeakSet()
-        
-        log.info("üîó Initialized connection pool: connections=%d, maxsize=%d", 
-                pool_connections, pool_maxsize)
-    
+
+        log.info("üîó Initialized connection pool: connections=%d, maxsize=%d",
+                 pool_connections, pool_maxsize)
+
     def get_session(self) -> requests.Session:
         """Get or create a thread-local HTTP session with connection pooling."""
         if not hasattr(self._local, 'session'):
             session = requests.Session()
-            
+
             # Configure retry strategy
             retry_strategy = Retry(
                 total=self.max_retries,
                 backoff_factor=self.backoff_factor,
                 status_forcelist=[429, 500, 502, 503, 504],
-                allowed_methods=["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]
+                allowed_methods=["HEAD", "GET", "PUT",
+                                 "DELETE", "OPTIONS", "TRACE"]
             )
-            
+
             # Configure adapter with connection pooling
             adapter = HTTPAdapter(
                 pool_connections=self.pool_connections,
@@ -108,24 +109,25 @@
                 pool_block=self.pool_block,
                 max_retries=retry_strategy
             )
-            
+
             session.mount("http://", adapter)
             session.mount("https://", adapter)
-            
+
             # Set default headers
             session.headers.update({
                 'User-Agent': 'ETL-Pipeline/1.0 (Python requests)',
                 'Accept-Encoding': 'gzip, deflate',
                 'Connection': 'keep-alive'
             })
-            
+
             self._local.session = session
             self._sessions.add(session)
-            
-            log.debug("Created new HTTP session for thread %s", threading.current_thread().name)
-        
+
+            log.debug("Created new HTTP session for thread %s",
+                      threading.current_thread().name)
+
         return self._local.session
-    
+
     def close_all_sessions(self):
         """Close all active sessions and clean up connections."""
         closed_count = 0
@@ -135,17 +137,17 @@
                 closed_count += 1
             except Exception as e:
                 log.warning("Error closing session: %s", e)
-        
+
         # Clear thread-local storage
         if hasattr(self._local, 'session'):
             delattr(self._local, 'session')
-        
+
         log.info("üîí Closed %d HTTP sessions", closed_count)
 
 
 class ResponseCache:
     """In-memory cache for HTTP responses with TTL and size limits."""
-    
+
     def __init__(
         self,
         max_size: int = 1000,
@@ -155,13 +157,14 @@
         self.max_size = max_size
         self.default_ttl = default_ttl
         self.max_response_size = max_response_size
-        
+
         self._cache: Dict[str, Tuple[Any, float]] = {}
         self._access_times: Dict[str, float] = {}
         self._lock = threading.RLock()
-        
-        log.info("üíæ Initialized response cache: max_size=%d, ttl=%ds", max_size, default_ttl)
-    
+
+        log.info("üíæ Initialized response cache: max_size=%d, ttl=%ds",
+                 max_size, default_ttl)
+
     def _generate_key(self, url: str, params: Optional[Dict] = None, headers: Optional[Dict] = None) -> str:
         """Generate cache key from request parameters."""
         key_data = {
@@ -171,34 +174,34 @@
         }
         key_string = json.dumps(key_data, sort_keys=True)
         return hashlib.md5(key_string.encode()).hexdigest()
-    
+
     def get(self, url: str, params: Optional[Dict] = None, headers: Optional[Dict] = None) -> Optional[Any]:
         """Get cached response if available and not expired."""
         key = self._generate_key(url, params, headers)
-        
+
         with self._lock:
             if key not in self._cache:
                 return None
-            
+
             data, expire_time = self._cache[key]
-            
+
             # Check if expired
             if time.time() > expire_time:
                 del self._cache[key]
                 self._access_times.pop(key, None)
                 return None
-            
+
             # Update access time for LRU
             self._access_times[key] = time.time()
-            
+
             log.debug("Cache HIT for key: %s", key[:8])
             return data
-    
+
     def set(
-        self, 
-        url: str, 
-        data: Any, 
-        params: Optional[Dict] = None, 
+        self,
+        url: str,
+        data: Any,
+        params: Optional[Dict] = None,
         headers: Optional[Dict] = None,
         ttl: Optional[int] = None
     ):
@@ -206,42 +209,43 @@
         # Check response size
         try:
             if hasattr(data, '__len__') and len(str(data)) > self.max_response_size:
-                log.debug("Response too large to cache: %d bytes", len(str(data)))
+                log.debug("Response too large to cache: %d bytes",
+                          len(str(data)))
                 return
         except Exception:
             pass  # If we can't determine size, proceed with caching
-        
+
         key = self._generate_key(url, params, headers)
         expire_time = time.time() + (ttl or self.default_ttl)
-        
+
         with self._lock:
             # Evict oldest items if cache is full
             while len(self._cache) >= self.max_size:
                 self._evict_lru()
-            
+
             self._cache[key] = (data, expire_time)
             self._access_times[key] = time.time()
-            
+
             log.debug("Cache SET for key: %s", key[:8])
-    
+
     def _evict_lru(self):
         """Evict least recently used item."""
         if not self._access_times:
             return
-        
+
         lru_key = min(self._access_times.items(), key=lambda x: x[1])[0]
         self._cache.pop(lru_key, None)
         self._access_times.pop(lru_key, None)
-        
+
         log.debug("Evicted LRU cache entry: %s", lru_key[:8])
-    
+
     def clear(self):
         """Clear all cached items."""
         with self._lock:
             self._cache.clear()
             self._access_times.clear()
         log.info("üóëÔ∏è Cleared response cache")
-    
+
     def stats(self) -> Dict[str, Any]:
         """Get cache statistics."""
         with self._lock:
@@ -255,58 +259,63 @@
 
 class MemoryManager:
     """Memory management utilities for large data processing."""
-    
+
     def __init__(self, memory_limit_mb: int = 1024):
         self.memory_limit_bytes = memory_limit_mb * 1024 * 1024
-        self.chunk_size = min(50 * 1024 * 1024, self.memory_limit_bytes // 4)  # 50MB or 1/4 of limit
-        
-        log.info("üß† Memory manager initialized: limit=%dMB, chunk_size=%dMB", 
-                memory_limit_mb, self.chunk_size // (1024 * 1024))
-    
+        # 50MB or 1/4 of limit
+        self.chunk_size = min(50 * 1024 * 1024, self.memory_limit_bytes // 4)
+
+        log.info("üß† Memory manager initialized: limit=%dMB, chunk_size=%dMB",
+                 memory_limit_mb, self.chunk_size // (1024 * 1024))
+
     def get_optimal_chunk_size(self, total_size: int) -> int:
         """Calculate optimal chunk size based on total data size and memory limits."""
         if total_size <= self.chunk_size:
             return total_size
-        
+
         # Calculate number of chunks needed
         num_chunks = (total_size + self.chunk_size - 1) // self.chunk_size
-        
+
         # Optimize chunk size to minimize number of chunks while staying under limit
-        optimal_size = min(self.chunk_size, total_size // max(1, num_chunks - 1))
-        
+        optimal_size = min(self.chunk_size, total_size //
+                           max(1, num_chunks - 1))
+
         return optimal_size
-    
+
     def process_in_chunks(
-        self, 
-        data: Union[List, str, bytes], 
+        self,
+        data: Union[List, str, bytes],
         processor: Callable[[Any], Any],
         chunk_size: Optional[int] = None
     ) -> List[Any]:
         """Process large data in memory-efficient chunks."""
         if chunk_size is None:
             chunk_size = self.chunk_size
-        
+
         results = []
         total_size = len(data)
-        
-        log.debug("Processing %d items in chunks of %d", total_size, chunk_size)
-        
+
+        log.debug("Processing %d items in chunks of %d",
+                  total_size, chunk_size)
+
         for i in range(0, total_size, chunk_size):
             chunk = data[i:i + chunk_size]
             try:
                 result = processor(chunk)
                 results.append(result)
             except Exception as e:
-                log.error("Error processing chunk %d-%d: %s", i, i + len(chunk), e)
-                raise ResourceError(f"Chunk processing failed: {e}", resource_type="memory") from e
-        
+                log.error("Error processing chunk %d-%d: %s",
+                          i, i + len(chunk), e)
+                raise ResourceError(
+                    f"Chunk processing failed: {e}", resource_type="memory") from e
+
         return results
-    
+
     def stream_file_chunks(self, file_path: Path, chunk_size: Optional[int] = None):
         """Generator to stream file contents in chunks."""
         if chunk_size is None:
             chunk_size = self.chunk_size
-        
+
         try:
             with file_path.open('rb') as f:
                 while True:
@@ -315,18 +324,20 @@
                         break
                     yield chunk
         except Exception as e:
-            raise ResourceError(f"File streaming failed: {e}", resource_type="file_io") from e
+            raise ResourceError(
+                f"File streaming failed: {e}", resource_type="file_io") from e
 
 
 class ParallelProcessor:
     """Parallel processing utilities for ETL operations."""
-    
+
     def __init__(self, max_workers: Optional[int] = None):
         self.max_workers = max_workers
         self._metrics = PerformanceMetrics(start_time=time.time())
-        
-        log.info("‚ö° Parallel processor initialized: max_workers=%s", max_workers or "auto")
-    
+
+        log.info("‚ö° Parallel processor initialized: max_workers=%s",
+                 max_workers or "auto")
+
     def process_sources_parallel(
         self,
         sources: List[Any],
@@ -336,47 +347,52 @@
         """Process multiple sources in parallel with error handling."""
         workers = max_workers or self.max_workers
         results = []
-        
+
         self._metrics.start_time = time.time()
         self._metrics.operation_count = len(sources)
-        
-        log.info("üöÄ Processing %d sources in parallel (workers=%s)", len(sources), workers)
-        
+
+        log.info("üöÄ Processing %d sources in parallel (workers=%s)",
+                 len(sources), workers)
+
         with ThreadPoolExecutor(max_workers=workers) as executor:
             # Submit all tasks
             future_to_source = {
-                executor.submit(self._safe_processor, processor, source): source 
+                executor.submit(self._safe_processor, processor, source): source
                 for source in sources
             }
-            
+
             # Collect results as they complete
             for future in as_completed(future_to_source):
                 source = future_to_source[future]
                 try:
                     result = future.result()
                     results.append((source, result))
-                    log.debug("‚úÖ Completed processing source: %s", getattr(source, 'name', str(source)))
+                    log.debug("‚úÖ Completed processing source: %s",
+                              getattr(source, 'name', str(source)))
                 except Exception as e:
                     self._metrics.errors += 1
                     results.append((source, e))
-                    log.error("‚ùå Failed processing source %s: %s", getattr(source, 'name', str(source)), e)
-        
+                    log.error("‚ùå Failed processing source %s: %s",
+                              getattr(source, 'name', str(source)), e)
+
         self._metrics.end_time = time.time()
-        
-        success_count = len([r for r in results if not isinstance(r[1], Exception)])
+
+        success_count = len(
+            [r for r in results if not isinstance(r[1], Exception)])
         log.info("üìä Parallel processing completed: %d/%d successful in %.2fs",
-                success_count, len(sources), self._metrics.duration)
-        
+                 success_count, len(sources), self._metrics.duration)
+
         return results
-    
+
     def _safe_processor(self, processor: Callable, source: Any) -> Any:
         """Wrapper to safely execute processor with error handling."""
         try:
             return processor(source)
         except Exception as e:
-            log.error("Processing failed for source %s: %s", getattr(source, 'name', str(source)), e)
+            log.error("Processing failed for source %s: %s",
+                      getattr(source, 'name', str(source)), e)
             raise
-    
+
     def get_metrics(self) -> PerformanceMetrics:
         """Get performance metrics for the last parallel operation."""
         return self._metrics
@@ -394,15 +410,15 @@
             url = kwargs.get('url') or (args[0] if args else None)
             params = kwargs.get('params')
             headers = kwargs.get('headers')
-            
+
             if not url:
                 return func(*args, **kwargs)
-            
+
             # Check cache first
             cached_result = cache.get(url, params, headers)
             if cached_result is not None:
                 return cached_result
-            
+
             # Execute request and cache result
             try:
                 result = func(*args, **kwargs)
@@ -411,7 +427,7 @@
             except Exception as e:
                 log.debug("Request failed, not caching: %s", e)
                 raise
-        
+
         return wrapper
     return decorator
 
@@ -422,20 +438,21 @@
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             start_time = time.time()
-            
+
             try:
                 result = func(*args, **kwargs)
                 end_time = time.time()
                 duration = end_time - start_time
-                
+
                 log.info("‚è±Ô∏è  %s completed in %.3fs", operation_name, duration)
                 return result
             except Exception as e:
                 end_time = time.time()
                 duration = end_time - start_time
-                log.error("‚è±Ô∏è  %s failed after %.3fs: %s", operation_name, duration, e)
+                log.error("‚è±Ô∏è  %s failed after %.3fs: %s",
+                          operation_name, duration, e)
                 raise
-        
+
         return wrapper
     return decorator
 
@@ -473,15 +490,15 @@
 def cleanup_performance_resources():
     """Clean up global performance resources."""
     global _connection_pool, _response_cache, _memory_manager
-    
+
     if _connection_pool:
         _connection_pool.close_all_sessions()
-    
+
     if _response_cache:
         _response_cache.clear()
-    
+
     _connection_pool = None
     _response_cache = None
     _memory_manager = None
-    
-    log.info("üßπ Performance resources cleaned up")
\ No newline at end of file
+
+    log.info("üßπ Performance resources cleaned up")
--- original/./etl/utils/__init__.py
+++ fixed/./etl/utils/__init__.py
@@ -9,4 +9,4 @@
     "CHUNK",
     "download",
     "extract_zip",
-]
\ No newline at end of file
+]
--- original/./etl/utils/regression_detector.py
+++ fixed/./etl/utils/regression_detector.py
@@ -27,29 +27,30 @@
     """Baseline performance metrics for regression detection."""
     operation_name: str
     metrics: Dict[str, float]  # metric_name -> value
-    confidence_intervals: Dict[str, Tuple[float, float]]  # metric_name -> (lower, upper)
+    # metric_name -> (lower, upper)
+    confidence_intervals: Dict[str, Tuple[float, float]]
     sample_count: int
     established_at: float
     last_updated: float
     version: str = "1.0"
-    
+
     def is_regression(self, current_value: float, metric_name: str, sensitivity: float = 0.95) -> bool:
         """Check if current value indicates a regression."""
         if metric_name not in self.confidence_intervals:
             return False
-        
+
         lower, upper = self.confidence_intervals[metric_name]
-        
+
         # For metrics where lower is better (duration, memory_usage)
         if metric_name in ["duration", "memory_usage", "memory_peak"]:
             # Regression if significantly higher than baseline
             return current_value > upper
-        
+
         # For metrics where higher is better (throughput, success_rate)
         elif metric_name in ["throughput", "success_rate", "items_per_second"]:
             # Regression if significantly lower than baseline
             return current_value < lower
-        
+
         return False
 
 
@@ -64,12 +65,12 @@
     confidence_level: float
     detection_time: float
     severity: str  # "minor", "moderate", "major", "critical"
-    
+
     @property
     def is_significant(self) -> bool:
         """Check if regression is statistically significant."""
         return self.confidence_level > 0.95 and abs(self.regression_magnitude) > 0.1
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary for serialization."""
         return {
@@ -87,21 +88,21 @@
 
 class StatisticalAnalyzer:
     """Statistical analysis for performance regression detection."""
-    
+
     @staticmethod
     def calculate_confidence_interval(
-        values: List[float], 
+        values: List[float],
         confidence_level: float = 0.95
     ) -> Tuple[float, float]:
         """Calculate confidence interval for a dataset."""
         if len(values) < 2:
             mean_val = values[0] if values else 0
             return (mean_val, mean_val)
-        
+
         mean_val = statistics.mean(values)
         std_val = statistics.stdev(values)
         n = len(values)
-        
+
         # Use t-distribution for small samples
         if n < 30:
             t_critical = stats.t.ppf((1 + confidence_level) / 2, n - 1)
@@ -110,21 +111,22 @@
             # Use normal distribution for large samples
             z_critical = stats.norm.ppf((1 + confidence_level) / 2)
             margin_error = z_critical * (std_val / np.sqrt(n))
-        
+
         return (mean_val - margin_error, mean_val + margin_error)
-    
+
     @staticmethod
     def detect_trend(values: List[float], window_size: int = 10) -> str:
         """Detect trend in performance metrics."""
         if len(values) < window_size:
             return "insufficient_data"
-        
+
         recent_values = values[-window_size:]
-        
+
         # Calculate linear regression slope
         x = np.arange(len(recent_values))
-        slope, intercept, r_value, p_value, std_err = stats.linregress(x, recent_values)
-        
+        slope, intercept, r_value, p_value, std_err = stats.linregress(
+            x, recent_values)
+
         # Determine trend significance
         if p_value < 0.05:  # Statistically significant
             if slope > 0:
@@ -133,62 +135,62 @@
                 return "decreasing"
         else:
             return "stable"
-    
+
     @staticmethod
     def calculate_regression_magnitude(baseline: float, current: float) -> float:
         """Calculate regression magnitude as percentage change."""
         if baseline == 0:
             return 0.0
-        
+
         return ((current - baseline) / baseline) * 100
-    
+
     @staticmethod
     def detect_anomalies(values: List[float], threshold: float = 2.0) -> List[int]:
         """Detect anomalies using z-score method."""
         if len(values) < 3:
             return []
-        
+
         mean_val = statistics.mean(values)
         std_val = statistics.stdev(values)
-        
+
         if std_val == 0:
             return []
-        
+
         anomalies = []
         for i, value in enumerate(values):
             z_score = abs((value - mean_val) / std_val)
             if z_score > threshold:
                 anomalies.append(i)
-        
+
         return anomalies
-    
+
     @staticmethod
     def perform_change_point_detection(values: List[float], min_segment_length: int = 5) -> List[int]:
         """Detect change points in performance metrics."""
         if len(values) < min_segment_length * 2:
             return []
-        
+
         change_points = []
         n = len(values)
-        
+
         for i in range(min_segment_length, n - min_segment_length):
             # Split data at point i
             segment1 = values[:i]
             segment2 = values[i:]
-            
+
             # Perform t-test
             if len(segment1) >= 2 and len(segment2) >= 2:
                 t_stat, p_value = stats.ttest_ind(segment1, segment2)
-                
+
                 if p_value < 0.01:  # Highly significant change
                     change_points.append(i)
-        
+
         return change_points
 
 
 class PerformanceRegressionDetector:
     """Advanced performance regression detection system."""
-    
+
     def __init__(
         self,
         baseline_window_size: int = 50,
@@ -200,81 +202,83 @@
         self.detection_window_size = detection_window_size
         self.min_samples_for_baseline = min_samples_for_baseline
         self.baseline_file = baseline_file or Path("performance_baselines.pkl")
-        
+
         # Data storage
         self.performance_data: Dict[str, deque] = defaultdict(
             lambda: deque(maxlen=self.baseline_window_size * 2)
         )
         self.baselines: Dict[str, RegressionBaseline] = {}
         self.detected_regressions: List[RegressionDetection] = []
-        
+
         # Statistical analyzer
         self.analyzer = StatisticalAnalyzer()
-        
+
         # Load existing baselines
         self._load_baselines()
-        
+
         log.info("Initialized PerformanceRegressionDetector")
-    
+
     def record_performance(self, metrics: PerformanceMetrics) -> Optional[List[RegressionDetection]]:
         """Record performance metrics and check for regressions."""
         operation = metrics.operation_name
-        
+
         # Store metrics
         self.performance_data[operation].append(metrics)
-        
+
         # Update or establish baseline
         if self._should_update_baseline(operation):
             self._update_baseline(operation)
-        
+
         # Check for regressions
         if operation in self.baselines:
             regressions = self._detect_regressions(operation, metrics)
             if regressions:
                 self.detected_regressions.extend(regressions)
                 return regressions
-        
+
         return None
-    
+
     def establish_baseline(self, operation: str, force_update: bool = False) -> bool:
         """Establish performance baseline for an operation."""
         if operation not in self.performance_data:
             log.warning("No performance data available for %s", operation)
             return False
-        
+
         data = list(self.performance_data[operation])
-        
+
         if len(data) < self.min_samples_for_baseline:
-            log.warning("Insufficient data for baseline: %s (%d samples)", operation, len(data))
+            log.warning(
+                "Insufficient data for baseline: %s (%d samples)", operation, len(data))
             return False
-        
+
         if operation in self.baselines and not force_update:
             log.info("Baseline already exists for %s", operation)
             return True
-        
+
         return self._update_baseline(operation)
-    
+
     def get_baseline(self, operation: str) -> Optional[RegressionBaseline]:
         """Get baseline for an operation."""
         return self.baselines.get(operation)
-    
+
     def get_regression_summary(self) -> Dict[str, Any]:
         """Get summary of detected regressions."""
         if not self.detected_regressions:
             return {"total_regressions": 0, "operations_affected": 0}
-        
+
         # Group by operation
         by_operation = defaultdict(list)
         for regression in self.detected_regressions:
             by_operation[regression.operation_name].append(regression)
-        
+
         # Calculate summary statistics
         severities = defaultdict(int)
         for regression in self.detected_regressions:
             severities[regression.severity] += 1
-        
-        significant_regressions = [r for r in self.detected_regressions if r.is_significant]
-        
+
+        significant_regressions = [
+            r for r in self.detected_regressions if r.is_significant]
+
         return {
             "total_regressions": len(self.detected_regressions),
             "significant_regressions": len(significant_regressions),
@@ -285,49 +289,49 @@
             },
             "recent_regressions": [
                 r.to_dict() for r in sorted(
-                    self.detected_regressions, 
-                    key=lambda x: x.detection_time, 
+                    self.detected_regressions,
+                    key=lambda x: x.detection_time,
                     reverse=True
                 )[:10]
             ]
         }
-    
+
     def analyze_performance_trends(self, operation: str) -> Dict[str, Any]:
         """Analyze performance trends for an operation."""
         if operation not in self.performance_data:
             return {"error": f"No data for operation: {operation}"}
-        
+
         data = list(self.performance_data[operation])
-        
+
         if len(data) < 5:
             return {"error": "Insufficient data for trend analysis"}
-        
+
         # Extract metrics
         durations = [m.duration for m in data]
         throughputs = [m.throughput_items_per_sec for m in data]
         memory_usage = [m.memory_peak for m in data]
-        
+
         # Analyze trends
         trends = {
             "duration": self.analyzer.detect_trend(durations),
             "throughput": self.analyzer.detect_trend(throughputs),
             "memory_usage": self.analyzer.detect_trend(memory_usage)
         }
-        
+
         # Detect anomalies
         anomalies = {
             "duration": self.analyzer.detect_anomalies(durations),
             "throughput": self.analyzer.detect_anomalies(throughputs),
             "memory_usage": self.analyzer.detect_anomalies(memory_usage)
         }
-        
+
         # Detect change points
         change_points = {
             "duration": self.analyzer.perform_change_point_detection(durations),
             "throughput": self.analyzer.perform_change_point_detection(throughputs),
             "memory_usage": self.analyzer.perform_change_point_detection(memory_usage)
         }
-        
+
         return {
             "operation": operation,
             "sample_count": len(data),
@@ -336,104 +340,109 @@
             "change_points": change_points,
             "stability_score": self._calculate_stability_score(data)
         }
-    
+
     def generate_regression_report(self, hours_back: int = 24) -> Dict[str, Any]:
         """Generate comprehensive regression report."""
         cutoff_time = time.time() - (hours_back * 3600)
-        
+
         # Filter recent regressions
         recent_regressions = [
-            r for r in self.detected_regressions 
+            r for r in self.detected_regressions
             if r.detection_time > cutoff_time
         ]
-        
+
         # Analyze by operation
         operation_analysis = {}
         for operation in self.performance_data.keys():
             if operation in self.baselines:
-                operation_analysis[operation] = self.analyze_performance_trends(operation)
-        
+                operation_analysis[operation] = self.analyze_performance_trends(
+                    operation)
+
         # Generate recommendations
-        recommendations = self._generate_regression_recommendations(recent_regressions)
-        
+        recommendations = self._generate_regression_recommendations(
+            recent_regressions)
+
         return {
             "report_period_hours": hours_back,
             "recent_regressions": len(recent_regressions),
             "total_operations_monitored": len(self.performance_data),
             "baselines_established": len(self.baselines),
             "regressions_by_severity": {
-                severity: len([r for r in recent_regressions if r.severity == severity])
+                severity: len(
+                    [r for r in recent_regressions if r.severity == severity])
                 for severity in ["minor", "moderate", "major", "critical"]
             },
             "operation_analysis": operation_analysis,
             "recommendations": recommendations,
             "detailed_regressions": [r.to_dict() for r in recent_regressions]
         }
-    
+
     def save_baselines(self) -> None:
         """Save baselines to file."""
         try:
             with self.baseline_file.open('wb') as f:
                 pickle.dump(self.baselines, f)
-            log.info("Saved %d baselines to %s", len(self.baselines), self.baseline_file)
+            log.info("Saved %d baselines to %s", len(
+                self.baselines), self.baseline_file)
         except Exception as e:
             log.error("Failed to save baselines: %s", e)
-    
+
     def _load_baselines(self) -> None:
         """Load baselines from file."""
         if not self.baseline_file.exists():
             return
-        
+
         try:
             with self.baseline_file.open('rb') as f:
                 self.baselines = pickle.load(f)
-            log.info("Loaded %d baselines from %s", len(self.baselines), self.baseline_file)
+            log.info("Loaded %d baselines from %s", len(
+                self.baselines), self.baseline_file)
         except Exception as e:
             log.warning("Failed to load baselines: %s", e)
             self.baselines = {}
-    
+
     def _should_update_baseline(self, operation: str) -> bool:
         """Determine if baseline should be updated."""
         if operation not in self.baselines:
             return len(self.performance_data[operation]) >= self.min_samples_for_baseline
-        
+
         baseline = self.baselines[operation]
-        
+
         # Update if baseline is old (> 7 days) and we have enough new data
         age_days = (time.time() - baseline.last_updated) / (24 * 3600)
         if age_days > 7 and len(self.performance_data[operation]) >= self.min_samples_for_baseline:
             return True
-        
+
         return False
-    
+
     def _update_baseline(self, operation: str) -> bool:
         """Update baseline for an operation."""
         data = list(self.performance_data[operation])
-        
+
         if len(data) < self.min_samples_for_baseline:
             return False
-        
+
         # Use recent stable data for baseline
         baseline_data = data[-self.baseline_window_size:]
-        
+
         # Calculate baseline metrics
         durations = [m.duration for m in baseline_data]
         throughputs = [m.throughput_items_per_sec for m in baseline_data]
         memory_usage = [m.memory_peak for m in baseline_data]
-        
+
         # Calculate confidence intervals
         metrics = {
             "duration": statistics.mean(durations),
             "throughput": statistics.mean(throughputs),
             "memory_usage": statistics.mean(memory_usage)
         }
-        
+
         confidence_intervals = {
             "duration": self.analyzer.calculate_confidence_interval(durations),
             "throughput": self.analyzer.calculate_confidence_interval(throughputs),
             "memory_usage": self.analyzer.calculate_confidence_interval(memory_usage)
         }
-        
+
         # Create baseline
         baseline = RegressionBaseline(
             operation_name=operation,
@@ -443,40 +452,41 @@
             established_at=time.time(),
             last_updated=time.time()
         )
-        
+
         self.baselines[operation] = baseline
-        
+
         # Save baselines
         self.save_baselines()
-        
-        log.info("Updated baseline for %s with %d samples", operation, len(baseline_data))
+
+        log.info("Updated baseline for %s with %d samples",
+                 operation, len(baseline_data))
         return True
-    
+
     def _detect_regressions(self, operation: str, metrics: PerformanceMetrics) -> List[RegressionDetection]:
         """Detect regressions in current metrics."""
         if operation not in self.baselines:
             return []
-        
+
         baseline = self.baselines[operation]
         regressions = []
-        
+
         # Check each metric
         current_metrics = {
             "duration": metrics.duration,
             "throughput": metrics.throughput_items_per_sec,
             "memory_usage": metrics.memory_peak
         }
-        
+
         for metric_name, current_value in current_metrics.items():
             if baseline.is_regression(current_value, metric_name):
                 baseline_value = baseline.metrics[metric_name]
                 regression_magnitude = self.analyzer.calculate_regression_magnitude(
                     baseline_value, current_value
                 )
-                
+
                 # Determine severity
                 severity = self._determine_severity(abs(regression_magnitude))
-                
+
                 regression = RegressionDetection(
                     operation_name=operation,
                     metric_name=metric_name,
@@ -487,16 +497,16 @@
                     detection_time=time.time(),
                     severity=severity
                 )
-                
+
                 regressions.append(regression)
-                
+
                 log.warning(
                     "üîç Regression detected: %s.%s = %.2f (baseline: %.2f, change: %.1f%%)",
                     operation, metric_name, current_value, baseline_value, regression_magnitude
                 )
-        
+
         return regressions
-    
+
     def _determine_severity(self, magnitude: float) -> str:
         """Determine severity based on regression magnitude."""
         if magnitude > 100:  # More than 100% change
@@ -507,60 +517,62 @@
             return "moderate"
         else:
             return "minor"
-    
+
     def _calculate_stability_score(self, data: List[PerformanceMetrics]) -> float:
         """Calculate stability score for an operation (0-1, higher is better)."""
         if len(data) < 3:
             return 0.0
-        
+
         # Calculate coefficient of variation for key metrics
         durations = [m.duration for m in data]
         throughputs = [m.throughput_items_per_sec for m in data]
-        
+
         duration_cv = statistics.stdev(durations) / statistics.mean(durations)
-        throughput_cv = statistics.stdev(throughputs) / statistics.mean(throughputs) if statistics.mean(throughputs) > 0 else 0
-        
+        throughput_cv = statistics.stdev(
+            throughputs) / statistics.mean(throughputs) if statistics.mean(throughputs) > 0 else 0
+
         # Lower CV means higher stability
         avg_cv = (duration_cv + throughput_cv) / 2
         stability_score = max(0, 1 - avg_cv)
-        
+
         return stability_score
-    
+
     def _generate_regression_recommendations(self, regressions: List[RegressionDetection]) -> List[str]:
         """Generate recommendations based on detected regressions."""
         recommendations = []
-        
+
         # Group by operation
         by_operation = defaultdict(list)
         for regression in regressions:
             by_operation[regression.operation_name].append(regression)
-        
+
         # Generate operation-specific recommendations
         for operation, op_regressions in by_operation.items():
             metric_types = {r.metric_name for r in op_regressions}
-            
+
             if "duration" in metric_types:
                 recommendations.append(
                     f"Operation '{operation}' showing increased duration - check for resource contention or inefficient processing"
                 )
-            
+
             if "throughput" in metric_types:
                 recommendations.append(
                     f"Operation '{operation}' showing decreased throughput - consider scaling resources or optimizing algorithms"
                 )
-            
+
             if "memory_usage" in metric_types:
                 recommendations.append(
                     f"Operation '{operation}' showing increased memory usage - check for memory leaks or inefficient data structures"
                 )
-        
+
         # Generate general recommendations
-        critical_regressions = [r for r in regressions if r.severity == "critical"]
+        critical_regressions = [
+            r for r in regressions if r.severity == "critical"]
         if critical_regressions:
             recommendations.append(
                 f"Critical regressions detected in {len(critical_regressions)} operations - immediate investigation recommended"
             )
-        
+
         return recommendations
 
 
@@ -596,20 +608,20 @@
         def wrapper(*args, **kwargs):
             import psutil
             from .performance_optimizer import PerformanceMetrics
-            
+
             detector = get_global_detector()
-            
+
             # Record performance
             start_time = time.time()
             start_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-            
+
             try:
                 result = func(*args, **kwargs)
-                
+
                 # Create performance metrics
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=operation_name,
                     start_time=start_time,
@@ -622,21 +634,22 @@
                     worker_count=1,
                     items_processed=1
                 )
-                
+
                 # Check for regressions
                 regressions = detector.record_performance(metrics)
                 if regressions:
                     for regression in regressions:
                         if regression.is_significant:
-                            log.warning("üîç Performance regression detected in %s", operation_name)
-                
+                            log.warning(
+                                "üîç Performance regression detected in %s", operation_name)
+
                 return result
-                
+
             except Exception as e:
                 # Still record performance for failed operations
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=f"{operation_name}_failed",
                     start_time=start_time,
@@ -649,9 +662,9 @@
                     worker_count=1,
                     items_processed=0
                 )
-                
+
                 detector.record_performance(metrics)
                 raise
-        
+
         return wrapper
-    return decorator
\ No newline at end of file
+    return decorator
--- original/./etl/utils/shapefile_validation.py
+++ fixed/./etl/utils/shapefile_validation.py
@@ -21,71 +21,74 @@
 
 def validate_shapefile_components(shp_file_path: Path) -> ShapefileValidationResult:
     """üîç Validate that all required shapefile components exist.
-    
+
     Args:
         shp_file_path: Path to the .shp file to validate.
-        
+
     Returns:
         ShapefileValidationResult with validation status and details.
     """
     shp_stem: str = shp_file_path.stem
     shp_directory: Path = shp_file_path.parent
-    
+
     required_extensions: List[str] = ['.shx', '.dbf']
     missing_components: List[str] = []
-    
-    log.debug("üîç Validating shapefile components for stem: '%s' in directory: %s", 
-             shp_stem, shp_directory)
-    
+
+    log.debug("üîç Validating shapefile components for stem: '%s' in directory: %s",
+              shp_stem, shp_directory)
+
     for ext in required_extensions:
         component_file: Path = shp_directory / f"{shp_stem}{ext}"
         log.debug("   Checking for component file: %s", component_file)
-        
+
         if not component_file.exists():
             missing_components.append(ext)
             log.debug("   ‚ùå Missing component: %s", component_file)
         else:
             log.debug("   ‚úÖ Found component: %s", component_file)
-    
+
     if missing_components:
         error_msg: str = f"Missing required components: {', '.join(missing_components)}"
         return ShapefileValidationResult(False, error_msg, missing_components)
-    
+
     log.debug("‚úÖ All shapefile components validated for: %s", shp_file_path.name)
     return ShapefileValidationResult(True, "All components present", [])
 
 
 def find_alternative_shapefile(directory: Path) -> Path | None:
     """üîç Find a valid alternative shapefile in the given directory.
-    
+
     Args:
         directory: Directory to search for valid shapefiles.
-        
+
     Returns:
         Path to a valid shapefile, or None if none found.
     """
     if not directory.exists():
         return None
-    
+
     shp_files: List[Path] = list(directory.glob("*.shp"))
-    log.debug("üîç Found %d shapefile(s) in directory %s", len(shp_files), directory)
-    
+    log.debug("üîç Found %d shapefile(s) in directory %s",
+              len(shp_files), directory)
+
     for shp_file in shp_files:
         log.debug("   Validating shapefile: %s", shp_file.name)
-        validation_result: ShapefileValidationResult = validate_shapefile_components(shp_file)
+        validation_result: ShapefileValidationResult = validate_shapefile_components(
+            shp_file)
         if validation_result.is_valid:
             log.info("‚úÖ Found valid shapefile: %s", shp_file.name)
             return shp_file
         else:
-            log.debug("   ‚ùå Invalid shapefile %s: %s", shp_file.name, validation_result.error_message)
-    
+            log.debug("   ‚ùå Invalid shapefile %s: %s",
+                      shp_file.name, validation_result.error_message)
+
     log.warning("‚ö†Ô∏è No valid shapefiles found in directory: %s", directory)
     return None
 
 
 def log_directory_contents(directory: Path, context: str) -> None:
     """üîç Log directory contents for debugging purposes.
-    
+
     Args:
         directory: Directory to list contents for.
         context: Context string for logging.
@@ -93,14 +96,16 @@
     if not directory.exists():
         log.debug("   Directory does not exist for logging: %s", directory)
         return
-    
+
     try:
         files: List[Path] = list(directory.iterdir())
         log.debug("   Directory contents (%s) for %s:", context, directory)
         for file_path in sorted(files):
             if file_path.is_file():
-                log.debug("     üìÑ %s (%d bytes)", file_path.name, file_path.stat().st_size)
+                log.debug("     üìÑ %s (%d bytes)", file_path.name,
+                          file_path.stat().st_size)
             elif file_path.is_dir():
                 log.debug("     üìÅ %s/", file_path.name)
     except Exception as e:
-        log.warning("   ‚ö†Ô∏è Could not list directory contents for %s: %s", directory, e)
+        log.warning(
+            "   ‚ö†Ô∏è Could not list directory contents for %s: %s", directory, e)
--- original/./etl/utils/sanitize.py
+++ fixed/./etl/utils/sanitize.py
@@ -4,9 +4,10 @@
 
 _SWEDISH_MAP: Final = str.maketrans("√•√§√∂√Ö√Ñ√ñ", "aaoAAO")
 
+
 def slugify(text: str) -> str:
     """ascii-safe, lower-case, underscore-joined identifier."""
     text = text.translate(_SWEDISH_MAP).lower()
     text = re.sub(r"[^\w\-]+", "_", text)      # keep letters, digits, _
     return re.sub(r"__+", "_", text).strip("_") or "unnamed"
-    # collapse repeats, strip leading/trailing _; return "unnamed" if empty
\ No newline at end of file
+    # collapse repeats, strip leading/trailing _; return "unnamed" if empty
--- original/./etl/utils/concurrent_safe.py
+++ fixed/./etl/utils/concurrent_safe.py
@@ -31,7 +31,7 @@
     max_workers: int = 5
     timeout: Optional[float] = None
     fail_fast: bool = False
-    
+
     def copy(self) -> 'ConcurrentConfig':
         """Create a copy of the configuration."""
         return ConcurrentConfig(
@@ -43,13 +43,13 @@
 
 class ThreadSafeConcurrentDownloader:
     """Thread-safe concurrent downloader that accepts configuration parameters."""
-    
+
     def __init__(self, default_config: Optional[ConcurrentConfig] = None):
         self.default_config = default_config or ConcurrentConfig()
         self._lock = threading.RLock()
-    
+
     def download_layers_concurrent(
-        self, 
+        self,
         handler,  # RestApiDownloadHandler instance
         layers_info: List[Dict[str, Any]],
         config: Optional[ConcurrentConfig] = None
@@ -57,18 +57,19 @@
         """Download multiple layers concurrently with thread-safe configuration."""
         if not layers_info:
             return []
-        
+
         # Use provided config or default
         effective_config = config or self.default_config.copy()
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for layer_info in layers_info:
-            layer_name = layer_info.get("name", f"layer_{layer_info.get('id', 'unknown')}")
+            layer_name = layer_info.get(
+                "name", f"layer_{layer_info.get('id', 'unknown')}")
             task_names.append(f"layer_{layer_name}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._fetch_layer_data,
@@ -76,12 +77,12 @@
                 {"layer_metadata_from_service": layer_info.get("metadata")}
             )
             tasks.append(task)
-        
-        log.info("Starting concurrent download of %d layers with %d workers", 
-                len(layers_info), effective_config.max_workers)
-        
+
+        log.info("Starting concurrent download of %d layers with %d workers",
+                 len(layers_info), effective_config.max_workers)
+
         return self._execute_concurrent_tasks(tasks, task_names, effective_config)
-    
+
     def download_collections_concurrent(
         self,
         handler,  # OgcApiDownloadHandler instance
@@ -91,18 +92,18 @@
         """Download multiple collections concurrently with thread-safe configuration."""
         if not collections:
             return []
-        
+
         # Use provided config or default
         effective_config = config or self.default_config.copy()
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for collection in collections:
             collection_id = collection.get("id", "unknown")
             task_names.append(f"collection_{collection_id}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._fetch_collection,
@@ -110,12 +111,12 @@
                 {}
             )
             tasks.append(task)
-        
-        log.info("Starting concurrent download of %d collections with %d workers", 
-                len(collections), effective_config.max_workers)
-        
+
+        log.info("Starting concurrent download of %d collections with %d workers",
+                 len(collections), effective_config.max_workers)
+
         return self._execute_concurrent_tasks(tasks, task_names, effective_config)
-    
+
     def download_files_concurrent(
         self,
         handler,  # FileDownloadHandler instance
@@ -125,17 +126,17 @@
         """Download multiple files concurrently with thread-safe configuration."""
         if not file_stems:
             return []
-        
+
         # Use provided config or default
         effective_config = config or self.default_config.copy()
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for file_stem in file_stems:
             task_names.append(f"file_{file_stem}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._download_single_file_stem,
@@ -143,12 +144,12 @@
                 {}
             )
             tasks.append(task)
-        
-        log.info("Starting concurrent download of %d files with %d workers", 
-                len(file_stems), effective_config.max_workers)
-        
+
+        log.info("Starting concurrent download of %d files with %d workers",
+                 len(file_stems), effective_config.max_workers)
+
         return self._execute_concurrent_tasks(tasks, task_names, effective_config)
-    
+
     def _execute_concurrent_tasks(
         self,
         tasks: List[Tuple[Callable, Tuple, Dict]],
@@ -157,28 +158,30 @@
     ) -> List[ConcurrentResult]:
         """Execute tasks concurrently with proper error handling."""
         results = []
-        
+
         with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
             # Submit all tasks
             future_to_task = {}
             for i, (task_func, args, kwargs) in enumerate(tasks):
-                future = executor.submit(self._execute_single_task, task_func, args, kwargs, task_names[i])
+                future = executor.submit(
+                    self._execute_single_task, task_func, args, kwargs, task_names[i])
                 future_to_task[future] = (i, task_names[i])
-            
+
             # Collect results
             for future in as_completed(future_to_task, timeout=config.timeout):
                 try:
                     result = future.result()
                     results.append(result)
-                    
+
                     if config.fail_fast and not result.success:
-                        log.warning("Fail-fast enabled, stopping on first failure")
+                        log.warning(
+                            "Fail-fast enabled, stopping on first failure")
                         # Cancel remaining futures
                         for remaining_future in future_to_task:
                             if not remaining_future.done():
                                 remaining_future.cancel()
                         break
-                        
+
                 except Exception as e:
                     task_index, task_name = future_to_task[future]
                     error_result = ConcurrentResult(
@@ -187,42 +190,45 @@
                         metadata={"task_name": task_name}
                     )
                     results.append(error_result)
-                    log.error("Task %s failed with exception: %s", task_name, e)
-        
+                    log.error("Task %s failed with exception: %s",
+                              task_name, e)
+
         # Sort results by original task order
-        results.sort(key=lambda r: task_names.index(r.metadata.get("task_name", "")))
-        
+        results.sort(key=lambda r: task_names.index(
+            r.metadata.get("task_name", "")))
+
         successful_count = sum(1 for r in results if r.success)
-        log.info("Concurrent execution completed: %d/%d successful", 
-                successful_count, len(tasks))
-        
+        log.info("Concurrent execution completed: %d/%d successful",
+                 successful_count, len(tasks))
+
         return results
-    
+
     def _execute_single_task(
-        self, 
-        task_func: Callable, 
-        args: Tuple, 
-        kwargs: Dict, 
+        self,
+        task_func: Callable,
+        args: Tuple,
+        kwargs: Dict,
         task_name: str
     ) -> ConcurrentResult:
         """Execute a single task with error handling and timing."""
         start_time = time.time()
-        
+
         try:
             result = task_func(*args, **kwargs)
             duration = time.time() - start_time
-            
+
             return ConcurrentResult(
                 success=True,
                 result=result,
                 duration=duration,
                 metadata={"task_name": task_name}
             )
-            
+
         except Exception as e:
             duration = time.time() - start_time
-            log.debug("Task '%s' failed after %.2fs: %s", task_name, duration, e)
-            
+            log.debug("Task '%s' failed after %.2fs: %s",
+                      task_name, duration, e)
+
             return ConcurrentResult(
                 success=False,
                 error=e,
@@ -267,4 +273,4 @@
 def create_file_downloader(max_workers: int = 4) -> ThreadSafeConcurrentDownloader:
     """Create a downloader optimized for file downloads."""
     config = ConcurrentConfig(max_workers=max_workers, timeout=1800.0)
-    return ThreadSafeConcurrentDownloader(config)
\ No newline at end of file
+    return ThreadSafeConcurrentDownloader(config)
--- original/./etl/utils/intelligent_cache.py
+++ fixed/./etl/utils/intelligent_cache.py
@@ -30,24 +30,24 @@
     ttl_seconds: Optional[float] = None
     tags: List[str] = field(default_factory=list)
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     @property
     def is_expired(self) -> bool:
         """Check if cache entry has expired."""
         if self.ttl_seconds is None:
             return False
         return time.time() - self.created_at > self.ttl_seconds
-    
+
     @property
     def age_seconds(self) -> float:
         """Get age of cache entry in seconds."""
         return time.time() - self.created_at
-    
+
     @property
     def last_access_seconds_ago(self) -> float:
         """Get time since last access in seconds."""
         return time.time() - self.last_accessed
-    
+
     def touch(self) -> None:
         """Update last accessed time and increment access count."""
         self.last_accessed = time.time()
@@ -62,13 +62,13 @@
     evictions: int = 0
     total_size_bytes: int = 0
     total_entries: int = 0
-    
+
     @property
     def hit_rate(self) -> float:
         """Calculate cache hit rate as percentage."""
         total_requests = self.hits + self.misses
         return (self.hits / total_requests * 100) if total_requests > 0 else 0
-    
+
     @property
     def miss_rate(self) -> float:
         """Calculate cache miss rate as percentage."""
@@ -77,7 +77,7 @@
 
 class IntelligentCache:
     """Intelligent multi-level cache with adaptive policies."""
-    
+
     def __init__(
         self,
         max_memory_mb: float = 256,
@@ -90,83 +90,83 @@
         self.default_ttl_seconds = default_ttl_seconds
         self.cache_dir = cache_dir or Path("cache")
         self.cache_dir.mkdir(exist_ok=True)
-        
+
         # In-memory cache (L1)
         self.memory_cache: Dict[str, CacheEntry] = {}
-        
+
         # Disk cache tracking (L2)
         self.disk_cache_index: Dict[str, Path] = {}
-        
+
         # Statistics and monitoring
         self.stats = CacheStats()
         self.lock = threading.RLock()
-        
+
         # Cache policies
         self.enable_adaptive_ttl = True
         self.enable_predictive_loading = True
         self.access_patterns: Dict[str, List[float]] = {}
-        
+
         log.info(
             "Initialized IntelligentCache: memory=%.1fMB, disk=%.1fMB, ttl=%ds",
             max_memory_mb, max_disk_mb, default_ttl_seconds
         )
-    
+
     def get(self, key: str, default: Any = None) -> Any:
         """Get item from cache with intelligent retrieval."""
         with self.lock:
             # Try memory cache first (L1)
             if key in self.memory_cache:
                 entry = self.memory_cache[key]
-                
+
                 if entry.is_expired:
                     self._remove_memory_entry(key)
                     self.stats.misses += 1
                     return default
-                
+
                 entry.touch()
                 self._record_access_pattern(key)
                 self.stats.hits += 1
-                
+
                 log.debug("Cache hit (memory): %s", key)
                 return entry.value
-            
+
             # Try disk cache (L2)
             if key in self.disk_cache_index:
                 disk_path = self.disk_cache_index[key]
-                
+
                 if disk_path.exists():
                     try:
                         entry = self._load_from_disk(disk_path)
-                        
+
                         if entry.is_expired:
                             self._remove_disk_entry(key)
                             self.stats.misses += 1
                             return default
-                        
+
                         # Promote to memory cache if frequently accessed
                         if self._should_promote_to_memory(entry):
                             self._promote_to_memory(key, entry)
-                        
+
                         entry.touch()
                         self._record_access_pattern(key)
                         self.stats.hits += 1
-                        
+
                         log.debug("Cache hit (disk): %s", key)
                         return entry.value
-                    
+
                     except Exception as e:
                         log.warning("Failed to load from disk cache: %s", e)
                         self._remove_disk_entry(key)
-                
+
                 else:
                     # Disk file missing, clean up index
                     self._remove_disk_entry(key)
-            
+
             # Cache miss
             self.stats.misses += 1
             log.debug("Cache miss: %s", key)
             return default
-    
+
     def set(
         self,
         key: str,
@@ -179,7 +179,7 @@
         with self.lock:
             ttl = ttl_seconds or self._calculate_adaptive_ttl(key)
             size_bytes = self._estimate_size(value)
-            
+
             entry = CacheEntry(
                 key=key,
                 value=value,
@@ -191,104 +191,112 @@
                 tags=tags or [],
                 metadata={"storage_tier": "memory"}
             )
-            
+
             # Decide storage tier
             if force_disk or size_bytes > self.max_memory_bytes * 0.1:
                 # Large items go directly to disk
                 self._store_to_disk(key, entry)
-                log.debug("Cached to disk (large item): %s (%.1fKB)", key, size_bytes / 1024)
+                log.debug("Cached to disk (large item): %s (%.1fKB)",
+                          key, size_bytes / 1024)
             else:
                 # Try memory first
                 if self._can_fit_in_memory(size_bytes):
                     self._store_to_memory(key, entry)
-                    log.debug("Cached to memory: %s (%.1fKB)", key, size_bytes / 1024)
+                    log.debug("Cached to memory: %s (%.1fKB)",
+                              key, size_bytes / 1024)
                 else:
                     # Memory full, evict and try again
                     self._evict_memory_entries(size_bytes)
                     if self._can_fit_in_memory(size_bytes):
                         self._store_to_memory(key, entry)
-                        log.debug("Cached to memory (after eviction): %s (%.1fKB)", key, size_bytes / 1024)
+                        log.debug(
+                            "Cached to memory (after eviction): %s (%.1fKB)", key, size_bytes / 1024)
                     else:
                         # Still can't fit, use disk
                         self._store_to_disk(key, entry)
-                        log.debug("Cached to disk (memory full): %s (%.1fKB)", key, size_bytes / 1024)
-    
+                        log.debug(
+                            "Cached to disk (memory full): %s (%.1fKB)", key, size_bytes / 1024)
+
     def delete(self, key: str) -> bool:
         """Delete item from cache."""
         with self.lock:
             removed = False
-            
+
             if key in self.memory_cache:
                 self._remove_memory_entry(key)
                 removed = True
-            
+
             if key in self.disk_cache_index:
                 self._remove_disk_entry(key)
                 removed = True
-            
+
             if removed:
                 log.debug("Cache entry deleted: %s", key)
-            
+
             return removed
-    
+
     def clear(self, tags: Optional[List[str]] = None) -> int:
         """Clear cache entries, optionally by tags."""
         with self.lock:
             cleared_count = 0
-            
+
             if tags is None:
                 # Clear all
-                cleared_count = len(self.memory_cache) + len(self.disk_cache_index)
+                cleared_count = len(self.memory_cache) + \
+                    len(self.disk_cache_index)
                 self.memory_cache.clear()
-                
+
                 for disk_path in self.disk_cache_index.values():
                     if disk_path.exists():
                         disk_path.unlink()
-                
+
                 self.disk_cache_index.clear()
                 self.stats = CacheStats()
-                
+
                 log.info("Cache cleared completely: %d entries", cleared_count)
-            
+
             else:
                 # Clear by tags
                 keys_to_remove = []
-                
+
                 for key, entry in self.memory_cache.items():
                     if any(tag in entry.tags for tag in tags):
                         keys_to_remove.append(key)
-                
+
                 for key in keys_to_remove:
                     self._remove_memory_entry(key)
                     cleared_count += 1
-                
+
                 # Check disk cache (need to load metadata)
                 keys_to_remove = []
                 for key in self.disk_cache_index:
                     try:
-                        entry = self._load_from_disk(self.disk_cache_index[key])
+                        entry = self._load_from_disk(
+                            self.disk_cache_index[key])
                         if any(tag in entry.tags for tag in tags):
                             keys_to_remove.append(key)
                     except Exception:
                         pass
-                
+
                 for key in keys_to_remove:
                     self._remove_disk_entry(key)
                     cleared_count += 1
-                
-                log.info("Cache cleared by tags %s: %d entries", tags, cleared_count)
-            
+
+                log.info("Cache cleared by tags %s: %d entries",
+                         tags, cleared_count)
+
             return cleared_count
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """Get comprehensive cache statistics."""
         with self.lock:
-            memory_size = sum(entry.size_bytes for entry in self.memory_cache.values())
+            memory_size = sum(
+                entry.size_bytes for entry in self.memory_cache.values())
             disk_size = sum(
-                path.stat().st_size for path in self.disk_cache_index.values() 
+                path.stat().st_size for path in self.disk_cache_index.values()
                 if path.exists()
             )
-            
+
             return {
                 "memory_cache": {
                     "entries": len(self.memory_cache),
@@ -312,119 +320,120 @@
                     "access_patterns": len(self.access_patterns)
                 }
             }
-    
+
     def optimize(self) -> None:
         """Perform cache optimization and cleanup."""
         with self.lock:
             log.info("üîÑ Starting cache optimization...")
-            
+
             # Clean expired entries
             expired_count = self._clean_expired_entries()
-            
+
             # Optimize memory layout
             optimized_count = self._optimize_memory_layout()
-            
+
             # Clean up disk cache
             disk_cleaned = self._cleanup_disk_cache()
-            
+
             # Update statistics
             self._update_cache_stats()
-            
+
             log.info(
                 "‚úÖ Cache optimization complete: expired=%d, optimized=%d, disk_cleaned=%d",
                 expired_count, optimized_count, disk_cleaned
             )
-    
+
     def _calculate_adaptive_ttl(self, key: str) -> float:
         """Calculate adaptive TTL based on access patterns."""
         if not self.enable_adaptive_ttl or key not in self.access_patterns:
             return self.default_ttl_seconds
-        
+
         # Analyze access pattern
         accesses = self.access_patterns[key]
         if len(accesses) < 2:
             return self.default_ttl_seconds
-        
+
         # Calculate access frequency
         time_span = accesses[-1] - accesses[0]
         frequency = len(accesses) / max(time_span, 1)  # accesses per second
-        
+
         # Adjust TTL based on frequency
         if frequency > 0.1:  # More than once every 10 seconds
             return self.default_ttl_seconds * 2  # Cache longer
         elif frequency < 0.01:  # Less than once every 100 seconds
             return self.default_ttl_seconds * 0.5  # Cache shorter
-        
+
         return self.default_ttl_seconds
-    
+
     def _record_access_pattern(self, key: str) -> None:
         """Record access pattern for predictive caching."""
         current_time = time.time()
-        
+
         if key not in self.access_patterns:
             self.access_patterns[key] = []
-        
+
         self.access_patterns[key].append(current_time)
-        
+
         # Keep only recent accesses (last hour)
         cutoff_time = current_time - 3600
         self.access_patterns[key] = [
             t for t in self.access_patterns[key] if t > cutoff_time
         ]
-    
+
     def _should_promote_to_memory(self, entry: CacheEntry) -> bool:
         """Determine if disk entry should be promoted to memory."""
         # Promote if frequently accessed
         access_frequency = entry.access_count / max(entry.age_seconds, 1)
         return access_frequency > 0.01 and entry.size_bytes < self.max_memory_bytes * 0.1
-    
+
     def _promote_to_memory(self, key: str, entry: CacheEntry) -> None:
         """Promote disk entry to memory cache."""
         if self._can_fit_in_memory(entry.size_bytes):
             self._store_to_memory(key, entry)
             self._remove_disk_entry(key)
             log.debug("Promoted to memory: %s", key)
-    
+
     def _can_fit_in_memory(self, size_bytes: int) -> bool:
         """Check if item can fit in memory cache."""
-        current_size = sum(entry.size_bytes for entry in self.memory_cache.values())
+        current_size = sum(
+            entry.size_bytes for entry in self.memory_cache.values())
         return current_size + size_bytes <= self.max_memory_bytes
-    
+
     def _store_to_memory(self, key: str, entry: CacheEntry) -> None:
         """Store entry in memory cache."""
         entry.metadata["storage_tier"] = "memory"
         self.memory_cache[key] = entry
         self.stats.total_entries += 1
         self.stats.total_size_bytes += entry.size_bytes
-    
+
     def _store_to_disk(self, key: str, entry: CacheEntry) -> None:
         """Store entry in disk cache."""
         cache_file = self.cache_dir / f"{self._hash_key(key)}.cache"
-        
+
         try:
             with cache_file.open('wb') as f:
                 pickle.dump(entry, f)
-            
+
             entry.metadata["storage_tier"] = "disk"
             self.disk_cache_index[key] = cache_file
             self.stats.total_entries += 1
             self.stats.total_size_bytes += entry.size_bytes
-            
+
         except Exception as e:
             log.error("Failed to store to disk cache: %s", e)
-    
+
     def _load_from_disk(self, disk_path: Path) -> CacheEntry:
         """Load entry from disk cache."""
         with disk_path.open('rb') as f:
             return pickle.load(f)
-    
+
     def _remove_memory_entry(self, key: str) -> None:
         """Remove entry from memory cache."""
         if key in self.memory_cache:
             entry = self.memory_cache.pop(key)
             self.stats.total_entries -= 1
             self.stats.total_size_bytes -= entry.size_bytes
-    
+
     def _remove_disk_entry(self, key: str) -> None:
         """Remove entry from disk cache."""
         if key in self.disk_cache_index:
@@ -437,52 +446,53 @@
                     disk_path.unlink()
                 except Exception as e:
                     log.warning("Failed to remove disk cache file: %s", e)
-    
+
     def _evict_memory_entries(self, needed_bytes: int) -> None:
         """Evict entries from memory cache using LFU + LRU policy."""
         if not self.memory_cache:
             return
-        
+
         # Sort by access frequency (ascending) and last access time (ascending)
         sorted_entries = sorted(
             self.memory_cache.items(),
-            key=lambda x: (x[1].access_count / max(x[1].age_seconds, 1), x[1].last_accessed)
+            key=lambda x: (x[1].access_count /
+                           max(x[1].age_seconds, 1), x[1].last_accessed)
         )
-        
+
         freed_bytes = 0
         evicted_keys = []
-        
+
         for key, entry in sorted_entries:
             if freed_bytes >= needed_bytes:
                 break
-            
+
             # Move to disk if possible, otherwise just remove
             if entry.size_bytes < self.max_disk_bytes * 0.1:
                 self._store_to_disk(key, entry)
-            
+
             self._remove_memory_entry(key)
             freed_bytes += entry.size_bytes
             evicted_keys.append(key)
             self.stats.evictions += 1
-        
+
         if evicted_keys:
-            log.debug("Evicted %d entries from memory (%.1fKB freed)", 
-                     len(evicted_keys), freed_bytes / 1024)
-    
+            log.debug("Evicted %d entries from memory (%.1fKB freed)",
+                      len(evicted_keys), freed_bytes / 1024)
+
     def _clean_expired_entries(self) -> int:
         """Clean expired entries from both caches."""
         expired_count = 0
-        
+
         # Clean memory cache
         expired_keys = [
-            key for key, entry in self.memory_cache.items() 
+            key for key, entry in self.memory_cache.items()
             if entry.is_expired
         ]
-        
+
         for key in expired_keys:
             self._remove_memory_entry(key)
             expired_count += 1
-        
+
         # Clean disk cache
         expired_keys = []
         for key, disk_path in self.disk_cache_index.items():
@@ -493,62 +503,65 @@
                         expired_keys.append(key)
             except Exception:
                 expired_keys.append(key)  # Remove corrupted entries
-        
+
         for key in expired_keys:
             self._remove_disk_entry(key)
             expired_count += 1
-        
+
         return expired_count
-    
+
     def _optimize_memory_layout(self) -> int:
         """Optimize memory cache layout for better performance."""
         # Move large, infrequently accessed items to disk
         optimized_count = 0
         keys_to_move = []
-        
+
         for key, entry in self.memory_cache.items():
             access_frequency = entry.access_count / max(entry.age_seconds, 1)
-            if (entry.size_bytes > self.max_memory_bytes * 0.05 and 
-                access_frequency < 0.001):  # Less than once per 1000 seconds
+            if (entry.size_bytes > self.max_memory_bytes * 0.05 and
+                    access_frequency < 0.001):  # Less than once per 1000 seconds
                 keys_to_move.append(key)
-        
+
         for key in keys_to_move:
             entry = self.memory_cache[key]
             self._store_to_disk(key, entry)
             self._remove_memory_entry(key)
             optimized_count += 1
-        
+
         return optimized_count
-    
+
     def _cleanup_disk_cache(self) -> int:
         """Clean up disk cache directory."""
         cleaned_count = 0
-        
+
         # Remove orphaned cache files
         cache_files = set(self.cache_dir.glob("*.cache"))
         indexed_files = set(self.disk_cache_index.values())
         orphaned_files = cache_files - indexed_files
-        
+
         for orphan in orphaned_files:
             try:
                 orphan.unlink()
                 cleaned_count += 1
             except Exception as e:
-                log.warning("Failed to remove orphaned cache file %s: %s", orphan, e)
-        
+                log.warning(
+                    "Failed to remove orphaned cache file %s: %s", orphan, e)
+
         return cleaned_count
-    
+
     def _update_cache_stats(self) -> None:
         """Update cache statistics."""
-        memory_size = sum(entry.size_bytes for entry in self.memory_cache.values())
+        memory_size = sum(
+            entry.size_bytes for entry in self.memory_cache.values())
         disk_size = sum(
-            path.stat().st_size for path in self.disk_cache_index.values() 
+            path.stat().st_size for path in self.disk_cache_index.values()
             if path.exists()
         )
-        
-        self.stats.total_entries = len(self.memory_cache) + len(self.disk_cache_index)
+
+        self.stats.total_entries = len(
+            self.memory_cache) + len(self.disk_cache_index)
         self.stats.total_size_bytes = memory_size + disk_size
-    
+
     def _estimate_size(self, obj: Any) -> int:
         """Estimate memory size of object."""
         try:
@@ -561,12 +574,12 @@
                 return sum(self._estimate_size(item) for item in obj[:10]) * len(obj) // 10
             elif isinstance(obj, dict):
                 sample_items = list(obj.items())[:10]
-                item_size = sum(self._estimate_size(k) + self._estimate_size(v) 
-                              for k, v in sample_items) 
+                item_size = sum(self._estimate_size(k) + self._estimate_size(v)
+                                for k, v in sample_items)
                 return item_size * len(obj) // max(len(sample_items), 1)
             else:
                 return 1024  # Default estimate
-    
+
     def _hash_key(self, key: str) -> str:
         """Generate hash for cache key."""
         return hashlib.md5(key.encode()).hexdigest()
@@ -582,7 +595,7 @@
     def decorator(func: Callable[..., T]) -> Callable[..., T]:
         def wrapper(*args, **kwargs) -> T:
             cache = cache_instance or get_global_cache()
-            
+
             # Generate cache key from function name and arguments
             key_data = {
                 "function": func.__name__,
@@ -590,18 +603,18 @@
                 "kwargs": sorted(kwargs.items())
             }
             cache_key = f"func_{hashlib.md5(str(key_data).encode()).hexdigest()}"
-            
+
             # Try to get from cache
             result = cache.get(cache_key)
             if result is not None:
                 return result
-            
+
             # Execute function and cache result
             result = func(*args, **kwargs)
             cache.set(cache_key, result, ttl_seconds=ttl_seconds, tags=tags)
-            
+
             return result
-        
+
         return wrapper
     return decorator
 
@@ -643,4 +656,4 @@
         default_ttl_seconds=default_ttl_seconds,
         cache_dir=cache_dir
     )
-    return _global_cache
\ No newline at end of file
+    return _global_cache
--- original/./etl/utils/retry.py
+++ fixed/./etl/utils/retry.py
@@ -33,7 +33,7 @@
 
 class RetryConfig:
     """Configuration for retry behavior."""
-    
+
     def __init__(
         self,
         max_attempts: int = 3,
@@ -59,19 +59,19 @@
             ConnectionError,
             TimeoutError
         ]
-    
+
     def should_retry(self, exception: Exception, attempt: int) -> bool:
         """Determine if an exception should trigger a retry."""
         if attempt >= self.max_attempts:
             return False
-        
+
         # Check ETL-specific retry logic
         if isinstance(exception, ETLError):
             return is_recoverable_error(exception)
-        
+
         # Check against configured recoverable exceptions
         return any(isinstance(exception, exc_type) for exc_type in self.recoverable_exceptions)
-    
+
     def get_delay(self, attempt: int, exception: Optional[Exception] = None) -> float:
         """Calculate delay before next retry attempt."""
         # Check if exception specifies a retry delay
@@ -79,22 +79,22 @@
             suggested_delay = get_retry_delay(exception)
             if suggested_delay:
                 return float(suggested_delay)
-        
+
         if self.exponential:
             delay = self.base_delay * (self.backoff_factor ** (attempt - 1))
         else:
             delay = self.base_delay
-        
+
         # Apply jitter to avoid thundering herd
         if self.jitter:
             delay *= (0.5 + random.random() * 0.5)
-        
+
         return min(delay, self.max_delay)
 
 
 class CircuitBreaker:
     """Circuit breaker pattern implementation for external service calls."""
-    
+
     def __init__(
         self,
         failure_threshold: int = 5,
@@ -104,18 +104,18 @@
         self.failure_threshold = failure_threshold
         self.recovery_timeout = recovery_timeout
         self.expected_exception = expected_exception
-        
+
         self.failure_count = 0
         self.last_failure_time: Optional[float] = None
         self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
-    
+
     def __call__(self, func: Callable) -> Callable:
         """Decorator to apply circuit breaker to a function."""
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             return self._call_with_circuit_breaker(func, *args, **kwargs)
         return wrapper
-    
+
     def _call_with_circuit_breaker(self, func: Callable, *args, **kwargs) -> Any:
         """Execute function with circuit breaker logic."""
         if self.state == "OPEN":
@@ -128,7 +128,7 @@
                     dependency=func.__name__,
                     context=ErrorContext(operation="circuit_breaker")
                 )
-        
+
         try:
             result = func(*args, **kwargs)
             self._on_success()
@@ -136,28 +136,29 @@
         except self.expected_exception as e:
             self._on_failure()
             raise
-    
+
     def _should_attempt_reset(self) -> bool:
         """Check if enough time has passed to attempt circuit breaker reset."""
         if self.last_failure_time is None:
             return True
         return time.time() - self.last_failure_time >= self.recovery_timeout
-    
+
     def _on_success(self) -> None:
         """Handle successful function execution."""
         if self.state == "HALF_OPEN":
             self.state = "CLOSED"
             log.info("‚úÖ Circuit breaker reset to CLOSED")
         self.failure_count = 0
-    
+
     def _on_failure(self) -> None:
         """Handle failed function execution."""
         self.failure_count += 1
         self.last_failure_time = time.time()
-        
+
         if self.failure_count >= self.failure_threshold:
             self.state = "OPEN"
-            log.warning("üî¥ Circuit breaker OPEN after %d failures", self.failure_count)
+            log.warning("üî¥ Circuit breaker OPEN after %d failures",
+                        self.failure_count)
 
 
 def retry_with_backoff(
@@ -169,7 +170,7 @@
     jitter: bool = True
 ) -> Callable:
     """Decorator to add retry logic with exponential backoff to a function.
-    
+
     Args:
         config: RetryConfig object with retry settings
         max_attempts: Maximum number of retry attempts
@@ -177,7 +178,7 @@
         backoff_factor: Exponential backoff multiplier
         max_delay: Maximum delay between retries (seconds)
         jitter: Add random jitter to delay
-    
+
     Returns:
         Decorated function with retry logic
     """
@@ -189,45 +190,49 @@
             max_delay=max_delay,
             jitter=jitter
         )
-    
+
     def decorator(func: Callable) -> Callable:
         @functools.wraps(func)
         def wrapper(*args, **kwargs) -> Any:
             last_exception = None
-            
+
             for attempt in range(1, config.max_attempts + 1):
                 try:
-                    log.debug("üîÑ Attempting %s (attempt %d/%d)", func.__name__, attempt, config.max_attempts)
+                    log.debug("üîÑ Attempting %s (attempt %d/%d)",
+                              func.__name__, attempt, config.max_attempts)
                     result = func(*args, **kwargs)
-                    
+
                     if attempt > 1:
-                        log.info("‚úÖ %s succeeded on attempt %d", func.__name__, attempt)
-                    
+                        log.info("‚úÖ %s succeeded on attempt %d",
+                                 func.__name__, attempt)
+
                     return result
-                
+
                 except Exception as e:
                     last_exception = e
-                    
+
                     if not config.should_retry(e, attempt):
-                        log.debug("‚ùå %s failed with non-recoverable error: %s", func.__name__, e)
+                        log.debug(
+                            "‚ùå %s failed with non-recoverable error: %s", func.__name__, e)
                         raise
-                    
+
                     if attempt == config.max_attempts:
-                        log.error("‚ùå %s failed after %d attempts: %s", func.__name__, attempt, e)
+                        log.error("‚ùå %s failed after %d attempts: %s",
+                                  func.__name__, attempt, e)
                         raise
-                    
+
                     delay = config.get_delay(attempt, e)
                     log.warning(
                         "‚ö†Ô∏è  %s failed (attempt %d/%d): %s. Retrying in %.1fs",
                         func.__name__, attempt, config.max_attempts, e, delay
                     )
-                    
+
                     time.sleep(delay)
-            
+
             # This should never be reached, but just in case
             if last_exception:
                 raise last_exception
-        
+
         return wrapper
     return decorator
 
@@ -238,18 +243,18 @@
     delay: float = 1.0
 ) -> Callable:
     """Simple retry decorator for specific exception types.
-    
+
     Args:
         exceptions: Exception type(s) to retry on
         max_attempts: Maximum number of attempts
         delay: Fixed delay between retries
-    
+
     Returns:
         Decorated function with retry logic
     """
     if not isinstance(exceptions, (list, tuple)):
         exceptions = [exceptions]
-    
+
     def decorator(func: Callable) -> Callable:
         @functools.wraps(func)
         def wrapper(*args, **kwargs) -> Any:
@@ -259,20 +264,21 @@
                 except Exception as e:
                     if not any(isinstance(e, exc_type) for exc_type in exceptions):
                         raise
-                    
+
                     if attempt == max_attempts - 1:
                         raise
-                    
-                    log.warning("Retrying %s due to %s", func.__name__, type(e).__name__)
+
+                    log.warning("Retrying %s due to %s",
+                                func.__name__, type(e).__name__)
                     time.sleep(delay)
-        
+
         return wrapper
     return decorator
 
 
 class RetryableOperation:
     """Context manager for retryable operations with detailed logging."""
-    
+
     def __init__(
         self,
         operation_name: str,
@@ -284,31 +290,32 @@
         self.context = context or {}
         self.attempt = 0
         self.start_time = 0.0
-    
+
     def __enter__(self):
         self.attempt += 1
         self.start_time = time.time()
-        log.debug("üöÄ Starting %s (attempt %d)", self.operation_name, self.attempt)
+        log.debug("üöÄ Starting %s (attempt %d)",
+                  self.operation_name, self.attempt)
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         duration = time.time() - self.start_time
-        
+
         if exc_type is None:
             log.debug("‚úÖ %s completed in %.2fs", self.operation_name, duration)
             return True
-        
+
         log.error(
             "‚ùå %s failed after %.2fs: %s",
             self.operation_name, duration, exc_val
         )
-        
+
         return False  # Don't suppress exceptions
-    
+
     def should_retry(self, exception: Exception) -> bool:
         """Check if operation should be retried."""
         return self.config.should_retry(exception, self.attempt)
-    
+
     def get_retry_delay(self, exception: Exception) -> float:
         """Get delay before next retry."""
         return self.config.get_delay(self.attempt, exception)
@@ -316,13 +323,13 @@
 
 class RetryStatistics:
     """Global retry statistics tracking."""
-    
+
     def __init__(self):
         self.operation_stats: Dict[str, Dict[str, int]] = {}
         self.total_retries = 0
         self.total_successes = 0
         self.total_failures = 0
-    
+
     def record_attempt(self, operation: str, success: bool, attempt_number: int):
         """Record a retry attempt."""
         if operation not in self.operation_stats:
@@ -332,21 +339,21 @@
                 "failures": 0,
                 "total_retry_attempts": 0
             }
-        
+
         stats = self.operation_stats[operation]
         stats["attempts"] += 1
-        
+
         if success:
             stats["successes"] += 1
             self.total_successes += 1
         else:
             stats["failures"] += 1
             self.total_failures += 1
-        
+
         if attempt_number > 1:
             stats["total_retry_attempts"] += (attempt_number - 1)
             self.total_retries += (attempt_number - 1)
-    
+
     def get_summary(self) -> Dict[str, Any]:
         """Get summary of retry statistics."""
         return {
@@ -354,8 +361,8 @@
             "total_retries": self.total_retries,
             "total_successes": self.total_successes,
             "total_failures": self.total_failures,
-            "success_rate": (self.total_successes / (self.total_successes + self.total_failures)) * 100 
-                           if (self.total_successes + self.total_failures) > 0 else 0,
+            "success_rate": (self.total_successes / (self.total_successes + self.total_failures)) * 100
+            if (self.total_successes + self.total_failures) > 0 else 0,
             "operations": self.operation_stats
         }
 
@@ -375,7 +382,8 @@
     base_delay=2.0,
     max_delay=120.0,
     backoff_factor=2.0,
-    recoverable_exceptions=[NetworkError, SourceError, ConnectionError, TimeoutError]
+    recoverable_exceptions=[NetworkError,
+                            SourceError, ConnectionError, TimeoutError]
 )
 
 DATABASE_RETRY_CONFIG = RetryConfig(
@@ -410,57 +418,65 @@
     """Enhanced retry decorator with statistics tracking."""
     if config is None:
         config = RetryConfig()
-    
+
     def decorator(func: Callable) -> Callable:
         @functools.wraps(func)
         def wrapper(*args, **kwargs) -> Any:
             last_exception = None
-            
+
             for attempt in range(1, config.max_attempts + 1):
                 try:
-                    log.debug("üîÑ %s attempt %d/%d", operation_name, attempt, config.max_attempts)
+                    log.debug("üîÑ %s attempt %d/%d", operation_name,
+                              attempt, config.max_attempts)
                     result = func(*args, **kwargs)
-                    
+
                     # Record successful attempt
                     _retry_stats.record_attempt(operation_name, True, attempt)
-                    
+
                     if attempt > 1:
-                        log.info("‚úÖ %s succeeded on attempt %d", operation_name, attempt)
-                    
+                        log.info("‚úÖ %s succeeded on attempt %d",
+                                 operation_name, attempt)
+
                     return result
-                
+
                 except Exception as e:
                     last_exception = e
-                    
+
                     # Classify unknown exceptions
                     if not isinstance(e, ETLError):
                         classified_error = classify_exception(e)
-                        log.debug("üîç Classified %s as %s", type(e).__name__, type(classified_error).__name__)
+                        log.debug("üîç Classified %s as %s", type(
+                            e).__name__, type(classified_error).__name__)
                         last_exception = classified_error
-                    
+
                     if not config.should_retry(last_exception, attempt):
-                        log.debug("‚ùå %s failed with non-recoverable error: %s", operation_name, last_exception)
-                        _retry_stats.record_attempt(operation_name, False, attempt)
+                        log.debug(
+                            "‚ùå %s failed with non-recoverable error: %s", operation_name, last_exception)
+                        _retry_stats.record_attempt(
+                            operation_name, False, attempt)
                         raise last_exception
-                    
+
                     if attempt == config.max_attempts:
-                        log.error("‚ùå %s failed after %d attempts: %s", operation_name, attempt, last_exception)
-                        _retry_stats.record_attempt(operation_name, False, attempt)
+                        log.error("‚ùå %s failed after %d attempts: %s",
+                                  operation_name, attempt, last_exception)
+                        _retry_stats.record_attempt(
+                            operation_name, False, attempt)
                         raise last_exception
-                    
+
                     delay = config.get_delay(attempt, last_exception)
                     log.warning(
                         "‚ö†Ô∏è  %s failed (attempt %d/%d): %s. Retrying in %.1fs",
                         operation_name, attempt, config.max_attempts, last_exception, delay
                     )
-                    
+
                     time.sleep(delay)
-            
+
             # This should never be reached, but just in case
             if last_exception:
-                _retry_stats.record_attempt(operation_name, False, config.max_attempts)
+                _retry_stats.record_attempt(
+                    operation_name, False, config.max_attempts)
                 raise last_exception
-        
+
         return wrapper
     return decorator
 
@@ -471,7 +487,7 @@
         nonlocal operation_name
         if operation_name is None:
             operation_name = func.__name__
-        
+
         # Select appropriate config based on operation name patterns
         if any(pattern in operation_name.lower() for pattern in ['http', 'request', 'download', 'fetch']):
             config = NETWORK_RETRY_CONFIG
@@ -483,6 +499,6 @@
             config = CONCURRENT_RETRY_CONFIG
         else:
             config = RetryConfig()  # Default config
-        
+
         return enhanced_retry_with_stats(operation_name, config)(func)
-    return decorator
\ No newline at end of file
+    return decorator
--- original/./etl/utils/arcpy_context.py
+++ fixed/./etl/utils/arcpy_context.py
@@ -14,36 +14,37 @@
 
 class ArcPyWorkspaceManager:
     """Context manager for safe ArcPy workspace management."""
-    
+
     def __init__(self, workspace: Union[str, Path], overwrite_output: bool = True):
         self.workspace = str(workspace)
         self.overwrite_output = overwrite_output
         self.original_workspace: Optional[str] = None
         self.original_overwrite: Optional[bool] = None
-        
+
     def __enter__(self) -> str:
         """Enter the context and set workspace."""
         # Store original settings
         self.original_workspace = arcpy.env.workspace
         self.original_overwrite = arcpy.env.overwriteOutput
-        
+
         # Set new workspace
         arcpy.env.workspace = self.workspace
         arcpy.env.overwriteOutput = self.overwrite_output
-        
+
         log.debug("Set ArcPy workspace to: %s", self.workspace)
         return self.workspace
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         """Exit the context and restore original settings."""
         try:
             # Restore original settings
             arcpy.env.workspace = self.original_workspace
             arcpy.env.overwriteOutput = self.original_overwrite
-            log.debug("Restored ArcPy workspace to: %s", self.original_workspace)
+            log.debug("Restored ArcPy workspace to: %s",
+                      self.original_workspace)
         except Exception as e:
             log.error("Failed to restore ArcPy workspace: %s", e)
-        
+
         # Clear any ArcPy geoprocessing results
         try:
             arcpy.ClearWorkspaceCache_management()
@@ -53,11 +54,11 @@
 
 class ArcPyEnvironmentManager:
     """Context manager for comprehensive ArcPy environment management."""
-    
+
     def __init__(self, **env_settings):
         self.env_settings = env_settings
         self.original_settings: Dict[str, Any] = {}
-        
+
     def __enter__(self) -> Dict[str, Any]:
         """Enter the context and set environment variables."""
         # Store original settings
@@ -66,9 +67,9 @@
                 self.original_settings[key] = getattr(arcpy.env, key)
                 setattr(arcpy.env, key, value)
                 log.debug("Set arcpy.env.%s = %s", key, value)
-        
+
         return self.env_settings
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         """Exit the context and restore original settings."""
         try:
@@ -76,10 +77,11 @@
             for key, original_value in self.original_settings.items():
                 if hasattr(arcpy.env, key):
                     setattr(arcpy.env, key, original_value)
-                    log.debug("Restored arcpy.env.%s = %s", key, original_value)
+                    log.debug("Restored arcpy.env.%s = %s",
+                              key, original_value)
         except Exception as e:
             log.error("Failed to restore ArcPy environment: %s", e)
-        
+
         # Clear workspace cache
         try:
             arcpy.ClearWorkspaceCache_management()
@@ -89,50 +91,51 @@
 
 class ArcPyTempWorkspace:
     """Context manager for temporary workspace operations."""
-    
+
     def __init__(self, prefix: str = "etl_temp_", cleanup: bool = True):
         self.prefix = prefix
         self.cleanup = cleanup
         self.temp_dir: Optional[Path] = None
         self.original_workspace: Optional[str] = None
-        
+
     def __enter__(self) -> Path:
         """Create temporary workspace and set as current."""
         # Create temporary directory
         self.temp_dir = Path(tempfile.mkdtemp(prefix=self.prefix))
-        
+
         # Store original workspace
         self.original_workspace = arcpy.env.workspace
-        
+
         # Set temporary workspace
         arcpy.env.workspace = str(self.temp_dir)
-        
+
         log.debug("Created temporary workspace: %s", self.temp_dir)
         return self.temp_dir
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         """Restore original workspace and cleanup if requested."""
         try:
             # Restore original workspace
             arcpy.env.workspace = self.original_workspace
-            log.debug("Restored workspace from temporary: %s", self.original_workspace)
-            
+            log.debug("Restored workspace from temporary: %s",
+                      self.original_workspace)
+
             # Clear workspace cache
             arcpy.ClearWorkspaceCache_management()
-            
+
             # Cleanup temporary directory if requested
             if self.cleanup and self.temp_dir and self.temp_dir.exists():
                 import shutil
                 shutil.rmtree(self.temp_dir, ignore_errors=True)
                 log.debug("Cleaned up temporary workspace: %s", self.temp_dir)
-                
+
         except Exception as e:
             log.error("Failed to cleanup temporary workspace: %s", e)
 
 
 @contextmanager
-def arcpy_workspace(workspace: Union[str, Path], 
-                   overwrite_output: bool = True) -> Generator[str, None, None]:
+def arcpy_workspace(workspace: Union[str, Path],
+                    overwrite_output: bool = True) -> Generator[str, None, None]:
     """Context manager for safe ArcPy workspace operations."""
     with ArcPyWorkspaceManager(workspace, overwrite_output) as ws:
         yield ws
@@ -146,8 +149,8 @@
 
 
 @contextmanager
-def arcpy_temp_workspace(prefix: str = "etl_temp_", 
-                        cleanup: bool = True) -> Generator[Path, None, None]:
+def arcpy_temp_workspace(prefix: str = "etl_temp_",
+                         cleanup: bool = True) -> Generator[Path, None, None]:
     """Context manager for temporary workspace operations."""
     with ArcPyTempWorkspace(prefix, cleanup) as temp_ws:
         yield temp_ws
@@ -164,4 +167,4 @@
                 arcpy.ClearWorkspaceCache_management()
             except Exception as e:
                 log.debug("Failed to clear workspace cache: %s", e)
-    return wrapper
\ No newline at end of file
+    return wrapper
--- original/./etl/utils/logging_cfg.py
+++ fixed/./etl/utils/logging_cfg.py
@@ -9,7 +9,8 @@
 
 # ---------------------------------------------------------------------------
 
-LOG_DIR: Final = Path("logs")         # <‚Äî adjust if you want a different folder
+# <‚Äî adjust if you want a different folder
+LOG_DIR: Final = Path("logs")
 LOG_DIR.mkdir(parents=True, exist_ok=True)
 
 
@@ -18,7 +19,7 @@
     today = datetime.now().strftime("%Y%m%d")
 
     summary_file = LOG_DIR / f"etl-{today}.log"
-    debug_file   = LOG_DIR / f"etl-{today}-debug.log"
+    debug_file = LOG_DIR / f"etl-{today}-debug.log"
 
     cfg: dict = {
         "version": 1,
@@ -71,7 +72,7 @@
                 "propagate": False,
             },
             "etl.utils.io": {
-                "level": "DEBUG", 
+                "level": "DEBUG",
                 "handlers": ["summary_file", "debug_file"],
                 "propagate": False,
             },
--- original/./etl/utils/run_summary.py
+++ fixed/./etl/utils/run_summary.py
@@ -5,6 +5,7 @@
 from collections import Counter
 from dataclasses import dataclass, field
 from typing import List
+
 
 @dataclass(slots=True)
 class Summary:
@@ -39,7 +40,7 @@
                 sum(self.staging.values()))
         lg.info("üìä SDE loading complete: %d loaded, %d errors",
                 self.sde["done"], self.sde["error"])
-        
+
         if self.errors:
             lg.info("üö® First errors:")
             for line in self.errors:
--- original/./etl/utils/naming.py
+++ fixed/./etl/utils/naming.py
@@ -9,7 +9,8 @@
 from .sanitize import slugify  # central helper keeps hyphens for readability
 
 _ILLEGAL_ARCGIS: Final = re.compile(r"[^A-Za-z0-9_]")   # stricter pattern
-_ARCGIS_MAX_LEN: Final = 128                             # FGDB feature class limit
+# FGDB feature class limit
+_ARCGIS_MAX_LEN: Final = 128
 
 
 # ---------------------------------------------------------------------------
@@ -30,26 +31,27 @@
         txt = f"_{txt}"                         # 4) SDE can‚Äôt start with digit
     return (txt or "unnamed")[:_ARCGIS_MAX_LEN]
 
+
 def generate_fc_name(authority: str, source: str) -> str:
     """
     üè∑Ô∏è Generate a feature class name with authority prefix.
-    
+
     Examples:
         generate_fc_name("RAA", "byggnader_sverige_point") ‚Üí "raa_byggnader_sverige_point"
         generate_fc_name("RAA", "raa_byggnader_sverige_point") ‚Üí "raa_byggnader_sverige_point"
         generate_fc_name("LSTD", "lstd_gi_betesmark_data") ‚Üí "lstd_gi_betesmark_data"
     """
     authority_lower = authority.lower()
-    
+
     # Clean the source string and sanitize it first
     source_clean = sanitize_for_arcgis_name(source)
-    
+
     # Check if it already starts with the authority prefix
     expected_prefix = f"{authority_lower}_"
     if source_clean.lower().startswith(expected_prefix):
         # Already has the prefix, just return it
         return source_clean[:_ARCGIS_MAX_LEN].rstrip("_")
-    
+
     # Add the authority prefix
     result = f"{authority_lower}_{source_clean}"
     return result[:_ARCGIS_MAX_LEN].rstrip("_")
@@ -57,7 +59,7 @@
 
 def sanitize_sde_name(name: str) -> str:
     """üßπ Sanitize feature class name for SDE compatibility.
-    
+
     SDE naming rules:
     - Must start with letter or underscore
     - Can contain letters, numbers, underscores
@@ -65,22 +67,24 @@
     - Max 128 characters (plenty of room)
     """
     original_name = name
-    
+
     # Replace problematic characters
     # Convert common problematic chars to underscores
     name = re.sub(r'[-\s\.]+', '_', name)  # hyphens, spaces, dots ‚Üí underscore
-    name = re.sub(r'[√•√§√∂]', lambda m: {'√•': 'a', '√§': 'a', '√∂': 'o'}[m.group()], name)  # Swedish chars
-    name = re.sub(r'[^\w]', '_', name)  # Any remaining non-word chars ‚Üí underscore
-    name = re.sub(r'_{2,}', '_', name)  # Multiple underscores ‚Üí single underscore
+    name = re.sub(r'[√•√§√∂]', lambda m: {'√•': 'a', '√§': 'a', '√∂': 'o'}[
+                  m.group()], name)  # Swedish chars
+    # Any remaining non-word chars ‚Üí underscore
+    name = re.sub(r'[^\w]', '_', name)
+    # Multiple underscores ‚Üí single underscore
+    name = re.sub(r'_{2,}', '_', name)
     name = name.strip('_')  # Remove leading/trailing underscores
-    
+
     # Ensure it starts with letter or underscore (not number)
     if name and name[0].isdigit():
         name = f"fc_{name}"
-    
+
     # Ensure not empty
     if not name:
         name = "unnamed_fc"
-        
+
     return name
-
--- original/./etl/utils/cleanup.py
+++ fixed/./etl/utils/cleanup.py
@@ -18,25 +18,26 @@
 
 def cleanup_downloads_folder() -> int:
     """üßπ Clean the downloads folder completely before pipeline run.
-    
+
     Returns:
         Number of items removed.
     """
     downloads_dir = paths.DOWNLOADS
-    
+
     if not downloads_dir.exists():
         log.info("üìÅ Downloads folder doesn't exist, nothing to clean")
         return 0
-    
+
     # Count items before cleanup
     items_before = sum(1 for _ in downloads_dir.rglob("*") if _.is_file())
-    
+
     if items_before == 0:
         log.info("üìÅ Downloads folder is already empty")
         return 0
-    
-    log.info("üßπ Cleaning downloads folder: %s (%d files)", downloads_dir, items_before)
-    
+
+    log.info("üßπ Cleaning downloads folder: %s (%d files)",
+             downloads_dir, items_before)
+
     try:
         # Remove all contents but keep the directory
         for item in downloads_dir.iterdir():
@@ -46,10 +47,10 @@
             else:
                 item.unlink()
                 log.debug("üóëÔ∏è Removed file: %s", item.name)
-        
+
         log.info("‚úÖ Downloads folder cleaned: removed %d items", items_before)
         return items_before
-        
+
     except Exception as e:
         log.error("‚ùå Failed to clean downloads folder: %s", e)
         raise
@@ -57,25 +58,26 @@
 
 def cleanup_staging_folder() -> int:
     """üßπ Clean the staging folder completely before pipeline run.
-    
+
     Returns:
         Number of items removed.
     """
     staging_dir = paths.STAGING
-    
+
     if not staging_dir.exists():
         log.info("üìÅ Staging folder doesn't exist, nothing to clean")
         return 0
-    
+
     # Count items before cleanup
     items_before = sum(1 for _ in staging_dir.rglob("*") if _.is_file())
-    
+
     if items_before == 0:
         log.info("üìÅ Staging folder is already empty")
         return 0
-    
-    log.info("üßπ Cleaning staging folder: %s (%d files)", staging_dir, items_before)
-    
+
+    log.info("üßπ Cleaning staging folder: %s (%d files)",
+             staging_dir, items_before)
+
     try:
         # Remove all contents but keep the directory
         for item in staging_dir.iterdir():
@@ -85,10 +87,10 @@
             else:
                 item.unlink()
                 log.debug("üóëÔ∏è Removed file: %s", item.name)
-        
+
         log.info("‚úÖ Staging folder cleaned: removed %d items", items_before)
         return items_before
-        
+
     except Exception as e:
         log.error("‚ùå Failed to clean staging folder: %s", e)
         raise
@@ -96,26 +98,26 @@
 
 class TempFileManager:
     """Manages temporary files and directories with automatic cleanup."""
-    
+
     def __init__(self, max_age_hours: int = 24):
         self.max_age_hours = max_age_hours
         self.tracked_paths: Set[Path] = set()
         self.lock = threading.RLock()
         self.cleanup_registered = False
-    
+
     def track_path(self, path: Path) -> None:
         """Track a path for cleanup."""
         with self.lock:
             self.tracked_paths.add(path)
             self._register_cleanup()
         log.debug("Started tracking path for cleanup: %s", path)
-    
+
     def untrack_path(self, path: Path) -> None:
         """Stop tracking a path."""
         with self.lock:
             self.tracked_paths.discard(path)
         log.debug("Stopped tracking path: %s", path)
-    
+
     def cleanup_path(self, path: Path) -> bool:
         """Clean up a specific path."""
         try:
@@ -129,37 +131,37 @@
         except Exception as e:
             log.warning("Failed to cleanup path %s: %s", path, e)
         return False
-    
+
     def cleanup_all(self) -> None:
         """Clean up all tracked paths."""
         with self.lock:
             paths_to_cleanup = list(self.tracked_paths)
             self.tracked_paths.clear()
-        
+
         cleaned_count = 0
         for path in paths_to_cleanup:
             if self.cleanup_path(path):
                 cleaned_count += 1
-        
+
         if cleaned_count > 0:
             log.info("Cleaned up %d temporary paths", cleaned_count)
-    
+
     def cleanup_old_temp_files(self) -> None:
         """Clean up old temporary files in system temp directory."""
         temp_dir = Path(tempfile.gettempdir())
         if not temp_dir.exists():
             return
-        
+
         cutoff_time = datetime.now() - timedelta(hours=self.max_age_hours)
         cutoff_timestamp = cutoff_time.timestamp()
-        
+
         cleaned_count = 0
         for item in temp_dir.iterdir():
             try:
                 # Check if it's an ETL temp file/directory
                 if not item.name.startswith(('etl_temp_', 'shp_', 'unzip_')):
                     continue
-                
+
                 # Check if it's old enough
                 if item.stat().st_mtime < cutoff_timestamp:
                     if item.is_dir():
@@ -168,13 +170,13 @@
                         item.unlink(missing_ok=True)
                     cleaned_count += 1
                     log.debug("Cleaned up old temp item: %s", item)
-                    
+
             except Exception as e:
                 log.debug("Failed to cleanup old temp item %s: %s", item, e)
-        
+
         if cleaned_count > 0:
             log.info("Cleaned up %d old temporary files", cleaned_count)
-    
+
     def _register_cleanup(self) -> None:
         """Register cleanup function to run at exit."""
         if not self.cleanup_registered:
@@ -208,28 +210,29 @@
 
 def cleanup_before_pipeline_run(clean_downloads: bool = True, clean_staging: bool = True) -> None:
     """üßπ Perform complete cleanup before pipeline run.
-    
+
     Args:
         clean_downloads: Whether to clean the downloads folder.
         clean_staging: Whether to clean the staging folder.
     """
     lg_sum = logging.getLogger("summary")
-    
+
     total_cleaned = 0
-    
+
     if clean_downloads:
         downloads_cleaned = cleanup_downloads_folder()
         total_cleaned += downloads_cleaned
-    
+
     if clean_staging:
         staging_cleaned = cleanup_staging_folder()
         total_cleaned += staging_cleaned
-    
+
     # Clean up old temporary files
     cleanup_old_temp_files()
-    
+
     if total_cleaned > 0:
-        lg_sum.info("üßπ Pre-pipeline cleanup complete: %d items removed", total_cleaned)
+        lg_sum.info(
+            "üßπ Pre-pipeline cleanup complete: %d items removed", total_cleaned)
     else:
         lg_sum.info("üìÅ Folders already clean, ready to start pipeline")
 
--- original/./etl/utils/gdb_utils.py
+++ fixed/./etl/utils/gdb_utils.py
@@ -39,7 +39,8 @@
         r"[-\s\.]+", "_", sanitized_name
     )  # hyphens, spaces, dots ‚Üí underscore
     sanitized_name = re.sub(
-        r"[√•√§√∂]", lambda m: {"√•": "a", "√§": "a", "√∂": "o"}[m.group()], sanitized_name
+        r"[√•√§√∂]", lambda m: {"√•": "a", "√§": "a",
+                             "√∂": "o"}[m.group()], sanitized_name
     )  # Swedish chars
     sanitized_name = re.sub(
         r"[^\w]", "_", sanitized_name
@@ -47,7 +48,8 @@
     sanitized_name = re.sub(
         r"_{2,}", "_", sanitized_name
     )  # Multiple underscores ‚Üí single underscore
-    sanitized_name = sanitized_name.strip("_")  # Remove leading/trailing underscores
+    # Remove leading/trailing underscores
+    sanitized_name = sanitized_name.strip("_")
 
     # Ensure it starts with letter or underscore (not number)
     if sanitized_name and sanitized_name[0].isdigit():
--- original/./etl/utils/http_session.py
+++ fixed/./etl/utils/http_session.py
@@ -106,7 +106,8 @@
                     session.close()
                     log.debug("Closed HTTP session for: %s", session_key)
                 except Exception as e:
-                    log.warning("Failed to close session %s: %s", session_key, e)
+                    log.warning("Failed to close session %s: %s",
+                                session_key, e)
             self._sessions.clear()
 
     def __del__(self):
@@ -173,7 +174,8 @@
     def session(self) -> requests.Session:
         """Get the HTTP session for this handler."""
         if self._session is None:
-            self._session = get_http_session(self.base_url, **self.session_config)
+            self._session = get_http_session(
+                self.base_url, **self.session_config)
         return self._session
 
     def close_session(self):
--- original/./etl/utils/performance_monitor.py
+++ fixed/./etl/utils/performance_monitor.py
@@ -31,22 +31,22 @@
     duration_seconds: int = 60
     severity: str = "warning"  # "info", "warning", "error", "critical"
     enabled: bool = True
-    
+
     def check(self, value: float, duration: float) -> bool:
         """Check if alert condition is met."""
         if not self.enabled:
             return False
-        
+
         if duration < self.duration_seconds:
             return False
-        
+
         if self.condition == "gt":
             return value > self.threshold
         elif self.condition == "lt":
             return value < self.threshold
         elif self.condition == "eq":
             return abs(value - self.threshold) < 0.01
-        
+
         return False
 
 
@@ -62,12 +62,12 @@
     message: str
     acknowledged: bool = False
     resolved: bool = False
-    
+
     def acknowledge(self) -> None:
         """Acknowledge the alert."""
         self.acknowledged = True
         log.info("Alert acknowledged: %s", self.rule_name)
-    
+
     def resolve(self) -> None:
         """Resolve the alert."""
         self.resolved = True
@@ -83,7 +83,7 @@
     system_metrics: Dict[str, Any]
     alerts: List[PerformanceAlert]
     recommendations: List[str]
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert report to dictionary."""
         return {
@@ -112,7 +112,7 @@
 
 class PerformanceMonitor:
     """Comprehensive performance monitoring system."""
-    
+
     def __init__(
         self,
         monitoring_interval: float = 10.0,
@@ -124,19 +124,20 @@
         self.history_retention_hours = history_retention_hours
         self.enable_alerts = enable_alerts
         self.report_interval_minutes = report_interval_minutes
-        
+
         # Data storage
-        self.performance_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
+        self.performance_history: Dict[str, deque] = defaultdict(
+            lambda: deque(maxlen=1000))
         self.system_history: deque = deque(maxlen=1000)
         self.alerts: List[PerformanceAlert] = []
         self.alert_rules: List[AlertRule] = []
         self.active_alerts: Dict[str, PerformanceAlert] = {}
-        
+
         # Monitoring state
         self.monitoring_active = False
         self.monitoring_thread: Optional[threading.Thread] = None
         self.stop_event = threading.Event()
-        
+
         # Statistics
         self.stats = {
             "total_operations": 0,
@@ -144,103 +145,107 @@
             "monitoring_start_time": time.time(),
             "last_report_time": time.time()
         }
-        
+
         # Setup default alert rules
         self._setup_default_alert_rules()
-        
+
         log.info("Initialized PerformanceMonitor")
-    
+
     def start_monitoring(self) -> None:
         """Start continuous performance monitoring."""
         if self.monitoring_active:
             log.warning("Performance monitoring already active")
             return
-        
+
         self.monitoring_active = True
         self.stop_event.clear()
-        
-        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
+
+        self.monitoring_thread = threading.Thread(
+            target=self._monitoring_loop, daemon=True)
         self.monitoring_thread.start()
-        
-        log.info("üîç Started performance monitoring (interval: %.1fs)", self.monitoring_interval)
-    
+
+        log.info("üîç Started performance monitoring (interval: %.1fs)",
+                 self.monitoring_interval)
+
     def stop_monitoring(self) -> None:
         """Stop performance monitoring."""
         if not self.monitoring_active:
             return
-        
+
         self.monitoring_active = False
         self.stop_event.set()
-        
+
         if self.monitoring_thread:
             self.monitoring_thread.join(timeout=5.0)
-        
+
         log.info("Stopped performance monitoring")
-    
+
     def record_performance(self, metrics: PerformanceMetrics) -> None:
         """Record performance metrics."""
         operation = metrics.operation_name
-        
+
         # Store in history
         self.performance_history[operation].append(metrics)
         self.stats["total_operations"] += 1
-        
+
         # Check alerts
         if self.enable_alerts:
             self._check_performance_alerts(metrics)
-        
+
         # Auto-tune if enabled
         tuner = get_global_tuner()
         tuner.record_performance(metrics)
-        
+
         log.debug(
             "Recorded performance: %s (%.2fs, %.2f items/s)",
             operation, metrics.duration, metrics.throughput_items_per_sec
         )
-    
+
     def add_alert_rule(self, rule: AlertRule) -> None:
         """Add performance alert rule."""
         self.alert_rules.append(rule)
         log.info("Added alert rule: %s", rule.name)
-    
+
     def remove_alert_rule(self, rule_name: str) -> bool:
         """Remove alert rule by name."""
         original_count = len(self.alert_rules)
-        self.alert_rules = [rule for rule in self.alert_rules if rule.name != rule_name]
-        
+        self.alert_rules = [
+            rule for rule in self.alert_rules if rule.name != rule_name]
+
         if len(self.alert_rules) < original_count:
             log.info("Removed alert rule: %s", rule_name)
             return True
-        
+
         return False
-    
+
     def get_performance_summary(self, operation: Optional[str] = None) -> Dict[str, Any]:
         """Get performance summary for operation or all operations."""
         if operation:
             if operation not in self.performance_history:
                 return {"error": f"No data for operation: {operation}"}
-            
+
             history = list(self.performance_history[operation])
             return self._summarize_operation_metrics(operation, history)
-        
+
         # Summary for all operations
         summary = {}
         for op_name, history in self.performance_history.items():
-            summary[op_name] = self._summarize_operation_metrics(op_name, list(history))
-        
+            summary[op_name] = self._summarize_operation_metrics(
+                op_name, list(history))
+
         return summary
-    
+
     def get_system_health(self) -> Dict[str, Any]:
         """Get current system health status."""
         if not self.system_history:
             return {"status": "no_data"}
-        
+
         recent_metrics = list(self.system_history)[-10:]  # Last 10 samples
-        
+
         avg_cpu = statistics.mean(m.cpu_percent for m in recent_metrics)
         avg_memory = statistics.mean(m.memory_percent for m in recent_metrics)
         min_disk_space = min(m.disk_free_gb for m in recent_metrics)
-        
+
         # Determine health status
         if avg_cpu > 90 or avg_memory > 95 or min_disk_space < 1:
             status = "critical"
@@ -250,7 +255,7 @@
             status = "degraded"
         else:
             status = "healthy"
-        
+
         return {
             "status": status,
             "cpu_percent": avg_cpu,
@@ -259,29 +264,31 @@
             "active_alerts": len(self.active_alerts),
             "monitoring_uptime_hours": (time.time() - self.stats["monitoring_start_time"]) / 3600
         }
-    
+
     def generate_report(self, hours_back: int = 1) -> PerformanceReport:
         """Generate comprehensive performance report."""
         end_time = time.time()
         start_time = end_time - (hours_back * 3600)
-        
+
         # Collect operation metrics
         operations = {}
         for op_name, history in self.performance_history.items():
             relevant_metrics = [
-                m for m in history 
+                m for m in history
                 if start_time <= m.start_time <= end_time
             ]
-            
+
             if relevant_metrics:
-                operations[op_name] = self._summarize_operation_metrics(op_name, relevant_metrics)
-        
+                operations[op_name] = self._summarize_operation_metrics(
+                    op_name, relevant_metrics)
+
         # Collect system metrics
         relevant_system_metrics = [
-            m for m in self.system_history 
-            if start_time <= time.time() <= end_time  # Approximate, system metrics don't have timestamps
+            m for m in self.system_history
+            # Approximate, system metrics don't have timestamps
+            if start_time <= time.time() <= end_time
         ]
-        
+
         system_metrics = {}
         if relevant_system_metrics:
             system_metrics = {
@@ -290,16 +297,17 @@
                 "min_disk_free_gb": min(m.disk_free_gb for m in relevant_system_metrics),
                 "pressure_periods": sum(1 for m in relevant_system_metrics if m.is_under_pressure)
             }
-        
+
         # Collect alerts
         relevant_alerts = [
-            alert for alert in self.alerts 
+            alert for alert in self.alerts
             if start_time <= alert.timestamp <= end_time
         ]
-        
+
         # Generate recommendations
-        recommendations = self._generate_recommendations(operations, system_metrics, relevant_alerts)
-        
+        recommendations = self._generate_recommendations(
+            operations, system_metrics, relevant_alerts)
+
         report = PerformanceReport(
             period_start=start_time,
             period_end=end_time,
@@ -308,51 +316,52 @@
             alerts=relevant_alerts,
             recommendations=recommendations
         )
-        
+
         self.stats["last_report_time"] = time.time()
-        
+
         return report
-    
+
     def save_report(self, report: PerformanceReport, file_path: Path) -> None:
         """Save performance report to file."""
         try:
             with file_path.open('w') as f:
                 json.dump(report.to_dict(), f, indent=2)
-            
+
             log.info("Performance report saved to %s", file_path)
         except Exception as e:
             log.error("Failed to save performance report: %s", e)
-    
+
     def _monitoring_loop(self) -> None:
         """Main monitoring loop."""
         next_report_time = time.time() + (self.report_interval_minutes * 60)
-        
+
         while not self.stop_event.wait(self.monitoring_interval):
             try:
                 # Collect system metrics
                 system_resources = self._get_system_resources()
                 self.system_history.append(system_resources)
-                
+
                 # Check system alerts
                 if self.enable_alerts:
                     self._check_system_alerts(system_resources)
-                
+
                 # Generate periodic reports
                 if time.time() >= next_report_time:
                     self._generate_periodic_report()
                     next_report_time = time.time() + (self.report_interval_minutes * 60)
-                
+
                 # Cleanup old data
                 self._cleanup_old_data()
-                
+
             except Exception as e:
                 log.warning("Error in monitoring loop: %s", e)
-    
+
     def _get_system_resources(self) -> SystemResources:
         """Get current system resources."""
         memory = psutil.virtual_memory()
-        disk = psutil.disk_usage(getattr(self, "_ROOT_PATH", Path.cwd().anchor))
-        
+        disk = psutil.disk_usage(
+            getattr(self, "_ROOT_PATH", Path.cwd().anchor))
+
         return SystemResources(
             cpu_percent=psutil.cpu_percent(interval=0.1),
             memory_percent=memory.percent,
@@ -360,20 +369,20 @@
             disk_free_gb=disk.free / (1024**3),
             network_connections=len(psutil.net_connections())
         )
-    
+
     def _check_performance_alerts(self, metrics: PerformanceMetrics) -> None:
         """Check performance metrics against alert rules."""
         for rule in self.alert_rules:
             if not rule.enabled:
                 continue
-            
+
             value = self._get_metric_value(metrics, rule.metric)
             if value is None:
                 continue
-            
+
             if rule.check(value, metrics.duration):
                 alert_key = f"{rule.name}_{metrics.operation_name}"
-                
+
                 if alert_key not in self.active_alerts:
                     alert = PerformanceAlert(
                         rule_name=rule.name,
@@ -384,13 +393,13 @@
                         timestamp=time.time(),
                         message=f"Performance alert: {rule.metric} = {value:.2f} (threshold: {rule.threshold})"
                     )
-                    
+
                     self.active_alerts[alert_key] = alert
                     self.alerts.append(alert)
                     self.stats["total_alerts"] += 1
-                    
+
                     log.warning("üö® Performance alert: %s", alert.message)
-    
+
     def _check_system_alerts(self, resources: SystemResources) -> None:
         """Check system resources against alert rules."""
         system_metrics = {
@@ -399,13 +408,13 @@
             "disk_free_gb": resources.disk_free_gb,
             "memory_available_gb": resources.memory_available_gb
         }
-        
+
         for metric, value in system_metrics.items():
             for rule in self.alert_rules:
                 if rule.metric == metric and rule.enabled:
                     if rule.check(value, self.monitoring_interval):
                         alert_key = f"{rule.name}_system"
-                        
+
                         if alert_key not in self.active_alerts:
                             alert = PerformanceAlert(
                                 rule_name=rule.name,
@@ -416,13 +425,13 @@
                                 timestamp=time.time(),
                                 message=f"System alert: {metric} = {value:.2f} (threshold: {rule.threshold})"
                             )
-                            
+
                             self.active_alerts[alert_key] = alert
                             self.alerts.append(alert)
                             self.stats["total_alerts"] += 1
-                            
+
                             log.warning("üö® System alert: %s", alert.message)
-    
+
     def _get_metric_value(self, metrics: PerformanceMetrics, metric_name: str) -> Optional[float]:
         """Extract metric value from performance metrics."""
         metric_map = {
@@ -434,18 +443,18 @@
             "worker_count": metrics.worker_count,
             "items_processed": metrics.items_processed
         }
-        
+
         return metric_map.get(metric_name)
-    
+
     def _summarize_operation_metrics(self, operation: str, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
         """Summarize metrics for an operation."""
         if not metrics:
             return {"error": "No metrics available"}
-        
+
         durations = [m.duration for m in metrics]
         throughputs = [m.throughput_items_per_sec for m in metrics]
         memory_usage = [m.memory_peak for m in metrics]
-        
+
         return {
             "operation_count": len(metrics),
             "duration": {
@@ -472,94 +481,102 @@
                 "total_bytes_processed": sum(m.bytes_processed for m in metrics)
             }
         }
-    
+
     def _generate_recommendations(
-        self, 
-        operations: Dict[str, Dict[str, Any]], 
-        system_metrics: Dict[str, Any], 
+        self,
+        operations: Dict[str, Dict[str, Any]],
+        system_metrics: Dict[str, Any],
         alerts: List[PerformanceAlert]
     ) -> List[str]:
         """Generate performance recommendations."""
         recommendations = []
-        
+
         # Check for high-variance operations
         for op_name, metrics in operations.items():
             if "duration" in metrics and metrics["duration"]["std"] > metrics["duration"]["avg"] * 0.5:
                 recommendations.append(
                     f"Operation '{op_name}' has high duration variance - consider investigating intermittent issues"
                 )
-        
+
         # Check for low throughput operations
         for op_name, metrics in operations.items():
             if "throughput" in metrics and metrics["throughput"]["avg"] < 0.5:
                 recommendations.append(
                     f"Operation '{op_name}' has low throughput - consider increasing concurrency or optimizing processing"
                 )
-        
+
         # Check for memory-intensive operations
         for op_name, metrics in operations.items():
             if "memory" in metrics and metrics["memory"]["avg_mb"] > 500:
                 recommendations.append(
                     f"Operation '{op_name}' uses high memory - consider batch processing or memory optimization"
                 )
-        
+
         # Check system-level issues
         if system_metrics and system_metrics.get("avg_cpu_percent", 0) > 80:
-            recommendations.append("High CPU usage detected - consider reducing concurrency or optimizing operations")
-        
+            recommendations.append(
+                "High CPU usage detected - consider reducing concurrency or optimizing operations")
+
         if system_metrics and system_metrics.get("avg_memory_percent", 0) > 85:
-            recommendations.append("High memory usage detected - consider increasing memory or optimizing data structures")
-        
+            recommendations.append(
+                "High memory usage detected - consider increasing memory or optimizing data structures")
+
         # Check cache performance
         cache_stats = get_global_cache().get_stats()
         if cache_stats["performance"]["hit_rate_percent"] < 50:
-            recommendations.append("Low cache hit rate - consider increasing cache size or adjusting TTL settings")
-        
+            recommendations.append(
+                "Low cache hit rate - consider increasing cache size or adjusting TTL settings")
+
         return recommendations
-    
+
     def _generate_periodic_report(self) -> None:
         """Generate and log periodic performance report."""
         try:
             report = self.generate_report(hours_back=1)
-            
+
             log.info("üìä Performance Report (last hour):")
             log.info(f"   Operations monitored: {len(report.operations)}")
-            log.info(f"   Active alerts: {len([a for a in report.alerts if not a.resolved])}")
+            log.info(
+                f"   Active alerts: {len([a for a in report.alerts if not a.resolved])}")
             log.info(f"   Recommendations: {len(report.recommendations)}")
-            
+
             if report.system_metrics:
-                log.info(f"   Avg CPU: {report.system_metrics.get('avg_cpu_percent', 0):.1f}%")
-                log.info(f"   Avg Memory: {report.system_metrics.get('avg_memory_percent', 0):.1f}%")
-            
-            for rec in report.recommendations[:3]:  # Show top 3 recommendations
+                log.info(
+                    f"   Avg CPU: {report.system_metrics.get('avg_cpu_percent', 0):.1f}%")
+                log.info(
+                    f"   Avg Memory: {report.system_metrics.get('avg_memory_percent', 0):.1f}%")
+
+            # Show top 3 recommendations
+            for rec in report.recommendations[:3]:
                 log.info(f"   üí° {rec}")
-            
+
         except Exception as e:
             log.error("Failed to generate periodic report: %s", e)
-    
+
     def _cleanup_old_data(self) -> None:
         """Clean up old performance data."""
         cutoff_time = time.time() - (self.history_retention_hours * 3600)
-        
+
         # Clean up performance history
         for operation in self.performance_history:
             history = self.performance_history[operation]
             # Keep only recent metrics
             while history and history[0].start_time < cutoff_time:
                 history.popleft()
-        
+
         # Clean up alerts
-        self.alerts = [alert for alert in self.alerts if alert.timestamp > cutoff_time]
-        
+        self.alerts = [
+            alert for alert in self.alerts if alert.timestamp > cutoff_time]
+
         # Clean up active alerts that are resolved
         resolved_alerts = [
-            key for key, alert in self.active_alerts.items() 
+            key for key, alert in self.active_alerts.items()
             if alert.resolved or alert.timestamp < cutoff_time
         ]
-        
+
         for key in resolved_alerts:
             del self.active_alerts[key]
-    
+
     def _setup_default_alert_rules(self) -> None:
         """Setup default performance alert rules."""
         default_rules = [
@@ -606,7 +623,7 @@
                 severity="critical"
             )
         ]
-        
+
         self.alert_rules.extend(default_rules)
         log.info("Setup %d default alert rules", len(default_rules))
 
@@ -628,10 +645,10 @@
 ) -> PerformanceMonitor:
     """Configure the global performance monitor."""
     global _global_monitor
-    
+
     # Stop existing monitor
     _global_monitor.stop_monitoring()
-    
+
     # Create new monitor
     _global_monitor = PerformanceMonitor(
         monitoring_interval=monitoring_interval,
@@ -639,7 +656,7 @@
         enable_alerts=enable_alerts,
         report_interval_minutes=report_interval_minutes
     )
-    
+
     return _global_monitor
 
 
@@ -658,18 +675,18 @@
     def decorator(func: Callable) -> Callable:
         def wrapper(*args, **kwargs):
             monitor = get_global_monitor()
-            
+
             # Record performance
             start_time = time.time()
             start_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-            
+
             try:
                 result = func(*args, **kwargs)
-                
+
                 # Create performance metrics
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=operation_name,
                     start_time=start_time,
@@ -682,15 +699,15 @@
                     worker_count=1,
                     items_processed=1
                 )
-                
+
                 monitor.record_performance(metrics)
                 return result
-                
+
             except Exception as e:
                 # Still record performance for failed operations
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=f"{operation_name}_failed",
                     start_time=start_time,
@@ -703,9 +720,9 @@
                     worker_count=1,
                     items_processed=0
                 )
-                
+
                 monitor.record_performance(metrics)
                 raise
-        
+
         return wrapper
-    return decorator
\ No newline at end of file
+    return decorator
--- original/./etl/utils/recovery.py
+++ fixed/./etl/utils/recovery.py
@@ -48,7 +48,7 @@
     description: str = ""
     priority: int = 0
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def execute(self) -> Any:
         """Execute the recovery action."""
         if self.action_func:
@@ -69,14 +69,14 @@
 
 class RecoveryManager:
     """Manages error recovery strategies and graceful degradation."""
-    
+
     def __init__(self):
         self.recovery_strategies: Dict[str, List[RecoveryAction]] = {}
         self.global_strategies: List[RecoveryAction] = []
         self.recovery_stats: Dict[str, Dict[str, int]] = {}
         self.degradation_level = 0
         self.max_degradation_level = 3
-    
+
     def register_recovery_strategy(
         self,
         error_type: Union[str, Type[Exception]],
@@ -88,11 +88,12 @@
         metadata: Optional[Dict[str, Any]] = None
     ) -> None:
         """Register a recovery strategy for a specific error type."""
-        error_key = error_type.__name__ if isinstance(error_type, type) else str(error_type)
-        
+        error_key = error_type.__name__ if isinstance(
+            error_type, type) else str(error_type)
+
         if error_key not in self.recovery_strategies:
             self.recovery_strategies[error_key] = []
-        
+
         recovery_action = RecoveryAction(
             strategy=strategy,
             action_func=action_func,
@@ -101,14 +102,16 @@
             priority=priority,
             metadata=metadata or {}
         )
-        
+
         self.recovery_strategies[error_key].append(recovery_action)
-        
+
         # Sort by priority (higher first)
-        self.recovery_strategies[error_key].sort(key=lambda x: x.priority, reverse=True)
-        
-        log.debug("üìù Registered recovery strategy for %s: %s", error_key, strategy.value)
-    
+        self.recovery_strategies[error_key].sort(
+            key=lambda x: x.priority, reverse=True)
+
+        log.debug("üìù Registered recovery strategy for %s: %s",
+                  error_key, strategy.value)
+
     def register_global_strategy(
         self,
         strategy: RecoveryStrategy,
@@ -125,12 +128,12 @@
             description=description,
             priority=priority
         )
-        
+
         self.global_strategies.append(recovery_action)
         self.global_strategies.sort(key=lambda x: x.priority, reverse=True)
-        
+
         log.debug("üìù Registered global recovery strategy: %s", strategy.value)
-    
+
     def recover_from_error(
         self,
         error: Exception,
@@ -139,7 +142,7 @@
     ) -> RecoveryResult:
         """Attempt to recover from an error using registered strategies."""
         error_type = type(error).__name__
-        
+
         # Update statistics
         if operation_context not in self.recovery_stats:
             self.recovery_stats[operation_context] = {
@@ -147,46 +150,53 @@
                 "successes": 0,
                 "failures": 0
             }
-        
+
         self.recovery_stats[operation_context]["attempts"] += 1
-        
+
         log.info(
             "üîÑ Attempting recovery from %s in %s: %s",
             error_type,
             operation_context,
             str(error)
         )
-        
+
         # Try specific strategies first
         strategies_to_try = []
-        
+
         # Add error-specific strategies
         if error_type in self.recovery_strategies:
             strategies_to_try.extend(self.recovery_strategies[error_type])
-        
+
         # Add global strategies
         strategies_to_try.extend(self.global_strategies)
-        
+
         # Sort by priority
         strategies_to_try.sort(key=lambda x: x.priority, reverse=True)
-        
+
         # Try each strategy
         for strategy in strategies_to_try:
             try:
-                log.debug("üîÑ Trying recovery strategy: %s", strategy.strategy.value)
-                
+                log.debug("üîÑ Trying recovery strategy: %s",
+                          strategy.strategy.value)
+
                 if strategy.strategy == RecoveryStrategy.SKIP:
-                    result = self._handle_skip_strategy(error, operation_context, strategy)
+                    result = self._handle_skip_strategy(
+                        error, operation_context, strategy)
                 elif strategy.strategy == RecoveryStrategy.FALLBACK:
-                    result = self._handle_fallback_strategy(error, operation_context, strategy, fallback_data)
+                    result = self._handle_fallback_strategy(
+                        error, operation_context, strategy, fallback_data)
                 elif strategy.strategy == RecoveryStrategy.PARTIAL:
-                    result = self._handle_partial_strategy(error, operation_context, strategy)
+                    result = self._handle_partial_strategy(
+                        error, operation_context, strategy)
                 elif strategy.strategy == RecoveryStrategy.DEGRADE:
-                    result = self._handle_degrade_strategy(error, operation_context, strategy)
+                    result = self._handle_degrade_strategy(
+                        error, operation_context, strategy)
                 elif strategy.strategy == RecoveryStrategy.MANUAL:
-                    result = self._handle_manual_strategy(error, operation_context, strategy)
+                    result = self._handle_manual_strategy(
+                        error, operation_context, strategy)
                 elif strategy.strategy == RecoveryStrategy.ABORT:
-                    result = self._handle_abort_strategy(error, operation_context, strategy)
+                    result = self._handle_abort_strategy(
+                        error, operation_context, strategy)
                 else:
                     # Custom strategy with action function
                     recovered_data = strategy.execute()
@@ -196,12 +206,13 @@
                         recovered_data=recovered_data,
                         message=f"Custom recovery strategy succeeded: {strategy.description}"
                     )
-                
+
                 if result.success:
                     self.recovery_stats[operation_context]["successes"] += 1
-                    log.info("‚úÖ Recovery successful using %s strategy", strategy.strategy.value)
+                    log.info("‚úÖ Recovery successful using %s strategy",
+                             strategy.strategy.value)
                     return result
-                    
+
             except Exception as recovery_error:
                 log.warning(
                     "‚ö†Ô∏è Recovery strategy %s failed: %s",
@@ -209,18 +220,18 @@
                     recovery_error
                 )
                 continue
-        
+
         # All strategies failed
         self.recovery_stats[operation_context]["failures"] += 1
         log.error("‚ùå All recovery strategies failed for %s", operation_context)
-        
+
         return RecoveryResult(
             success=False,
             strategy_used=RecoveryStrategy.ABORT,
             error=error,
             message=f"No recovery strategy succeeded for {error_type}"
         )
-    
+
     def _handle_skip_strategy(
         self,
         error: Exception,
@@ -229,14 +240,14 @@
     ) -> RecoveryResult:
         """Handle skip recovery strategy."""
         log.warning("‚è≠Ô∏è Skipping failed operation: %s", operation_context)
-        
+
         return RecoveryResult(
             success=True,
             strategy_used=RecoveryStrategy.SKIP,
             recovered_data=None,
             message=f"Skipped operation due to error: {error}"
         )
-    
+
     def _handle_fallback_strategy(
         self,
         error: Exception,
@@ -246,10 +257,10 @@
     ) -> RecoveryResult:
         """Handle fallback recovery strategy."""
         data_to_use = fallback_data or strategy.fallback_data
-        
+
         if data_to_use is None and strategy.action_func:
             data_to_use = strategy.action_func()
-        
+
         if data_to_use is not None:
             log.info("üîÑ Using fallback data for: %s", operation_context)
             return RecoveryResult(
@@ -258,14 +269,14 @@
                 recovered_data=data_to_use,
                 message=f"Used fallback data: {strategy.description}"
             )
-        
+
         return RecoveryResult(
             success=False,
             strategy_used=RecoveryStrategy.FALLBACK,
             error=error,
             message="No fallback data available"
         )
-    
+
     def _handle_partial_strategy(
         self,
         error: Exception,
@@ -274,16 +285,16 @@
     ) -> RecoveryResult:
         """Handle partial recovery strategy."""
         log.info("üîÑ Continuing with partial results for: %s", operation_context)
-        
+
         partial_data = strategy.execute() if strategy.action_func else []
-        
+
         return RecoveryResult(
             success=True,
             strategy_used=RecoveryStrategy.PARTIAL,
             recovered_data=partial_data,
             message=f"Continuing with partial results: {strategy.description}"
         )
-    
+
     def _handle_degrade_strategy(
         self,
         error: Exception,
@@ -292,19 +303,21 @@
     ) -> RecoveryResult:
         """Handle degradation recovery strategy."""
         if self.degradation_level >= self.max_degradation_level:
-            log.error("‚ùå Maximum degradation level reached, cannot degrade further")
+            log.error(
+                "‚ùå Maximum degradation level reached, cannot degrade further")
             return RecoveryResult(
                 success=False,
                 strategy_used=RecoveryStrategy.DEGRADE,
                 error=error,
                 message="Maximum degradation level reached"
             )
-        
+
         self.degradation_level += 1
-        log.warning("‚¨áÔ∏è Degrading service level to %d for: %s", self.degradation_level, operation_context)
-        
+        log.warning("‚¨áÔ∏è Degrading service level to %d for: %s",
+                    self.degradation_level, operation_context)
+
         degraded_data = strategy.execute() if strategy.action_func else None
-        
+
         return RecoveryResult(
             success=True,
             strategy_used=RecoveryStrategy.DEGRADE,
@@ -312,7 +325,7 @@
             message=f"Service degraded to level {self.degradation_level}: {strategy.description}",
             metadata={"degradation_level": self.degradation_level}
         )
-    
+
     def _handle_manual_strategy(
         self,
         error: Exception,
@@ -322,15 +335,16 @@
         """Handle manual intervention recovery strategy."""
         log.error("üîß Manual intervention required for: %s", operation_context)
         log.error("   Error: %s", str(error))
-        log.error("   Instructions: %s", strategy.description or "No specific instructions provided")
-        
+        log.error("   Instructions: %s",
+                  strategy.description or "No specific instructions provided")
+
         return RecoveryResult(
             success=False,
             strategy_used=RecoveryStrategy.MANUAL,
             error=error,
             message=f"Manual intervention required: {strategy.description}"
         )
-    
+
     def _handle_abort_strategy(
         self,
         error: Exception,
@@ -339,33 +353,34 @@
     ) -> RecoveryResult:
         """Handle abort recovery strategy."""
         log.error("üõë Aborting operation: %s", operation_context)
-        
+
         return RecoveryResult(
             success=False,
             strategy_used=RecoveryStrategy.ABORT,
             error=error,
             message=f"Operation aborted: {strategy.description}"
         )
-    
+
     def get_recovery_stats(self) -> Dict[str, Dict[str, Union[int, float]]]:
         """Get recovery statistics."""
         stats = {}
         for context, data in self.recovery_stats.items():
             total = data["attempts"]
-            success_rate = (data["successes"] / total * 100) if total > 0 else 0
+            success_rate = (data["successes"] / total *
+                            100) if total > 0 else 0
             stats[context] = {
                 **data,
                 "success_rate": success_rate
             }
         return stats
-    
+
     def reset_degradation_level(self) -> None:
         """Reset degradation level to normal."""
         old_level = self.degradation_level
         self.degradation_level = 0
         if old_level > 0:
             log.info("‚¨ÜÔ∏è Service degradation level reset from %d to 0", old_level)
-    
+
     def get_degradation_level(self) -> int:
         """Get current degradation level."""
         return self.degradation_level
@@ -379,24 +394,26 @@
 ):
     """Context manager for graceful degradation of operations."""
     mgr = recovery_manager or get_global_recovery_manager()
-    
+
     try:
         yield mgr
     except Exception as e:
-        log.warning("‚ö†Ô∏è Operation '%s' failed, attempting recovery", operation_name)
-        
+        log.warning(
+            "‚ö†Ô∏è Operation '%s' failed, attempting recovery", operation_name)
+
         recovery_result = mgr.recover_from_error(
             error=e,
             operation_context=operation_name,
             fallback_data=fallback_data
         )
-        
+
         if recovery_result.success:
             log.info("‚úÖ Recovery successful for '%s'", operation_name)
             # Return recovered data somehow - this is a limitation of context managers
             # In practice, you'd handle this at the application level
         else:
-            log.error("‚ùå Recovery failed for '%s', re-raising exception", operation_name)
+            log.error(
+                "‚ùå Recovery failed for '%s', re-raising exception", operation_name)
             raise
 
 
@@ -409,25 +426,27 @@
     def decorator(func: Callable[..., T]) -> Callable[..., T]:
         def wrapper(*args, **kwargs) -> T:
             mgr = recovery_manager or get_global_recovery_manager()
-            
+
             try:
                 return func(*args, **kwargs)
             except Exception as e:
-                log.warning("‚ö†Ô∏è Function '%s' failed, attempting recovery", operation_name)
-                
+                log.warning(
+                    "‚ö†Ô∏è Function '%s' failed, attempting recovery", operation_name)
+
                 recovery_result = mgr.recover_from_error(
                     error=e,
                     operation_context=operation_name,
                     fallback_data=fallback_data
                 )
-                
+
                 if recovery_result.success:
                     log.info("‚úÖ Recovery successful for '%s'", operation_name)
                     return recovery_result.recovered_data
                 else:
-                    log.error("‚ùå Recovery failed for '%s', re-raising exception", operation_name)
+                    log.error(
+                        "‚ùå Recovery failed for '%s', re-raising exception", operation_name)
                     raise
-        
+
         return wrapper
     return decorator
 
@@ -444,7 +463,7 @@
 def setup_default_recovery_strategies():
     """Setup default recovery strategies for common error types."""
     mgr = _global_recovery_manager
-    
+
     # Network errors - retry with degradation
     mgr.register_recovery_strategy(
         NetworkError,
@@ -452,14 +471,14 @@
         description="Reduce concurrent connections and retry",
         priority=3
     )
-    
+
     mgr.register_recovery_strategy(
         NetworkError,
         RecoveryStrategy.SKIP,
         description="Skip failed network operation",
         priority=1
     )
-    
+
     # Source errors - fallback to cached data
     mgr.register_recovery_strategy(
         SourceError,
@@ -467,14 +486,14 @@
         description="Use cached data if available",
         priority=2
     )
-    
+
     mgr.register_recovery_strategy(
         SourceError,
         RecoveryStrategy.SKIP,
         description="Skip unavailable source",
         priority=1
     )
-    
+
     # Data errors - use partial data
     mgr.register_recovery_strategy(
         DataError,
@@ -482,14 +501,14 @@
         description="Continue with valid data only",
         priority=2
     )
-    
+
     mgr.register_recovery_strategy(
         DataError,
         RecoveryStrategy.SKIP,
         description="Skip invalid data",
         priority=1
     )
-    
+
     # System errors - manual intervention
     mgr.register_recovery_strategy(
         SystemError,
@@ -497,14 +516,14 @@
         description="Check system resources and permissions",
         priority=3
     )
-    
+
     mgr.register_recovery_strategy(
         SystemError,
         RecoveryStrategy.SKIP,
         description="Skip system-dependent operation",
         priority=1
     )
-    
+
     # Processing errors - degrade quality
     mgr.register_recovery_strategy(
         ProcessingError,
@@ -512,14 +531,14 @@
         description="Reduce processing quality/complexity",
         priority=2
     )
-    
+
     mgr.register_recovery_strategy(
         ProcessingError,
         RecoveryStrategy.SKIP,
         description="Skip complex processing",
         priority=1
     )
-    
+
     # Pipeline errors - abort
     mgr.register_recovery_strategy(
         PipelineError,
@@ -527,7 +546,7 @@
         description="Critical pipeline failure",
         priority=1
     )
-    
+
     # Concurrent errors - retry with reduced concurrency
     mgr.register_recovery_strategy(
         ConcurrentError,
@@ -535,14 +554,14 @@
         description="Reduce concurrent workers",
         priority=2
     )
-    
+
     mgr.register_recovery_strategy(
         ConcurrentError,
         RecoveryStrategy.SKIP,
         description="Skip concurrent operation",
         priority=1
     )
-    
+
     # Global fallback - skip operation
     mgr.register_global_strategy(
         RecoveryStrategy.SKIP,
@@ -553,7 +572,7 @@
 
 class GracefulDegradationConfig:
     """Configuration for graceful degradation behavior."""
-    
+
     def __init__(self):
         self.max_concurrent_downloads = 5
         self.max_retry_attempts = 3
@@ -565,7 +584,7 @@
             2: {"concurrent_downloads": 1, "timeout": 120, "max_file_size": 25},
             3: {"concurrent_downloads": 1, "timeout": 300, "max_file_size": 10}
         }
-    
+
     def get_degraded_config(self, level: int) -> Dict[str, Any]:
         """Get configuration for a specific degradation level."""
         if level == 0:
@@ -574,9 +593,9 @@
                 "timeout": self.timeout_seconds,
                 "max_file_size": self.max_file_size_mb
             }
-        
+
         return self.degradation_thresholds.get(level, self.degradation_thresholds[3])
 
 
 # Initialize default strategies
-setup_default_recovery_strategies()
\ No newline at end of file
+setup_default_recovery_strategies()
--- original/./etl/utils/performance_optimizer.py
+++ fixed/./etl/utils/performance_optimizer.py
@@ -17,6 +17,8 @@
 import psutil
 
 # ... other code ...
+
+
 def test_system_resources_functionality(self):
     # Import the SystemResources class definition directly
     sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
@@ -131,7 +133,8 @@
                 peak_memory = current
 
         # Start memory monitoring
-        monitor_thread = threading.Thread(target=lambda: monitor_memory(), daemon=True)
+        monitor_thread = threading.Thread(
+            target=lambda: monitor_memory(), daemon=True)
         monitor_thread.start()
 
         try:
@@ -151,7 +154,8 @@
 
             # Trigger garbage collection if memory usage is high
             if self.get_memory_usage() > self.gc_threshold:
-                log.info("üßπ High memory usage detected, triggering garbage collection")
+                log.info(
+                    "üßπ High memory usage detected, triggering garbage collection")
                 collected = gc.collect()
                 log.debug("Garbage collection freed %d objects", collected)
 
@@ -160,7 +164,8 @@
         current_usage = self.get_memory_usage()
 
         if current_usage > self.memory_threshold:
-            log.info("üßπ Memory usage at %.1f%%, optimizing...", current_usage * 100)
+            log.info("üßπ Memory usage at %.1f%%, optimizing...",
+                     current_usage * 100)
 
             # Force garbage collection
             collected = gc.collect()
@@ -306,7 +311,8 @@
 
         # Analyze recent performance
         recent_metrics = performance_metrics[-5:]  # Last 5 operations
-        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
+        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / \
+            len(recent_metrics)
         avg_throughput = sum(m.throughput_items_per_sec for m in recent_metrics) / len(
             recent_metrics
         )
@@ -530,7 +536,8 @@
 
         # Ensure batch size is reasonable
         min_batch = 1
-        max_batch = min(1000, total_items)  # Never more than 1000 items per batch
+        # Never more than 1000 items per batch
+        max_batch = min(1000, total_items)
 
         optimal_batch = max(min_batch, min(items_per_batch, max_batch))
 
@@ -556,15 +563,17 @@
             return []
 
         total_items = len(items)
-        batch_size = self.calculate_optimal_batch_size(total_items, item_size_mb)
-
-        log.info("üîÑ Processing %d items in batches of %d", total_items, batch_size)
+        batch_size = self.calculate_optimal_batch_size(
+            total_items, item_size_mb)
+
+        log.info("üîÑ Processing %d items in batches of %d",
+                 total_items, batch_size)
 
         results = []
         processed_count = 0
 
         for i in range(0, total_items, batch_size):
-            batch = items[i : i + batch_size]
+            batch = items[i: i + batch_size]
             batch_num = i // batch_size + 1
             total_batches = (total_items + batch_size - 1) // batch_size
 
@@ -591,7 +600,8 @@
                         self.memory_optimizer.optimize_memory_usage()
 
                 except Exception as e:
-                    log.error("Batch %d/%d failed: %s", batch_num, total_batches, e)
+                    log.error("Batch %d/%d failed: %s",
+                              batch_num, total_batches, e)
                     # Continue with next batch rather than failing entirely
                     continue
 
@@ -767,7 +777,8 @@
                 peak_memory = current
 
         # Start memory monitoring
-        monitor_thread = threading.Thread(target=lambda: monitor_memory(), daemon=True)
+        monitor_thread = threading.Thread(
+            target=lambda: monitor_memory(), daemon=True)
         monitor_thread.start()
 
         try:
@@ -787,7 +798,8 @@
 
             # Trigger garbage collection if memory usage is high
             if self.get_memory_usage() > self.gc_threshold:
-                log.info("üßπ High memory usage detected, triggering garbage collection")
+                log.info(
+                    "üßπ High memory usage detected, triggering garbage collection")
                 collected = gc.collect()
                 log.debug("Garbage collection freed %d objects", collected)
 
@@ -796,7 +808,8 @@
         current_usage = self.get_memory_usage()
 
         if current_usage > self.memory_threshold:
-            log.info("üßπ Memory usage at %.1f%%, optimizing...", current_usage * 100)
+            log.info("üßπ Memory usage at %.1f%%, optimizing...",
+                     current_usage * 100)
 
             # Force garbage collection
             collected = gc.collect()
@@ -942,7 +955,8 @@
 
         # Analyze recent performance
         recent_metrics = performance_metrics[-5:]  # Last 5 operations
-        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
+        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / \
+            len(recent_metrics)
         avg_throughput = sum(m.throughput_items_per_sec for m in recent_metrics) / len(
             recent_metrics
         )
@@ -1166,7 +1180,8 @@
 
         # Ensure batch size is reasonable
         min_batch = 1
-        max_batch = min(1000, total_items)  # Never more than 1000 items per batch
+        # Never more than 1000 items per batch
+        max_batch = min(1000, total_items)
 
         optimal_batch = max(min_batch, min(items_per_batch, max_batch))
 
@@ -1192,15 +1207,17 @@
             return []
 
         total_items = len(items)
-        batch_size = self.calculate_optimal_batch_size(total_items, item_size_mb)
-
-        log.info("üîÑ Processing %d items in batches of %d", total_items, batch_size)
+        batch_size = self.calculate_optimal_batch_size(
+            total_items, item_size_mb)
+
+        log.info("üîÑ Processing %d items in batches of %d",
+                 total_items, batch_size)
 
         results = []
         processed_count = 0
 
         for i in range(0, total_items, batch_size):
-            batch = items[i : i + batch_size]
+            batch = items[i: i + batch_size]
             batch_num = i // batch_size + 1
             total_batches = (total_items + batch_size - 1) // batch_size
 
@@ -1227,7 +1244,8 @@
                         self.memory_optimizer.optimize_memory_usage()
 
                 except Exception as e:
-                    log.error("Batch %d/%d failed: %s", batch_num, total_batches, e)
+                    log.error("Batch %d/%d failed: %s",
+                              batch_num, total_batches, e)
                     # Continue with next batch rather than failing entirely
                     continue
 
--- original/./etl/utils/http_session_fix.py
+++ fixed/./etl/utils/http_session_fix.py
@@ -16,7 +16,7 @@
 
 class HTTPSessionManager:
     """Manages HTTP sessions with connection pooling and automatic cleanup."""
-    
+
     def __init__(self):
         self._sessions: Dict[str, requests.Session] = {}
         self._lock = threading.RLock()
@@ -27,7 +27,7 @@
             'backoff_factor': 0.3,
             'timeout': 30
         }
-    
+
     def get_session(self, base_url: Optional[str] = None, **config) -> requests.Session:
         """Get or create a session for the given base URL."""
         # Use domain as key for session reuse
@@ -36,24 +36,24 @@
             session_key = f"{parsed.scheme}://{parsed.netloc}"
         else:
             session_key = "default"
-        
+
         with self._lock:
             if session_key not in self._sessions:
                 self._sessions[session_key] = self._create_session(**config)
                 log.debug("Created new HTTP session for: %s", session_key)
-            
+
             return self._sessions[session_key]
-    
+
     def _create_session(self, **config) -> requests.Session:
         """Create a new session with proper configuration."""
         # Merge with default config
         session_config = {**self._default_config, **config}
-        
+
         # Extract timeout for proper handling
         default_timeout = session_config.pop('timeout', 30)
-        
+
         session = requests.Session()
-        
+
         # Configure connection pooling
         adapter = HTTPAdapter(
             pool_connections=session_config['pool_connections'],
@@ -64,24 +64,24 @@
                 status_forcelist=[500, 502, 503, 504]
             )
         )
-        
+
         session.mount('http://', adapter)
         session.mount('https://', adapter)
-        
+
         # Store timeout as custom attribute for use in requests
         session._default_timeout = default_timeout
-        
+
         # Override request method to use default timeout
         original_request = session.request
-        
+
         def request_with_timeout(method, url, **kwargs):
             # Use provided timeout or fall back to session default
             if 'timeout' not in kwargs:
                 kwargs['timeout'] = session._default_timeout
             return original_request(method, url, **kwargs)
-        
+
         session.request = request_with_timeout
-        
+
         # Set common headers
         session.headers.update({
             'User-Agent': 'ETL-Pipeline/1.0 (requests)',
@@ -89,9 +89,9 @@
             'Accept-Encoding': 'gzip, deflate',
             'Connection': 'keep-alive'
         })
-        
+
         return session
-    
+
     def close_session(self, base_url: Optional[str] = None):
         """Close a specific session."""
         if base_url:
@@ -99,13 +99,13 @@
             session_key = f"{parsed.scheme}://{parsed.netloc}"
         else:
             session_key = "default"
-        
+
         with self._lock:
             if session_key in self._sessions:
                 session = self._sessions.pop(session_key)
                 session.close()
                 log.debug("Closed HTTP session for: %s", session_key)
-    
+
     def close_all_sessions(self):
         """Close all sessions."""
         with self._lock:
@@ -114,9 +114,10 @@
                     session.close()
                     log.debug("Closed HTTP session for: %s", session_key)
                 except Exception as e:
-                    log.warning("Failed to close session %s: %s", session_key, e)
+                    log.warning("Failed to close session %s: %s",
+                                session_key, e)
             self._sessions.clear()
-    
+
     def __del__(self):
         """Cleanup on destruction."""
         self.close_all_sessions()
@@ -154,28 +155,29 @@
 
 class HTTPSessionHandler:
     """Base class for handlers that need HTTP session management."""
-    
+
     def __init__(self, base_url: Optional[str] = None, **session_config):
         self.base_url = base_url
         self.session_config = session_config
         self._session: Optional[requests.Session] = None
-    
+
     @property
     def session(self) -> requests.Session:
         """Get the HTTP session for this handler."""
         if self._session is None:
-            self._session = get_http_session(self.base_url, **self.session_config)
+            self._session = get_http_session(
+                self.base_url, **self.session_config)
         return self._session
-    
+
     def close_session(self):
         """Close the HTTP session."""
         if self._session:
             close_http_session(self.base_url)
             self._session = None
-    
+
     def __enter__(self):
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.close_session()
 
@@ -183,4 +185,4 @@
 # Cleanup function for application shutdown
 def cleanup_http_sessions():
     """Cleanup function to be called on application shutdown."""
-    close_all_http_sessions()
\ No newline at end of file
+    close_all_http_sessions()
--- original/./etl/utils/circuit_breaker.py
+++ fixed/./etl/utils/circuit_breaker.py
@@ -39,14 +39,14 @@
     last_failure_time: Optional[float] = None
     state_changes: int = 0
     last_state_change_time: Optional[float] = None
-    
+
     @property
     def failure_rate(self) -> float:
         """Calculate failure rate as percentage."""
         if self.total_calls == 0:
             return 0.0
         return (self.failed_calls / self.total_calls) * 100.0
-    
+
     @property
     def success_rate(self) -> float:
         """Calculate success rate as percentage."""
@@ -57,7 +57,7 @@
 
 class CircuitBreaker:
     """Circuit breaker pattern implementation with ETL-specific enhancements."""
-    
+
     def __init__(
         self,
         failure_threshold: int = 5,
@@ -70,7 +70,7 @@
         self.recovery_timeout = recovery_timeout
         self.half_open_max_calls = half_open_max_calls
         self.name = name or "unnamed_circuit_breaker"
-        
+
         # Default exceptions that trigger circuit breaker
         self.expected_exceptions = expected_exceptions or [
             NetworkError,
@@ -78,22 +78,22 @@
             ConnectionError,
             TimeoutError
         ]
-        
+
         # State management
         self.state = CircuitBreakerState.CLOSED
         self.stats = CircuitBreakerStats()
         self.half_open_calls = 0
-        
+
         # Thread safety
         self._lock = threading.Lock()
-    
+
     def __call__(self, func: Callable[..., T]) -> Callable[..., T]:
         """Decorator to apply circuit breaker to a function."""
         @functools.wraps(func)
         def wrapper(*args, **kwargs) -> T:
             return self._call_with_circuit_breaker(func, *args, **kwargs)
         return wrapper
-    
+
     def _call_with_circuit_breaker(self, func: Callable[..., T], *args, **kwargs) -> T:
         """Execute function with circuit breaker logic."""
         with self._lock:
@@ -114,11 +114,11 @@
                             }
                         )
                     )
-            
+
             # Track call in half-open state
             if self.state == CircuitBreakerState.HALF_OPEN:
                 self.half_open_calls += 1
-        
+
         # Execute the function
         start_time = time.time()
         try:
@@ -130,31 +130,32 @@
             if self._should_handle_exception(e):
                 self._on_failure(e)
             raise
-    
+
     def _should_attempt_reset(self) -> bool:
         """Check if enough time has passed to attempt circuit breaker reset."""
         if self.stats.last_failure_time is None:
             return True
         return time.time() - self.stats.last_failure_time >= self.recovery_timeout
-    
+
     def _should_handle_exception(self, exception: Exception) -> bool:
         """Check if the exception should trigger circuit breaker logic."""
         return any(isinstance(exception, exc_type) for exc_type in self.expected_exceptions)
-    
+
     def _on_success(self) -> None:
         """Handle successful function execution."""
         with self._lock:
             self.stats.total_calls += 1
             self.stats.successful_calls += 1
             self.stats.consecutive_failures = 0
-            
+
             if self.state == CircuitBreakerState.HALF_OPEN:
                 if self.half_open_calls >= self.half_open_max_calls:
                     self._transition_to_closed()
-                    log.info("‚úÖ Circuit breaker '%s' reset to CLOSED after successful test", self.name)
-            
+                    log.info(
+                        "‚úÖ Circuit breaker '%s' reset to CLOSED after successful test", self.name)
+
             log.debug("‚úÖ Circuit breaker '%s' recorded success", self.name)
-    
+
     def _on_failure(self, exception: Exception) -> None:
         """Handle failed function execution."""
         with self._lock:
@@ -162,10 +163,11 @@
             self.stats.failed_calls += 1
             self.stats.consecutive_failures += 1
             self.stats.last_failure_time = time.time()
-            
+
             if self.state == CircuitBreakerState.HALF_OPEN:
                 self._transition_to_open()
-                log.warning("üî¥ Circuit breaker '%s' reopened after failure during test", self.name)
+                log.warning(
+                    "üî¥ Circuit breaker '%s' reopened after failure during test", self.name)
             elif self.state == CircuitBreakerState.CLOSED:
                 if self.stats.consecutive_failures >= self.failure_threshold:
                     self._transition_to_open()
@@ -174,38 +176,40 @@
                         self.name,
                         self.stats.consecutive_failures
                     )
-            
-            log.debug("‚ùå Circuit breaker '%s' recorded failure: %s", self.name, exception)
-    
+
+            log.debug("‚ùå Circuit breaker '%s' recorded failure: %s",
+                      self.name, exception)
+
     def _transition_to_closed(self) -> None:
         """Transition to CLOSED state."""
         self.state = CircuitBreakerState.CLOSED
         self.half_open_calls = 0
         self.stats.state_changes += 1
         self.stats.last_state_change_time = time.time()
-    
+
     def _transition_to_open(self) -> None:
         """Transition to OPEN state."""
         self.state = CircuitBreakerState.OPEN
         self.half_open_calls = 0
         self.stats.state_changes += 1
         self.stats.last_state_change_time = time.time()
-    
+
     def _transition_to_half_open(self) -> None:
         """Transition to HALF_OPEN state."""
         self.state = CircuitBreakerState.HALF_OPEN
         self.half_open_calls = 0
         self.stats.state_changes += 1
         self.stats.last_state_change_time = time.time()
-        log.info("üîÑ Circuit breaker '%s' transitioned to HALF_OPEN for testing", self.name)
-    
+        log.info(
+            "üîÑ Circuit breaker '%s' transitioned to HALF_OPEN for testing", self.name)
+
     def reset(self) -> None:
         """Manually reset the circuit breaker to CLOSED state."""
         with self._lock:
             self._transition_to_closed()
             self.stats.consecutive_failures = 0
             log.info("üîÑ Circuit breaker '%s' manually reset", self.name)
-    
+
     def get_stats(self) -> CircuitBreakerStats:
         """Get circuit breaker statistics."""
         with self._lock:
@@ -218,11 +222,11 @@
                 state_changes=self.stats.state_changes,
                 last_state_change_time=self.stats.last_state_change_time
             )
-    
+
     def get_state(self) -> CircuitBreakerState:
         """Get current circuit breaker state."""
         return self.state
-    
+
     def is_call_permitted(self) -> bool:
         """Check if a call would be permitted without executing it."""
         with self._lock:
@@ -236,11 +240,11 @@
 
 class CircuitBreakerManager:
     """Manages multiple circuit breakers for different services."""
-    
+
     def __init__(self):
         self.circuit_breakers: Dict[str, CircuitBreaker] = {}
         self._lock = threading.Lock()
-    
+
     def get_circuit_breaker(
         self,
         name: str,
@@ -260,25 +264,25 @@
                     name=name
                 )
             return self.circuit_breakers[name]
-    
+
     def get_all_stats(self) -> Dict[str, CircuitBreakerStats]:
         """Get statistics for all circuit breakers."""
         with self._lock:
             return {name: cb.get_stats() for name, cb in self.circuit_breakers.items()}
-    
+
     def reset_all(self) -> None:
         """Reset all circuit breakers."""
         with self._lock:
             for cb in self.circuit_breakers.values():
                 cb.reset()
-    
+
     def get_summary(self) -> Dict[str, Any]:
         """Get summary of all circuit breakers."""
         with self._lock:
             return {
                 "total_breakers": len(self.circuit_breakers),
                 "states": {
-                    name: cb.get_state().value 
+                    name: cb.get_state().value
                     for name, cb in self.circuit_breakers.items()
                 },
                 "stats": self.get_all_stats()
@@ -321,7 +325,8 @@
         name=name,
         failure_threshold=failure_threshold,
         recovery_timeout=30.0,
-        expected_exceptions=[NetworkError, SourceError, ConnectionError, TimeoutError]
+        expected_exceptions=[NetworkError,
+                             SourceError, ConnectionError, TimeoutError]
     )
 
 
@@ -342,4 +347,4 @@
         failure_threshold=failure_threshold,
         recovery_timeout=15.0,
         expected_exceptions=[SystemError, IOError, OSError]
-    )
\ No newline at end of file
+    )
--- original/./etl/utils/http.py
+++ fixed/./etl/utils/http.py
@@ -32,18 +32,19 @@
     """
     if not header_value:
         return None
-    
+
     match = CONTENT_DISPOSITION_FILENAME_RE.search(header_value)
     if match:
         potential_filename: str = match.group(1).strip('" ')
         # If it's URL encoded (from filename*=UTF-8''...), unquote it
-        if "UTF-8''" in match.group(0).lower(): # Check if it was filename*
+        if "UTF-8''" in match.group(0).lower():  # Check if it was filename*
             return unquote(potential_filename)
         else:
             # For plain filename="value", ensure it's not accidentally percent-encoded
             # by the server sending a raw URL part. Unquoting here is generally safe.
-            return unquote(potential_filename) 
+            return unquote(potential_filename)
     return None
+
 
 def fetch_true_filename_parts(download_url: str, timeout: int = 10) -> Tuple[str, str]:
     """
@@ -59,7 +60,7 @@
         Returns a default name if no reliable filename can be determined.
     """
     true_filename_str: Optional[str] = None
-    final_url_after_redirects: str = download_url # Initialize with original URL
+    final_url_after_redirects: str = download_url  # Initialize with original URL
 
     log.debug("Attempting to determine true filename for URL: %s", download_url)
 
@@ -69,21 +70,26 @@
         # WARNING: This is generally insecure and should only be used if you trust the server
         # and cannot resolve SSL certificate issues otherwise.
         # context = ssl._create_unverified_context() # Use with caution
-        
+
         req = Request(download_url, method="HEAD")
         # Add a common user-agent to avoid potential blocks from servers
-        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')
+        req.add_header(
+            'User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')
 
-        with urlopen(req, timeout=timeout) as resp: # Potentially add context=context here
+        with urlopen(req, timeout=timeout) as resp:  # Potentially add context=context here
             # urlopen may follow redirects, get the final URL
             final_url_after_redirects = resp.geturl()
             cd_header: Optional[str] = resp.getheader("Content-Disposition")
             if cd_header:
-                log.debug("Found Content-Disposition for %s: %s", final_url_after_redirects, cd_header)
-                true_filename_str = _parse_filename_from_content_disposition(cd_header)
+                log.debug("Found Content-Disposition for %s: %s",
+                          final_url_after_redirects, cd_header)
+                true_filename_str = _parse_filename_from_content_disposition(
+                    cd_header)
                 if true_filename_str:
-                    log.info("    Derived filename from Content-Disposition: %s", true_filename_str)
-    except Exception as e: # Catching a broad range of exceptions (socket.timeout, URLError, etc.)
+                    log.info(
+                        "    Derived filename from Content-Disposition: %s", true_filename_str)
+    # Catching a broad range of exceptions (socket.timeout, URLError, etc.)
+    except Exception as e:
         log.warning(
             f"    HEAD request for Content-Disposition failed for {download_url}: {e}. Will fall back to URL basename."
         )
@@ -93,20 +99,21 @@
         url_path_name: str = Path(final_url_after_redirects).name
         if url_path_name:
             true_filename_str = unquote(url_path_name)
-            log.info("    Derived filename by unquoting URL basename ('%s'): %s", 
-                         final_url_after_redirects, true_filename_str)
+            log.info("    Derived filename by unquoting URL basename ('%s'): %s",
+                     final_url_after_redirects, true_filename_str)
         else:
-            log.warning("    Could not derive filename from URL path for %s", final_url_after_redirects)
+            log.warning(
+                "    Could not derive filename from URL path for %s", final_url_after_redirects)
             # Fallback to a generic name if all else fails
             return "downloaded_file_from_url", ".unknown"
 
-    if not true_filename_str: # Should be extremely rare if URL has a path component
-         return "unknown_filename", ".tmp"
+    if not true_filename_str:  # Should be extremely rare if URL has a path component
+        return "unknown_filename", ".tmp"
 
     # 3. Split into stem and extension
     # Ensure we handle cases where filename might not have an extension
     if '.' in true_filename_str:
         true_stem, true_ext_part = true_filename_str.rsplit('.', 1)
-        return true_stem, "." + true_ext_part.lower() # Ensure extension is lowercased
-    else: # No extension found in the derived filename
+        return true_stem, "." + true_ext_part.lower()  # Ensure extension is lowercased
+    else:  # No extension found in the derived filename
         return true_filename_str, ""
--- original/./etl/utils/concurrent.py
+++ fixed/./etl/utils/concurrent.py
@@ -40,30 +40,30 @@
     avg_duration: float = 0.0
     max_duration: float = 0.0
     min_duration: float = float('inf')
-    
+
     def update(self, result: ConcurrentResult):
         """Update statistics with a new result."""
         self.completed_tasks += 1
         self.total_duration += result.duration
-        
+
         if result.success:
             self.successful_tasks += 1
         else:
             self.failed_tasks += 1
-        
+
         self.max_duration = max(self.max_duration, result.duration)
         self.min_duration = min(self.min_duration, result.duration)
-        
+
         if self.completed_tasks > 0:
             self.avg_duration = self.total_duration / self.completed_tasks
-    
+
     @property
     def success_rate(self) -> float:
         """Calculate success rate as percentage."""
         if self.completed_tasks == 0:
             return 0.0
         return (self.successful_tasks / self.completed_tasks) * 100
-    
+
     @property
     def is_complete(self) -> bool:
         """Check if all tasks are completed."""
@@ -72,77 +72,80 @@
 
 class ConcurrentDownloadManager:
     """Manages concurrent download operations using only standard library."""
-    
+
     def __init__(self, max_workers: Optional[int] = None, timeout: Optional[float] = None):
         self.max_workers = max_workers or self._get_optimal_worker_count()
         self.timeout = timeout or 300.0
         self.stats = ConcurrentStats()
         self.lock = threading.RLock()
-        
-        log.info("Initialized ConcurrentDownloadManager with %d workers", self.max_workers)
-    
+
+        log.info(
+            "Initialized ConcurrentDownloadManager with %d workers", self.max_workers)
+
     def _get_optimal_worker_count(self) -> int:
         """Determine optimal number of workers based on CPU count."""
         cpu_count = os.cpu_count() or 4
         # Conservative approach: don't exceed 2x CPU count for I/O bound tasks
         return min(max(2, cpu_count), 8)
-    
+
     def execute_concurrent(
-        self, 
-        tasks: List[Tuple[Callable[..., T], Tuple, Dict[str, Any]]], 
+        self,
+        tasks: List[Tuple[Callable[..., T], Tuple, Dict[str, Any]]],
         task_names: Optional[List[str]] = None,
         fail_fast: bool = False
     ) -> List[ConcurrentResult]:
         """
         Execute multiple tasks concurrently using ThreadPoolExecutor.
-        
+
         Args:
             tasks: List of (function, args, kwargs) tuples
             task_names: Optional names for tasks (for logging)
             fail_fast: If True, stop on first failure
-            
+
         Returns:
             List of ConcurrentResult objects
         """
         if not tasks:
             return []
-        
+
         self.stats = ConcurrentStats()
         self.stats.total_tasks = len(tasks)
-        
+
         task_names = task_names or [f"task_{i}" for i in range(len(tasks))]
-        
-        log.info("Starting concurrent execution of %d tasks with %d workers", 
-                len(tasks), self.max_workers)
-        
+
+        log.info("Starting concurrent execution of %d tasks with %d workers",
+                 len(tasks), self.max_workers)
+
         results = []
-        
+
         with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
             # Submit all tasks
             future_to_task = {}
             for i, (func, args, kwargs) in enumerate(tasks):
-                future = executor.submit(self._execute_task, func, args, kwargs, task_names[i])
+                future = executor.submit(
+                    self._execute_task, func, args, kwargs, task_names[i])
                 future_to_task[future] = i
-            
+
             # Collect results as they complete
             for future in as_completed(future_to_task, timeout=self.timeout):
                 task_index = future_to_task[future]
-                
+
                 try:
                     result = future.result()
                     results.append((task_index, result))
-                    
+
                     with self.lock:
                         self.stats.update(result)
-                    
+
                     if fail_fast and not result.success:
-                        log.warning("Fail-fast enabled, cancelling remaining tasks")
+                        log.warning(
+                            "Fail-fast enabled, cancelling remaining tasks")
                         # Cancel remaining futures
                         for remaining_future in future_to_task:
                             if not remaining_future.done():
                                 remaining_future.cancel()
                         break
-                        
+
                 except Exception as e:
                     # Handle timeout or other executor errors
                     error_result = ConcurrentResult(
@@ -151,14 +154,14 @@
                         metadata={"task_name": task_names[task_index]}
                     )
                     results.append((task_index, error_result))
-                    
+
                     with self.lock:
                         self.stats.update(error_result)
-        
+
         # Sort results by original task order
         results.sort(key=lambda x: x[0])
         final_results = [result for _, result in results]
-        
+
         # Pad with error results for any missing tasks (cancelled, etc.)
         while len(final_results) < len(tasks):
             missing_result = ConcurrentResult(
@@ -167,56 +170,57 @@
                 metadata={"task_name": f"cancelled_task_{len(final_results)}"}
             )
             final_results.append(missing_result)
-        
+
         self._log_completion_stats()
         return final_results
-    
+
     def _execute_task(self, func: Callable, args: Tuple, kwargs: Dict, task_name: str) -> ConcurrentResult:
         """Execute a single task with error handling and timing."""
         start_time = time.time()
-        
+
         try:
             result = func(*args, **kwargs)
             duration = time.time() - start_time
-            
+
             return ConcurrentResult(
                 success=True,
                 result=result,
                 duration=duration,
                 metadata={"task_name": task_name}
             )
-            
+
         except Exception as e:
             duration = time.time() - start_time
-            log.debug("Task '%s' failed after %.2fs: %s", task_name, duration, e)
-            
+            log.debug("Task '%s' failed after %.2fs: %s",
+                      task_name, duration, e)
+
             return ConcurrentResult(
                 success=False,
                 error=e,
                 duration=duration,
                 metadata={"task_name": task_name}
             )
-    
+
     def _log_completion_stats(self):
         """Log completion statistics."""
         log.info("Concurrent execution completed: %d/%d successful (%.1f%%), "
-                "avg=%.2fs, max=%.2fs, total=%.2fs",
-                self.stats.successful_tasks,
-                self.stats.total_tasks,
-                self.stats.success_rate,
-                self.stats.avg_duration,
-                self.stats.max_duration,
-                self.stats.total_duration)
+                 "avg=%.2fs, max=%.2fs, total=%.2fs",
+                 self.stats.successful_tasks,
+                 self.stats.total_tasks,
+                 self.stats.success_rate,
+                 self.stats.avg_duration,
+                 self.stats.max_duration,
+                 self.stats.total_duration)
 
 
 class ConcurrentLayerDownloader:
     """Specialized downloader for REST API layers with concurrent processing."""
-    
+
     def __init__(self, max_workers: int = 5, timeout: float = 300.0):
         self.manager = ConcurrentDownloadManager(max_workers, timeout)
-    
+
     def download_layers_concurrent(
-        self, 
+        self,
         handler,  # RestApiDownloadHandler instance
         layers_info: List[Dict[str, Any]],
         fail_fast: bool = False
@@ -224,15 +228,16 @@
         """Download multiple layers concurrently."""
         if not layers_info:
             return []
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for layer_info in layers_info:
-            layer_name = layer_info.get("name", f"layer_{layer_info.get('id', 'unknown')}")
+            layer_name = layer_info.get(
+                "name", f"layer_{layer_info.get('id', 'unknown')}")
             task_names.append(f"layer_{layer_name}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._fetch_layer_data,
@@ -240,17 +245,17 @@
                 {"layer_metadata_from_service": layer_info.get("metadata")}
             )
             tasks.append(task)
-        
+
         log.info("Starting concurrent download of %d layers", len(layers_info))
         return self.manager.execute_concurrent(tasks, task_names, fail_fast)
 
 
 class ConcurrentCollectionDownloader:
     """Specialized downloader for OGC API collections with concurrent processing."""
-    
+
     def __init__(self, max_workers: int = 3, timeout: float = 600.0):
         self.manager = ConcurrentDownloadManager(max_workers, timeout)
-    
+
     def download_collections_concurrent(
         self,
         handler,  # OgcApiDownloadHandler instance
@@ -260,15 +265,15 @@
         """Download multiple collections concurrently."""
         if not collections:
             return []
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for collection in collections:
             collection_id = collection.get("id", "unknown")
             task_names.append(f"collection_{collection_id}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._fetch_collection,
@@ -276,17 +281,18 @@
                 {}
             )
             tasks.append(task)
-        
-        log.info("Starting concurrent download of %d collections", len(collections))
+
+        log.info("Starting concurrent download of %d collections",
+                 len(collections))
         return self.manager.execute_concurrent(tasks, task_names, fail_fast)
 
 
 class ConcurrentFileDownloader:
     """Specialized downloader for file downloads with concurrent processing."""
-    
+
     def __init__(self, max_workers: int = 4, timeout: float = 1800.0):
         self.manager = ConcurrentDownloadManager(max_workers, timeout)
-    
+
     def download_files_concurrent(
         self,
         handler,  # FileDownloadHandler instance
@@ -296,14 +302,14 @@
         """Download multiple files concurrently."""
         if not file_stems:
             return []
-        
+
         # Prepare tasks for concurrent execution
         tasks = []
         task_names = []
-        
+
         for file_stem in file_stems:
             task_names.append(f"file_{file_stem}")
-            
+
             # Create task tuple: (function, args, kwargs)
             task = (
                 handler._download_single_file_stem,
@@ -311,14 +317,14 @@
                 {}
             )
             tasks.append(task)
-        
+
         log.info("Starting concurrent download of %d files", len(file_stems))
         return self.manager.execute_concurrent(tasks, task_names, fail_fast)
 
 
 @contextmanager
 def concurrent_download_manager(
-    max_workers: Optional[int] = None, 
+    max_workers: Optional[int] = None,
     timeout: Optional[float] = None
 ) -> Generator[ConcurrentDownloadManager, None, None]:
     """Context manager for concurrent download operations."""
@@ -357,4 +363,4 @@
     global _file_downloader
     if _file_downloader is None:
         _file_downloader = ConcurrentFileDownloader()
-    return _file_downloader
\ No newline at end of file
+    return _file_downloader
--- original/./etl/utils/adaptive_tuning.py
+++ fixed/./etl/utils/adaptive_tuning.py
@@ -37,12 +37,14 @@
     success_rate: float
     sample_count: int
     established_at: float
-    
+
     def is_degraded(self, current_metrics: PerformanceMetrics, threshold: float = 0.2) -> bool:
         """Check if current performance is degraded compared to baseline."""
-        duration_increase = (current_metrics.duration - self.avg_duration) / self.avg_duration # type: ignore
-        throughput_decrease = (self.avg_throughput - current_metrics.throughput_items_per_sec) / self.avg_throughput
-        
+        duration_increase = (current_metrics.duration -
+                             self.avg_duration) / self.avg_duration  # type: ignore
+        throughput_decrease = (
+            self.avg_throughput - current_metrics.throughput_items_per_sec) / self.avg_throughput
+
         return duration_increase > threshold or throughput_decrease > threshold
 
 
@@ -56,26 +58,27 @@
     confidence: float  # 0.0 to 1.0
     expected_impact: str  # "positive", "negative", "neutral"
     priority: int = 0
-    
+
     def apply(self, config: Dict[str, Any]) -> bool:
         """Apply the tuning action to configuration."""
         try:
             old_value = config.get(self.parameter, self.current_value)
             config[self.parameter] = self.new_value
-            
+
             log.info(
                 "üîß Applied tuning: %s = %s ‚Üí %s (reason: %s, confidence: %.2f)",
                 self.parameter, old_value, self.new_value, self.reason, self.confidence
             )
             return True
         except Exception as e:
-            log.error("Failed to apply tuning action for %s: %s", self.parameter, e)
+            log.error("Failed to apply tuning action for %s: %s",
+                      self.parameter, e)
             return False
 
 
 class AdaptivePerformanceTuner:
     """Adaptive performance tuning engine that learns and optimizes."""
-    
+
     def __init__(
         self,
         strategy: TuningStrategy = TuningStrategy.BALANCED,
@@ -85,12 +88,13 @@
         self.strategy = strategy
         self.learning_window_size = learning_window_size
         self.min_confidence_threshold = min_confidence_threshold
-        
+
         # Performance tracking
         self.performance_history: Dict[str, deque] = {}
         self.baselines: Dict[str, PerformanceBaseline] = {}
-        self.tuning_history: List[Tuple[TuningAction, float]] = []  # (action, timestamp)
-        
+        # (action, timestamp)
+        self.tuning_history: List[Tuple[TuningAction, float]] = []
+
         # Tuning parameters and their ranges
         self.tunable_parameters = {
             "concurrent_download_workers": {
@@ -126,142 +130,153 @@
                 "impact": "throughput", "sensitivity": "medium"
             }
         }
-        
+
         # System monitoring
         self.system_monitor = SystemMonitor()
-        
+
         # Thread safety
         self.lock = threading.RLock()
-        
-        log.info("Initialized AdaptivePerformanceTuner with %s strategy", strategy.value)
-    
+
+        log.info(
+            "Initialized AdaptivePerformanceTuner with %s strategy", strategy.value)
+
     def record_performance(self, metrics: PerformanceMetrics) -> None:
         """Record performance metrics for learning."""
         with self.lock:
             operation = metrics.operation_name
-            
+
             # Initialize history if needed
             if operation not in self.performance_history:
-                self.performance_history[operation] = deque(maxlen=self.learning_window_size)
-            
+                self.performance_history[operation] = deque(
+                    maxlen=self.learning_window_size)
+
             # Add to history
             self.performance_history[operation].append(metrics)
-            
+
             # Update or establish baseline
             self._update_baseline(operation)
-            
+
             # Check if tuning is needed
             if self._should_tune(operation):
                 tuning_actions = self._generate_tuning_actions(operation)
                 if tuning_actions:
-                    log.info("üéØ Generated %d tuning actions for %s", len(tuning_actions), operation)
+                    log.info("üéØ Generated %d tuning actions for %s",
+                             len(tuning_actions), operation)
                     # The record_performance method should not return tuning actions directly.
-    
+
     def tune_configuration(self, config: Dict[str, Any], operation: str) -> List[TuningAction]:
         """Generate tuning actions for configuration optimization."""
         with self.lock:
             if operation not in self.performance_history:
-                log.debug("No performance history for %s, skipping tuning", operation)
+                log.debug(
+                    "No performance history for %s, skipping tuning", operation)
                 return []
-            
+
             actions = self._generate_tuning_actions(operation)
-            
+
             # Filter actions by confidence
             high_confidence_actions = [
-                action for action in actions 
+                action for action in actions
                 if action.confidence >= self.min_confidence_threshold
             ]
-            
+
             # Sort by priority and confidence
-            high_confidence_actions.sort(key=lambda x: (x.priority, x.confidence), reverse=True)
-            
+            high_confidence_actions.sort(key=lambda x: (
+                x.priority, x.confidence), reverse=True)
+
             return high_confidence_actions
-    
+
     def apply_tuning_actions(self, actions: List[TuningAction], config: Dict[str, Any]) -> int:
         """Apply tuning actions to configuration."""
         applied_count = 0
-        
+
         for action in actions:
             if action.apply(config):
                 self.tuning_history.append((action, time.time()))
                 applied_count += 1
-        
+
         if applied_count > 0:
             log.info("‚úÖ Applied %d tuning actions", applied_count)
-        
+
         return applied_count
-    
+
     def _should_tune(self, operation: str) -> bool:
         """Determine if tuning is needed for an operation."""
         if operation not in self.performance_history:
             return False
-        
+
         history = self.performance_history[operation]
-        
+
         # Need minimum samples
         if len(history) < 5:
             return False
-        
+
         # Check for performance degradation
         if operation in self.baselines:
             baseline = self.baselines[operation]
             recent_metrics = list(history)[-3:]  # Last 3 samples
-            
+
             degraded_count = sum(
-                1 for metrics in recent_metrics 
+                1 for metrics in recent_metrics
                 if baseline.is_degraded(metrics)
             )
-            
+
             return degraded_count >= 2  # 2 out of 3 recent samples degraded
-        
+
         # Check for performance variance (instability)
         durations = [m.duration for m in history]
         if len(durations) >= 5:
             cv = statistics.stdev(durations) / statistics.mean(durations)
             return cv > 0.3  # High coefficient of variation
-        
+
         return False
-    
+
     def _generate_tuning_actions(self, operation: str) -> List[TuningAction]:
         """Generate tuning actions based on performance analysis."""
         actions = []
-        
+
         if operation not in self.performance_history:
             return actions
-        
+
         history = list(self.performance_history[operation])
         recent_metrics = history[-3:]  # Last 3 samples
-        
+
         # Get current system resources
         system_resources = self.system_monitor.get_current_resources()
-        
+
         # Analyze performance patterns
         avg_duration = statistics.mean(m.duration for m in recent_metrics)
-        avg_throughput = statistics.mean(m.throughput_items_per_sec for m in recent_metrics)
+        avg_throughput = statistics.mean(
+            m.throughput_items_per_sec for m in recent_metrics)
         avg_cpu = statistics.mean(m.cpu_percent for m in recent_metrics)
         avg_memory = statistics.mean(m.memory_peak for m in recent_metrics)
-        
+
         # Generate actions based on analysis
-        actions.extend(self._analyze_concurrency_settings(operation, recent_metrics, system_resources))
-        actions.extend(self._analyze_timeout_settings(operation, recent_metrics))
-        actions.extend(self._analyze_memory_settings(operation, recent_metrics, system_resources))
-        actions.extend(self._analyze_caching_settings(operation, recent_metrics))
-        
+        actions.extend(self._analyze_concurrency_settings(
+            operation, recent_metrics, system_resources))
+        actions.extend(self._analyze_timeout_settings(
+            operation, recent_metrics))
+        actions.extend(self._analyze_memory_settings(
+            operation, recent_metrics, system_resources))
+        actions.extend(self._analyze_caching_settings(
+            operation, recent_metrics))
+
         return actions
-    
+
     def _analyze_concurrency_settings(
-        self, 
-        operation: str, 
-        metrics: List[PerformanceMetrics], 
+        self,
+        operation: str,
+        metrics: List[PerformanceMetrics],
         system_resources: SystemResources
     ) -> List[TuningAction]:
         """Analyze and tune concurrency settings."""
         actions = []
-        
+
         avg_cpu = statistics.mean(m.cpu_percent for m in metrics)
-        avg_throughput = statistics.mean(m.throughput_items_per_sec for m in metrics)
+        avg_throughput = statistics.mean(
+            m.throughput_items_per_sec for m in metrics)
         avg_workers = statistics.mean(m.worker_count for m in metrics)
-        
+
         # Concurrency tuning based on operation type
         if "download" in operation.lower():
             param = "concurrent_download_workers"
@@ -271,10 +286,10 @@
             param = "concurrent_file_workers"
         else:
             return actions
-        
+
         current_workers = int(avg_workers)
         param_config = self.tunable_parameters[param]
-        
+
         # CPU underutilization - increase workers
         if avg_cpu < 50 and not system_resources.is_under_pressure:
             new_workers = min(current_workers + 1, param_config["max"])
@@ -288,7 +303,7 @@
                     expected_impact="positive",
                     priority=2
                 ))
-        
+
         # CPU overutilization or system pressure - decrease workers
         elif avg_cpu > 85 or system_resources.is_under_pressure:
             new_workers = max(current_workers - 1, param_config["min"])
@@ -302,7 +317,7 @@
                     expected_impact="positive",
                     priority=3
                 ))
-        
+
         # Low throughput with moderate CPU - try increasing workers
         elif avg_throughput < 1.0 and avg_cpu < 70:
             new_workers = min(current_workers + 2, param_config["max"])
@@ -316,27 +331,28 @@
                     expected_impact="positive",
                     priority=2
                 ))
-        
+
         return actions
-    
+
     def _analyze_timeout_settings(
-        self, 
-        operation: str, 
+        self,
+        operation: str,
         metrics: List[PerformanceMetrics]
     ) -> List[TuningAction]:
         """Analyze and tune timeout settings."""
         actions = []
-        
+
         # Look for timeout-related patterns in duration
         durations = [m.duration for m in metrics]
         avg_duration = statistics.mean(durations)
         max_duration = max(durations)
-        
+
         # If operations are taking a long time, increase timeout
         if max_duration > 60:  # Operations taking more than 1 minute
             current_timeout = 30  # Default assumption
-            new_timeout = min(int(max_duration * 1.5), 300)  # 1.5x max duration, cap at 5 minutes
-            
+            # 1.5x max duration, cap at 5 minutes
+            new_timeout = min(int(max_duration * 1.5), 300)
+
             if new_timeout > current_timeout:
                 actions.append(TuningAction(
                     parameter="timeout",
@@ -347,12 +363,13 @@
                     expected_impact="positive",
                     priority=1
                 ))
-        
+
         # If operations are consistently fast, can reduce timeout
         elif avg_duration < 5 and max_duration < 10:
             current_timeout = 30
-            new_timeout = max(int(max_duration * 2), 10)  # 2x max duration, minimum 10s
-            
+            # 2x max duration, minimum 10s
+            new_timeout = max(int(max_duration * 2), 10)
+
             if new_timeout < current_timeout:
                 actions.append(TuningAction(
                     parameter="timeout",
@@ -363,21 +380,21 @@
                     expected_impact="neutral",
                     priority=0
                 ))
-        
+
         return actions
-    
+
     def _analyze_memory_settings(
-        self, 
-        operation: str, 
-        metrics: List[PerformanceMetrics], 
+        self,
+        operation: str,
+        metrics: List[PerformanceMetrics],
         system_resources: SystemResources
     ) -> List[TuningAction]:
         """Analyze and tune memory-related settings."""
         actions = []
-        
+
         avg_memory = statistics.mean(m.memory_peak for m in metrics)
         max_memory = max(m.memory_peak for m in metrics)
-        
+
         # High memory usage - reduce batch size or file size limits
         if avg_memory > 1024 or system_resources.memory_percent > 85:
             if "batch_size" in operation.lower():
@@ -390,7 +407,7 @@
                     expected_impact="positive",
                     priority=2
                 ))
-            
+
             actions.append(TuningAction(
                 parameter="max_file_size_mb",
                 current_value=100,  # Default assumption
@@ -400,7 +417,7 @@
                 expected_impact="positive",
                 priority=1
             ))
-        
+
         # Low memory usage - can increase batch size
         elif avg_memory < 256 and system_resources.memory_percent < 50:
             if "batch" in operation.lower():
@@ -413,29 +430,29 @@
                     expected_impact="positive",
                     priority=1
                 ))
-        
+
         return actions
-    
+
     def _analyze_caching_settings(
-        self, 
-        operation: str, 
+        self,
+        operation: str,
         metrics: List[PerformanceMetrics]
     ) -> List[TuningAction]:
         """Analyze and tune caching settings."""
         actions = []
-        
+
         # Get cache statistics
         cache = get_global_cache()
         cache_stats = cache.get_stats()
-        
+
         hit_rate = cache_stats["performance"]["hit_rate_percent"]
         memory_utilization = cache_stats["memory_cache"]["utilization_percent"]
-        
+
         # Low cache hit rate - increase cache size
         if hit_rate < 50 and memory_utilization > 80:
             current_cache_mb = cache_stats["memory_cache"]["size_mb"]
             new_cache_mb = min(current_cache_mb * 1.5, 1024)
-            
+
             actions.append(TuningAction(
                 parameter="cache_memory_mb",
                 current_value=current_cache_mb,
@@ -445,12 +462,12 @@
                 expected_impact="positive",
                 priority=1
             ))
-        
+
         # High cache hit rate but low utilization - can reduce cache size
         elif hit_rate > 90 and memory_utilization < 30:
             current_cache_mb = cache_stats["memory_cache"]["size_mb"]
             new_cache_mb = max(current_cache_mb * 0.8, 64)
-            
+
             actions.append(TuningAction(
                 parameter="cache_memory_mb",
                 current_value=current_cache_mb,
@@ -460,29 +477,29 @@
                 expected_impact="neutral",
                 priority=0
             ))
-        
+
         return actions
-    
+
     def _update_baseline(self, operation: str) -> None:
         """Update performance baseline for an operation."""
         if operation not in self.performance_history:
             return
-        
+
         history = self.performance_history[operation]
-        
+
         # Need minimum samples to establish baseline
         if len(history) < 10:
             return
-        
+
         # Calculate baseline metrics
         durations = [m.duration for m in history]
         throughputs = [m.throughput_items_per_sec for m in history]
         memory_usage = [m.memory_peak for m in history]
         cpu_usage = [m.cpu_percent for m in history]
-        
+
         # Calculate success rate (assuming all recorded metrics are from successful operations)
         success_rate = 1.0  # Could be enhanced with actual success tracking
-        
+
         baseline = PerformanceBaseline(
             operation_name=operation,
             avg_duration=statistics.mean(durations),
@@ -493,14 +510,14 @@
             sample_count=len(history),
             established_at=time.time()
         )
-        
+
         self.baselines[operation] = baseline
-        
+
         log.debug(
             "Updated baseline for %s: duration=%.2fs, throughput=%.2f/s, memory=%.1fMB",
             operation, baseline.avg_duration, baseline.avg_throughput, baseline.avg_memory_usage
         )
-    
+
     def get_tuning_summary(self) -> Dict[str, Any]:
         """Get summary of tuning activities and current state."""
         with self.lock:
@@ -508,7 +525,7 @@
                 action for action, timestamp in self.tuning_history
                 if time.time() - timestamp < 3600  # Last hour
             ]
-            
+
             return {
                 "strategy": self.strategy.value,
                 "operations_monitored": len(self.performance_history),
@@ -532,36 +549,37 @@
 
 class SystemMonitor:
     """System resource monitoring for performance tuning."""
-    
+
     def __init__(self):
         self.monitoring_interval = 5.0  # seconds
         self.history_size = 100
         self.resource_history: deque = deque(maxlen=self.history_size)
         self.monitoring_thread: Optional[threading.Thread] = None
         self.stop_monitoring = threading.Event()
-        
+
         # Platform-aware disk monitoring
         import os
         self.root_path = os.path.abspath(os.sep)
-        
+
     def start_monitoring(self) -> None:
         """Start continuous system monitoring."""
         if self.monitoring_thread and self.monitoring_thread.is_alive():
             return
-        
+
         self.stop_monitoring.clear()
-        self.monitoring_thread = threading.Thread(target=self._monitor_loop, daemon=True)
+        self.monitoring_thread = threading.Thread(
+            target=self._monitor_loop, daemon=True)
         self.monitoring_thread.start()
-        
+
         log.info("Started system monitoring")
-    
+
     def stop_monitoring_thread(self) -> None:
         """Stop system monitoring."""
         if self.monitoring_thread:
             self.stop_monitoring.set()
             self.monitoring_thread.join(timeout=1.0)
             log.info("Stopped system monitoring")
-    
+
     def _monitor_loop(self) -> None:
         """Main monitoring loop."""
         while not self.stop_monitoring.wait(self.monitoring_interval):
@@ -570,12 +588,12 @@
                 self.resource_history.append(resources)
             except Exception as e:
                 log.warning("Error in system monitoring: %s", e)
-    
+
     def get_current_resources(self) -> SystemResources:
         """Get current system resource usage."""
         memory = psutil.virtual_memory()
         disk = psutil.disk_usage(self.root_path)
-        
+
         return SystemResources(
             cpu_percent=psutil.cpu_percent(interval=0.1),
             memory_percent=memory.percent,
@@ -583,15 +601,15 @@
             disk_free_gb=disk.free / (1024**3),
             network_connections=len(psutil.net_connections())
         )
-    
+
     def get_resource_trends(self) -> Dict[str, Any]:
         """Get resource usage trends."""
         if len(self.resource_history) < 2:
             return {}
-        
+
         cpu_values = [r.cpu_percent for r in self.resource_history]
         memory_values = [r.memory_percent for r in self.resource_history]
-        
+
         return {
             "cpu_trend": "increasing" if cpu_values[-1] > cpu_values[-5] else "decreasing",
             "memory_trend": "increasing" if memory_values[-1] > memory_values[-5] else "decreasing",
@@ -630,18 +648,18 @@
     def decorator(func: Callable) -> Callable:
         def wrapper(*args, **kwargs):
             tuner = get_global_tuner()
-            
+
             # Record performance
             start_time = time.time()
             start_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-            
+
             try:
                 result = func(*args, **kwargs)
-                
+
                 # Create performance metrics
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=operation_name,
                     start_time=start_time,
@@ -654,15 +672,15 @@
                     worker_count=1,
                     items_processed=1
                 )
-                
+
                 tuner.record_performance(metrics)
                 return result
-                
+
             except Exception:
                 # Still record performance for failed operations
                 end_time = time.time()
                 end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
-                
+
                 metrics = PerformanceMetrics(
                     operation_name=f"{operation_name}_failed",
                     start_time=start_time,
@@ -675,9 +693,9 @@
                     worker_count=1,
                     items_processed=0
                 )
-                
+
                 tuner.record_performance(metrics)
                 raise
-        
+
         return wrapper
-    return decorator
\ No newline at end of file
+    return decorator
--- original/./etl/utils/rollback.py
+++ fixed/./etl/utils/rollback.py
@@ -45,7 +45,7 @@
     metadata: Dict[str, Any] = field(default_factory=dict)
     priority: int = 0  # Higher priority executed first
     created_at: float = field(default_factory=time.time)
-    
+
     def execute(self) -> bool:
         """Execute the rollback action."""
         try:
@@ -60,14 +60,14 @@
 
 class RollbackManager:
     """Manages rollback actions for ETL operations."""
-    
+
     def __init__(self, operation_name: str = "unknown"):
         self.operation_name = operation_name
         self.actions: List[RollbackAction] = []
         self.executed_actions: List[RollbackAction] = []
         self.rollback_enabled = True
         self._lock = threading.Lock()
-    
+
     def add_action(
         self,
         action_type: RollbackType,
@@ -87,7 +87,7 @@
             )
             self.actions.append(action)
             log.debug("üìù Added rollback action: %s", description)
-    
+
     def add_file_deletion(self, file_path: Path, description: Optional[str] = None) -> None:
         """Add file deletion rollback action."""
         desc = description or f"Delete file: {file_path}"
@@ -98,7 +98,7 @@
             {"file_path": str(file_path)},
             priority=1
         )
-    
+
     def add_directory_cleanup(self, dir_path: Path, description: Optional[str] = None) -> None:
         """Add directory cleanup rollback action."""
         desc = description or f"Clean directory: {dir_path}"
@@ -109,7 +109,7 @@
             {"dir_path": str(dir_path)},
             priority=2
         )
-    
+
     def add_feature_class_deletion(self, fc_path: str, description: Optional[str] = None) -> None:
         """Add feature class deletion rollback action."""
         desc = description or f"Delete feature class: {fc_path}"
@@ -120,7 +120,7 @@
             {"fc_path": fc_path},
             priority=3
         )
-    
+
     def add_temp_workspace_cleanup(self, workspace_path: str, description: Optional[str] = None) -> None:
         """Add temporary workspace cleanup rollback action."""
         desc = description or f"Clean temp workspace: {workspace_path}"
@@ -131,7 +131,7 @@
             {"workspace_path": workspace_path},
             priority=4
         )
-    
+
     def add_custom_action(
         self,
         action_func: Callable[[], None],
@@ -147,92 +147,96 @@
             metadata,
             priority
         )
-    
+
     def execute_rollback(self, reason: str = "Operation failed") -> bool:
         """Execute all rollback actions in reverse order (LIFO)."""
         if not self.rollback_enabled:
-            log.info("üö´ Rollback disabled for operation: %s", self.operation_name)
+            log.info("üö´ Rollback disabled for operation: %s",
+                     self.operation_name)
             return True
-        
+
         if not self.actions:
-            log.debug("‚ÑπÔ∏è No rollback actions to execute for: %s", self.operation_name)
+            log.debug("‚ÑπÔ∏è No rollback actions to execute for: %s",
+                      self.operation_name)
             return True
-        
-        log.info("üîÑ Starting rollback for '%s': %s", self.operation_name, reason)
-        
+
+        log.info("üîÑ Starting rollback for '%s': %s",
+                 self.operation_name, reason)
+
         # Sort by priority (higher first), then reverse chronological order
         sorted_actions = sorted(
             self.actions,
             key=lambda a: (a.priority, -a.created_at),
             reverse=True
         )
-        
+
         success_count = 0
         total_actions = len(sorted_actions)
-        
+
         for action in sorted_actions:
             if action.execute():
                 success_count += 1
                 self.executed_actions.append(action)
             # Continue even if individual actions fail
-        
+
         log.info(
             "üèÅ Rollback completed for '%s': %d/%d actions successful",
             self.operation_name,
             success_count,
             total_actions
         )
-        
+
         # Clear actions after rollback
         self.actions.clear()
-        
+
         return success_count == total_actions
-    
+
     def clear_actions(self) -> None:
         """Clear all pending rollback actions without executing them."""
         with self._lock:
-            log.debug("üóëÔ∏è Cleared %d rollback actions for: %s", len(self.actions), self.operation_name)
+            log.debug("üóëÔ∏è Cleared %d rollback actions for: %s",
+                      len(self.actions), self.operation_name)
             self.actions.clear()
-    
+
     def disable_rollback(self) -> None:
         """Disable rollback for this manager."""
         self.rollback_enabled = False
         log.debug("üö´ Rollback disabled for: %s", self.operation_name)
-    
+
     def enable_rollback(self) -> None:
         """Enable rollback for this manager."""
         self.rollback_enabled = True
         log.debug("‚úÖ Rollback enabled for: %s", self.operation_name)
-    
+
     def get_pending_actions(self) -> List[RollbackAction]:
         """Get list of pending rollback actions."""
         return self.actions.copy()
-    
+
     def get_executed_actions(self) -> List[RollbackAction]:
         """Get list of executed rollback actions."""
         return self.executed_actions.copy()
-    
+
     @staticmethod
     def _safe_file_delete(file_path: Path) -> None:
         """Safely delete a file."""
         if file_path.exists() and file_path.is_file():
             file_path.unlink()
             log.debug("üóëÔ∏è Deleted file: %s", file_path)
-    
+
     @staticmethod
     def _safe_directory_cleanup(dir_path: Path) -> None:
         """Safely clean a directory."""
         if dir_path.exists() and dir_path.is_dir():
             shutil.rmtree(dir_path)
             log.debug("üóëÔ∏è Cleaned directory: %s", dir_path)
-    
+
     @staticmethod
     def _safe_fc_delete(fc_path: str) -> None:
         """Safely delete a feature class."""
         if arcpy.Exists(fc_path):
             arcpy.management.Delete(fc_path)
             log.debug("üóëÔ∏è Deleted feature class: %s", fc_path)
-    
+
     @staticmethod
     def _safe_workspace_cleanup(workspace_path: str) -> None:
         """Safely cleanup temporary workspace."""
@@ -246,7 +250,8 @@
                     arcpy.management.Delete(table)
                 log.debug("üóëÔ∏è Cleaned workspace: %s", workspace_path)
             except Exception as e:
-                log.warning("‚ö†Ô∏è Failed to clean workspace %s: %s", workspace_path, e)
+                log.warning("‚ö†Ô∏è Failed to clean workspace %s: %s",
+                            workspace_path, e)
 
 
 @contextmanager
@@ -267,13 +272,13 @@
 
 class TransactionalOperation:
     """Base class for transactional operations with rollback support."""
-    
+
     def __init__(self, operation_name: str):
         self.operation_name = operation_name
         self.rollback_manager = RollbackManager(operation_name)
         self.started = False
         self.completed = False
-    
+
     def start(self) -> None:
         """Start the transactional operation."""
         if self.started:
@@ -281,10 +286,10 @@
                 f"Operation '{self.operation_name}' already started",
                 context=ErrorContext(operation="transactional_operation")
             )
-        
+
         self.started = True
         log.debug("üöÄ Started transactional operation: %s", self.operation_name)
-    
+
     def commit(self) -> None:
         """Commit the operation (clear rollback actions)."""
         if not self.started:
@@ -292,25 +297,28 @@
                 f"Operation '{self.operation_name}' not started",
                 context=ErrorContext(operation="transactional_operation")
             )
-        
+
         self.rollback_manager.clear_actions()
         self.completed = True
-        log.debug("‚úÖ Committed transactional operation: %s", self.operation_name)
-    
+        log.debug("‚úÖ Committed transactional operation: %s",
+                  self.operation_name)
+
     def rollback(self, reason: str = "Manual rollback") -> None:
         """Rollback the operation."""
         if not self.started:
-            log.warning("‚ö†Ô∏è Attempting to rollback operation that was not started: %s", self.operation_name)
+            log.warning(
+                "‚ö†Ô∏è Attempting to rollback operation that was not started: %s", self.operation_name)
             return
-        
+
         self.rollback_manager.execute_rollback(reason)
         self.completed = True
-        log.debug("üîÑ Rolled back transactional operation: %s", self.operation_name)
-    
+        log.debug("üîÑ Rolled back transactional operation: %s",
+                  self.operation_name)
+
     def __enter__(self):
         self.start()
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         if exc_type is not None and not self.completed:
             # Exception occurred, rollback
@@ -322,71 +330,71 @@
 
 class FileOperationTransaction(TransactionalOperation):
     """Transactional file operations with automatic rollback."""
-    
+
     def __init__(self, operation_name: str = "file_operation"):
         super().__init__(operation_name)
         self.created_files: List[Path] = []
         self.created_directories: List[Path] = []
-    
+
     def create_file(self, file_path: Path, content: str = "") -> Path:
         """Create a file with rollback support."""
         if file_path.exists():
             log.warning("‚ö†Ô∏è File already exists: %s", file_path)
             return file_path
-        
+
         # Create parent directories if needed
         file_path.parent.mkdir(parents=True, exist_ok=True)
-        
+
         # Create file
         file_path.write_text(content)
         self.created_files.append(file_path)
-        
+
         # Add rollback action
         self.rollback_manager.add_file_deletion(file_path)
-        
+
         log.debug("üìÑ Created file: %s", file_path)
         return file_path
-    
+
     def create_directory(self, dir_path: Path) -> Path:
         """Create a directory with rollback support."""
         if dir_path.exists():
             log.warning("‚ö†Ô∏è Directory already exists: %s", dir_path)
             return dir_path
-        
+
         dir_path.mkdir(parents=True, exist_ok=True)
         self.created_directories.append(dir_path)
-        
+
         # Add rollback action
         self.rollback_manager.add_directory_cleanup(dir_path)
-        
+
         log.debug("üìÅ Created directory: %s", dir_path)
         return dir_path
-    
+
     def copy_file(self, src: Path, dst: Path) -> Path:
         """Copy file with rollback support."""
         if dst.exists():
             log.warning("‚ö†Ô∏è Destination file already exists: %s", dst)
             return dst
-        
+
         dst.parent.mkdir(parents=True, exist_ok=True)
         shutil.copy2(src, dst)
         self.created_files.append(dst)
-        
+
         # Add rollback action
         self.rollback_manager.add_file_deletion(dst)
-        
+
         log.debug("üìã Copied file: %s -> %s", src, dst)
         return dst
 
 
 class ArcPyTransaction(TransactionalOperation):
     """Transactional ArcPy operations with automatic rollback."""
-    
+
     def __init__(self, operation_name: str = "arcpy_operation"):
         super().__init__(operation_name)
         self.created_feature_classes: List[str] = []
         self.temp_workspaces: List[str] = []
-    
+
     def create_feature_class(
         self,
         out_path: str,
@@ -396,44 +404,45 @@
     ) -> str:
         """Create feature class with rollback support."""
         fc_path = f"{out_path}\\{out_name}"
-        
+
         if arcpy.Exists(fc_path):
             log.warning("‚ö†Ô∏è Feature class already exists: %s", fc_path)
             return fc_path
-        
+
         arcpy.management.CreateFeatureclass(
             out_path=out_path,
             out_name=out_name,
             geometry_type=geometry_type,
             spatial_reference=spatial_reference
         )
-        
+
         self.created_feature_classes.append(fc_path)
-        
+
         # Add rollback action
         self.rollback_manager.add_feature_class_deletion(fc_path)
-        
+
         log.debug("üó∫Ô∏è Created feature class: %s", fc_path)
         return fc_path
-    
+
     def create_temp_workspace(self, workspace_type: str = "FILEGDB") -> str:
         """Create temporary workspace with rollback support."""
         import tempfile
-        
+
         if workspace_type.upper() == "FILEGDB":
             temp_dir = tempfile.mkdtemp(prefix="etl_temp_")
             workspace_path = str(Path(temp_dir) / "temp.gdb")
             arcpy.management.CreateFileGDB(temp_dir, "temp.gdb")
         else:
             workspace_path = tempfile.mkdtemp(prefix="etl_workspace_")
-        
+
         self.temp_workspaces.append(workspace_path)
-        
+
         # Add rollback action
         self.rollback_manager.add_temp_workspace_cleanup(workspace_path)
-        
+
         log.debug("üèóÔ∏è Created temp workspace: %s", workspace_path)
         return workspace_path
+
 
 # Global rollback manager for tracking pipeline-level rollbacks
 _global_rollback_manager = RollbackManager("global_pipeline")
@@ -442,6 +451,7 @@
 def get_global_rollback_manager() -> RollbackManager:
     """Get the global rollback manager for pipeline-level operations."""
     return _global_rollback_manager
+
 
 def add_pipeline_rollback_action(
     action_type: RollbackType,
@@ -459,6 +469,7 @@
         priority=priority
     )
 
+
 def execute_pipeline_rollback(reason: str = "Pipeline failed") -> bool:
     """Execute global pipeline rollback."""
     return _global_rollback_manager.execute_rollback(reason)
--- original/./etl/handlers/__init__.py
+++ fixed/./etl/handlers/__init__.py
@@ -1,22 +1,22 @@
 # handlers/__init__.py
 from __future__ import annotations
-from typing import Dict, Type, TypeAlias, Any # Added TypeAlias, Any
+from typing import Dict, Type, TypeAlias, Any  # Added TypeAlias, Any
 
 from .ogc_api import OgcApiDownloadHandler
 from .file import FileDownloadHandler
 from .atom_feed import AtomFeedDownloadHandler
-from .rest_api import RestApiDownloadHandler # <-- Add this
+from .rest_api import RestApiDownloadHandler  # <-- Add this
 
 # Define a more specific type for the handler classes if possible
 # from .base_handler import BaseDownloadHandler # Assuming you might create a base class
 # DownloadHandlerType: TypeAlias = Type[BaseDownloadHandler]
-DownloadHandlerType: TypeAlias = Type[Any] # Using Any for now for simplicity
+DownloadHandlerType: TypeAlias = Type[Any]  # Using Any for now for simplicity
 
 HANDLER_MAP: Dict[str, DownloadHandlerType] = {
     "file": FileDownloadHandler,
     "atom_feed": AtomFeedDownloadHandler,
     "rest_api": RestApiDownloadHandler,
-    "ogc_api": OgcApiDownloadHandler,  
+    "ogc_api": OgcApiDownloadHandler,
 }
 
-__all__ = ["HANDLER_MAP"]
\ No newline at end of file
+__all__ = ["HANDLER_MAP"]
--- original/./etl/handlers/file.py
+++ fixed/./etl/handlers/file.py
@@ -32,11 +32,11 @@
     Determines 'true' filenames by checking Content-Disposition or unquoting URL parts.
     For ZIPs, extracts contents. For direct GPKGs, copies them to the staging location.
     """
-    
+
     def __init__(self, src: Source, global_config: Optional[Dict[str, Any]] = None):
         self.src = src
         self.global_config = global_config or {}
-        
+
         # Initialize retry configuration
         retry_config = self.global_config.get("retry", {})
         self.retry_config = RetryConfig(
@@ -45,7 +45,7 @@
             backoff_factor=retry_config.get("backoff_factor", 2.0),
             max_delay=retry_config.get("max_delay", 300.0)
         )
-        
+
         ensure_dirs()
 
     def fetch(self) -> None:
@@ -75,7 +75,8 @@
             else:
                 self._download_single_resource()
         except Exception as e:
-            log.error("‚ùå Failed to fetch source '%s': %s", self.src.name, format_error_context(e) if hasattr(e, 'context') else str(e))
+            log.error("‚ùå Failed to fetch source '%s': %s", self.src.name,
+                      format_error_context(e) if hasattr(e, 'context') else str(e))
             raise
 
     def _download_multiple_files(self) -> None:
@@ -88,7 +89,7 @@
         )
 
         file_stems = list(self._iter_included_file_stems())
-        
+
         # Use concurrent downloads for multiple files if enabled
         if len(file_stems) > 1 and self.global_config.get("enable_concurrent_downloads", True):
             self._download_files_concurrent(file_stems)
@@ -100,14 +101,14 @@
     def _download_files_concurrent(self, file_stems: List[str]) -> None:
         """Download multiple files concurrently for improved performance."""
         log.info("üöÄ Starting concurrent download of %d files", len(file_stems))
-        
+
         # Get concurrent downloader
         downloader = get_file_downloader()
-        
+
         # Get configuration - use max_workers parameter instead of mutating singleton
         max_workers = self.global_config.get("concurrent_file_workers", 4)
         fail_fast = self.global_config.get("fail_fast_downloads", False)
-        
+
         # Execute concurrent downloads with max_workers parameter
         results = downloader.download_files_concurrent(
             handler=self,
@@ -115,19 +116,20 @@
             fail_fast=fail_fast,
             max_workers=max_workers  # Pass as parameter instead of mutating singleton
         )
-        
+
         # Process results and log statistics
         successful_downloads = sum(1 for r in results if r.success)
         failed_downloads = len(results) - successful_downloads
-        
-        log.info("üèÅ Concurrent file downloads completed: %d successful, %d failed", 
-                successful_downloads, failed_downloads)
-        
+
+        log.info("üèÅ Concurrent file downloads completed: %d successful, %d failed",
+                 successful_downloads, failed_downloads)
+
         # Log any failures
         for result in results:
             if not result.success:
                 file_name = result.metadata.get("task_name", "unknown")
-                log.error("‚ùå File download failed: %s - %s", file_name, result.error)
+                log.error("‚ùå File download failed: %s - %s",
+                          file_name, result.error)
 
     def _download_single_file_stem(self, included_filename_stem: str) -> None:
         """Download a single file stem (extracted from original loop)."""
@@ -157,7 +159,8 @@
             self.src.url,
         )
 
-        true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(self.src.url)
+        true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(
+            self.src.url)
         consistent_local_stem = sanitize_for_filename(self.src.name)
         final_extension = true_ext_from_web
 
@@ -226,17 +229,20 @@
         local_download_filename = explicit_local_filename_stem + explicit_local_file_ext
         download_target_path = paths.DOWNLOADS / local_download_filename
 
-        sanitized_staging_subdir_name = sanitize_for_filename(staging_subdir_name_override)
-        final_staging_destination_dir = paths.STAGING / self.src.authority / sanitized_staging_subdir_name
+        sanitized_staging_subdir_name = sanitize_for_filename(
+            staging_subdir_name_override)
+        final_staging_destination_dir = paths.STAGING / \
+            self.src.authority / sanitized_staging_subdir_name
         final_staging_destination_dir.mkdir(parents=True, exist_ok=True)
 
         log.debug("Attempting to download: %s \n    -> to local file: %s \n    -> staging dir: %s",
-                 download_url, download_target_path.name, final_staging_destination_dir.relative_to(paths.ROOT))
+                  download_url, download_target_path.name, final_staging_destination_dir.relative_to(paths.ROOT))
 
         try:
             downloaded_file_path = download(download_url, download_target_path)
         except Exception as e:
-            log.error("‚ùå Download failed for %s (Source: %s): %s", download_url, self.src.name, e, exc_info=True)
+            log.error("‚ùå Download failed for %s (Source: %s): %s",
+                      download_url, self.src.name, e, exc_info=True)
             return
 
         effective_staged_data_type = self.src.staged_data_type or ""
@@ -264,12 +270,12 @@
                              downloaded_file_path.name,
                              staged_gpkg_path.relative_to(paths.ROOT))
                 else:
-                    log.info("‚ÑπÔ∏è Downloaded GPKG '%s' is already in the target staging location.", 
-                            downloaded_file_path.name)
+                    log.info("‚ÑπÔ∏è Downloaded GPKG '%s' is already in the target staging location.",
+                             downloaded_file_path.name)
             except Exception as e:
-                log.error("‚ùå Failed to copy downloaded GPKG %s to staging location %s: %s", 
-                         downloaded_file_path.name, staged_gpkg_path, e, exc_info=True)
-        
+                log.error("‚ùå Failed to copy downloaded GPKG %s to staging location %s: %s",
+                          downloaded_file_path.name, staged_gpkg_path, e, exc_info=True)
+
         elif effective_staged_data_type == "geojson":
             staged_json_filename = sanitized_staging_subdir_name + explicit_local_file_ext
             staged_json_path = final_staging_destination_dir / staged_json_filename
@@ -280,11 +286,11 @@
                              downloaded_file_path.name,
                              staged_json_path.relative_to(paths.ROOT))
                 else:
-                    log.info("‚ÑπÔ∏è Downloaded GeoJSON/JSON '%s' is already in the target staging location.", 
-                            downloaded_file_path.name)
+                    log.info("‚ÑπÔ∏è Downloaded GeoJSON/JSON '%s' is already in the target staging location.",
+                             downloaded_file_path.name)
             except Exception as e:
-                log.error("‚ùå Failed to copy downloaded GeoJSON/JSON %s to staging location %s: %s", 
-                         downloaded_file_path.name, staged_json_path, e, exc_info=True)
+                log.error("‚ùå Failed to copy downloaded GeoJSON/JSON %s to staging location %s: %s",
+                          downloaded_file_path.name, staged_json_path, e, exc_info=True)
 
         elif effective_staged_data_type == "shapefile_collection":
             if explicit_local_file_ext.lower() != ".zip":
@@ -293,19 +299,20 @@
                     "but actual extension is '%s'. Attempting extraction anyway for '%s'.",
                     self.src.name, explicit_local_file_ext, downloaded_file_path.name
                 )
-            
+
             if explicit_local_file_ext.lower() == ".zip":
                 try:
-                    extract_zip(downloaded_file_path, final_staging_destination_dir)
+                    extract_zip(downloaded_file_path,
+                                final_staging_destination_dir)
                     log.info("‚ûï Extracted and staged archive %s to %s",
                              downloaded_file_path.name,
                              final_staging_destination_dir.relative_to(paths.ROOT))
                 except zipfile.BadZipFile:
-                    log.error("‚ùå File '%s' is not a valid ZIP file. Cannot extract for shapefile_collection.", 
-                             downloaded_file_path.name)
+                    log.error("‚ùå File '%s' is not a valid ZIP file. Cannot extract for shapefile_collection.",
+                              downloaded_file_path.name)
                 except Exception as e:
-                    log.error("‚ùå Failed to extract archive %s to %s: %s", 
-                             downloaded_file_path.name, final_staging_destination_dir, e, exc_info=True)
+                    log.error("‚ùå Failed to extract archive %s to %s: %s",
+                              downloaded_file_path.name, final_staging_destination_dir, e, exc_info=True)
 
         else:
             log.warning(
@@ -320,4 +327,4 @@
 
     def __exit__(self, exc_type, exc_val, exc_tb) -> None:
         """Exit the context manager. No cleanup needed for file downloads."""
-        pass
\ No newline at end of file
+        pass
--- original/./etl/handlers/geoprocess.py
+++ fixed/./etl/handlers/geoprocess.py
@@ -29,14 +29,14 @@
     # Validate inputs
     staging_gdb_path = Path(staging_gdb)
     aoi_fc_path = Path(aoi_fc)
-    
+
     if not staging_gdb_path.exists():
         raise FileNotFoundError(f"Staging GDB not found: {staging_gdb_path}")
     if not aoi_fc_path.exists():
         raise FileNotFoundError(f"AOI feature class not found: {aoi_fc_path}")
-    
+
     log.info("üîÑ Starting in-place geoprocessing of %s", staging_gdb_path.name)
-    
+
     # Configure environment using EnvManager
     with arcpy.EnvManager(
         workspace=str(staging_gdb_path),
@@ -49,30 +49,31 @@
         if not original_fcs:
             log.warning("‚ö†Ô∏è No feature classes found in %s", staging_gdb_path)
             return
-            
-        log.info("üîÑ Processing %d feature classes: clip + project only", len(original_fcs))
-        
+
+        log.info("üîÑ Processing %d feature classes: clip + project only",
+                 len(original_fcs))
+
         # Clip and project all FCs
         clip_and_project_fcs(original_fcs, aoi_fc_path)
-            
+
         log.info("‚úÖ Geoprocessing complete for %s", staging_gdb_path.name)
 
 
 def clip_and_project_fcs(feature_classes: List[str], aoi_fc: Path) -> None:
     """üîÑ Clip and project all feature classes in-place."""
     log.info("‚úÇÔ∏è Clipping and projecting feature classes")
-    
+
     processed_count = 0
     error_count = 0
-    
+
     for fc_name in feature_classes:
         try:
             # Create temporary clipped version
             temp_clipped = f"in_memory\\{fc_name}_temp"
-            
+
             # Clip (projection handled by environment)
             arcpy.analysis.PairwiseClip(fc_name, str(aoi_fc), temp_clipped)
-            
+
             # Replace original with clipped version
             arcpy.management.Delete(fc_name)
             arcpy.management.CopyFeatures(temp_clipped, fc_name)
@@ -82,15 +83,17 @@
 
             log.info("   ‚úÇÔ∏è clipped & projected ‚ûú %s", fc_name)
             processed_count += 1
-            
+
         except arcpy.ExecuteError:
-            log.error("   ‚ùå failed to process %s: %s", fc_name, arcpy.GetMessages(2))
+            log.error("   ‚ùå failed to process %s: %s",
+                      fc_name, arcpy.GetMessages(2))
             error_count += 1
-            
-    log.info("üìä Clip/project complete: %d processed, %d errors", processed_count, error_count)
+
+    log.info("üìä Clip/project complete: %d processed, %d errors",
+             processed_count, error_count)
 
 
 def create_naming_rules_from_config(config: Dict) -> Dict[str, Dict[str, str]]:
     """üîß Placeholder function for future naming rules (currently unused)."""
     # Simplified - no longer used
-    return {}
\ No newline at end of file
+    return {}
--- original/./etl/handlers/ogc_api.py
+++ fixed/./etl/handlers/ogc_api.py
@@ -41,7 +41,7 @@
         self.src = src
         self.global_config = global_config or {}
         self.bbox_params: Dict[str, str] = {}
-        
+
         # Initialize HTTP session with connection pooling
         super().__init__(
             base_url=src.url,
@@ -50,13 +50,13 @@
             max_retries=2,
             timeout=DEFAULT_TIMEOUT
         )
-        
+
         # Update session headers
         self.session.headers.update({
             "User-Agent": "ETL-Pipeline/1.0",
             "Accept": "application/geo+json, application/json;q=0.9",
         })
-        
+
         self._setup_bbox_params()
 
     def _setup_bbox_params(self) -> None:
@@ -67,7 +67,8 @@
 
         bbox_coords_str = self.src.raw.get(
             "ogc_bbox",
-            self.global_config.get("global_ogc_bbox_coords", DEFAULT_OGC_BBOX_COORDS),
+            self.global_config.get(
+                "global_ogc_bbox_coords", DEFAULT_OGC_BBOX_COORDS),
         )
 
         bbox_crs_input = str(
@@ -145,12 +146,14 @@
                 "f": "json",
             }
 
-            response = self.session.get(test_url, params=test_params, timeout=10)
+            response = self.session.get(
+                test_url, params=test_params, timeout=10)
 
             # If we get 500 or 400 with bbox-crs, try without it
             if response.status_code in [400, 500]:
                 test_params.pop("bbox-crs")
-                response2 = self.session.get(test_url, params=test_params, timeout=10)
+                response2 = self.session.get(
+                    test_url, params=test_params, timeout=10)
 
                 # If it works without bbox-crs, the service doesn't support it
                 if response2.status_code == 200:
@@ -187,7 +190,8 @@
 
             # Use concurrent downloads for multiple collections
             if len(collections) > 1:
-                processed_at_least_one_collection_successfully = self._fetch_collections_concurrent(collections)
+                processed_at_least_one_collection_successfully = self._fetch_collections_concurrent(
+                    collections)
             else:
                 # Single collection - use original sequential approach
                 processed_at_least_one_collection_successfully = False
@@ -235,15 +239,18 @@
 
     def _fetch_collections_concurrent(self, collections: List[Dict[str, Any]]) -> bool:
         """Fetch multiple collections concurrently for improved performance."""
-        log.info("üöÄ Starting concurrent download of %d collections", len(collections))
-        
+        log.info("üöÄ Starting concurrent download of %d collections",
+                 len(collections))
+
         # Get concurrent downloader
         downloader = get_collection_downloader()
-        
+
         # Enable parallel processing based on configuration
-        use_concurrent = self.global_config.get("enable_concurrent_downloads", True)
-        max_workers = self.global_config.get("concurrent_collection_workers", 3)
-        
+        use_concurrent = self.global_config.get(
+            "enable_concurrent_downloads", True)
+        max_workers = self.global_config.get(
+            "concurrent_collection_workers", 3)
+
         if not use_concurrent:
             log.info("‚ö†Ô∏è Concurrent downloads disabled, falling back to sequential")
             processed_successfully = False
@@ -251,31 +258,32 @@
                 if self._fetch_collection(collection_data):
                     processed_successfully = True
             return processed_successfully
-        
+
         # Update worker count if specified
         if max_workers != downloader.manager.max_workers:
             downloader.manager.max_workers = max_workers
-        
+
         # Execute concurrent downloads
         results = downloader.download_collections_concurrent(
             handler=self,
             collections=collections,
             fail_fast=self.global_config.get("fail_fast_downloads", False)
         )
-        
+
         # Process results and log statistics
         successful_downloads = sum(1 for r in results if r.success)
         failed_downloads = len(results) - successful_downloads
-        
-        log.info("üèÅ Concurrent collection downloads completed: %d successful, %d failed", 
-                successful_downloads, failed_downloads)
-        
+
+        log.info("üèÅ Concurrent collection downloads completed: %d successful, %d failed",
+                 successful_downloads, failed_downloads)
+
         # Log any failures
         for result in results:
             if not result.success:
                 collection_name = result.metadata.get("task_name", "unknown")
-                log.error("‚ùå Collection download failed: %s - %s", collection_name, result.error)
-        
+                log.error("‚ùå Collection download failed: %s - %s",
+                          collection_name, result.error)
+
         return successful_downloads > 0
 
     def _get_collections(self) -> List[Dict[str, Any]]:
@@ -309,7 +317,8 @@
                 if str(col.get("id")) in configured_collection_ids_str
             ]
             if len(selected_collections) != len(configured_collection_ids_str):
-                found_ids = {str(col.get("id")) for col in selected_collections}
+                found_ids = {str(col.get("id"))
+                             for col in selected_collections}
                 missing_ids = configured_collection_ids_str - found_ids
                 if missing_ids:
                     log.warning(
@@ -332,7 +341,8 @@
             collections_url = self.src.url.rstrip("/")
             log.info("üîÑ Discovering collections from: %s", collections_url)
 
-            response = self.session.get(collections_url, timeout=DEFAULT_TIMEOUT)
+            response = self.session.get(
+                collections_url, timeout=DEFAULT_TIMEOUT)
             response.raise_for_status()
             data = response.json()
             discovered = data.get("collections", [])
@@ -403,7 +413,8 @@
         collection_id = collection_data.get("id", "unknown_collection")
         collection_title = collection_data.get("title", collection_id)
 
-        log.info("    üì¶ Fetching collection: %s (%s)", collection_id, collection_title)
+        log.info("    üì¶ Fetching collection: %s (%s)",
+                 collection_id, collection_title)
 
         items_link = self._find_items_link(collection_data)
         if not items_link:
@@ -509,7 +520,8 @@
                 )
                 return False
         else:
-            log.info("    ‚ÑπÔ∏è No features retrieved for collection '%s'.", collection_id)
+            log.info(
+                "    ‚ÑπÔ∏è No features retrieved for collection '%s'.", collection_id)
             return True
 
     def _determine_output_crs(
@@ -692,7 +704,8 @@
                     return href
 
         log.error(
-            "    ‚ùå No 'items' link found in collection: %s", collection_data.get("id")
+            "    ‚ùå No 'items' link found in collection: %s", collection_data.get(
+                "id")
         )
         return None
 
@@ -728,7 +741,8 @@
             if next_page_url and not next_page_url.startswith(("http://", "https://")):
                 base_url_for_next_link = response.url
                 next_page_url = urljoin(base_url_for_next_link, next_page_url)
-                log.debug("        Resolved relative next link to: %s", next_page_url)
+                log.debug(
+                    "        Resolved relative next link to: %s", next_page_url)
 
             return features_on_page, next_page_url
 
@@ -781,4 +795,3 @@
             if link_info.get("rel") == "next" and link_info.get("href"):
                 return link_info["href"]
         return None
-
--- original/./etl/handlers/rest_api.py
+++ fixed/./etl/handlers/rest_api.py
@@ -36,6 +36,7 @@
 GEOJSON_FORMAT = "geojson"
 SWEREF99_TM_WKID = 3006
 
+
 class RestApiDownloadHandler(HTTPSessionHandler):
     """Handles downloading data from ESRI REST API MapServer and FeatureServer Query endpoints."""
 
@@ -43,7 +44,7 @@
         self.src = src
         self.global_config = global_config or {}
         ensure_dirs()
-        
+
         # Initialize HTTP session with connection pooling
         timeout = self.global_config.get("timeout", 30)
         super().__init__(
@@ -53,7 +54,7 @@
             max_retries=3,
             timeout=timeout
         )
-        
+
         # Initialize retry configuration
         retry_config = self.global_config.get("retry", {})
         self.retry_config = RetryConfig(
@@ -62,34 +63,35 @@
             backoff_factor=retry_config.get("backoff_factor", 2.0),
             max_delay=retry_config.get("max_delay", 300.0)
         )
-        
+
         # Initialize circuit breaker for this service
         self.circuit_breaker = CircuitBreaker(
             failure_threshold=retry_config.get("circuit_breaker_threshold", 5),
             recovery_timeout=retry_config.get("circuit_breaker_timeout", 60.0),
             expected_exception=Exception
         )
-        
-        log.info("üöÄ Initializing RestApiDownloadHandler for source: %s", self.src.name)
+
+        log.info(
+            "üöÄ Initializing RestApiDownloadHandler for source: %s", self.src.name)
 
     @retry_with_backoff()
     def _get_service_metadata(self, service_url: str) -> Optional[Dict[str, Any]]:
         """Fetches base metadata for the service (MapServer/FeatureServer) with retries."""
         return self._fetch_service_metadata_impl(service_url)
-    
+
     @smart_retry("fetch_service_metadata")
     @http_circuit_breaker("rest_api_metadata", failure_threshold=3)
     def _fetch_service_metadata_impl(self, service_url: str) -> Dict[str, Any]:
         """Implementation of service metadata fetching with circuit breaker."""
         params = {"f": "json"}
-        
+
         try:
             response = self.session.get(
-                service_url, 
-                params=params, 
+                service_url,
+                params=params,
                 timeout=self.session.timeout
             )
-            
+
             # Handle different HTTP status codes appropriately
             if response.status_code == 429:
                 retry_after = response.headers.get('Retry-After')
@@ -125,9 +127,9 @@
                         operation="fetch_metadata"
                     )
                 )
-            
+
             response.raise_for_status()
-            
+
             try:
                 return response.json()
             except json.JSONDecodeError as e:
@@ -140,7 +142,7 @@
                         operation="parse_json"
                     )
                 ) from e
-                
+
         except requests.exceptions.Timeout as e:
             raise NetworkError(
                 f"Timeout fetching metadata from {service_url}",
@@ -179,11 +181,13 @@
         """Fetches metadata for a specific layer."""
         try:
             params = {"f": "json"}
-            response = self.session.get(layer_url, params=params, timeout=self.session.timeout)
+            response = self.session.get(
+                layer_url, params=params, timeout=self.session.timeout)
             response.raise_for_status()
             return response.json()
         except requests.exceptions.RequestException as e:
-            log.error("‚ùå Failed to fetch layer metadata from %s: %s", layer_url, e)
+            log.error("‚ùå Failed to fetch layer metadata from %s: %s",
+                      layer_url, e)
             return None
 
     def _prepare_query_params(self) -> Dict[str, Any]:
@@ -218,7 +222,8 @@
     ) -> Optional[Dict[str, Any]]:
         """Execute a paginated request and return the JSON payload."""
         try:
-            response_obj = self.session.get(query_url, params=params, timeout=120)
+            response_obj = self.session.get(
+                query_url, params=params, timeout=120)
             response_obj.raise_for_status()
             return response_obj.json()
         except requests.exceptions.RequestException as e:
@@ -304,7 +309,8 @@
         try:
             with open(output_path, "w", encoding="utf-8") as f:
                 json.dump(final_output_data, f, ensure_ascii=False, indent=2)
-            log.info("‚úÖ %s: %d features", layer_name_sanitized, features_written_total)
+            log.info("‚úÖ %s: %d features", layer_name_sanitized,
+                     features_written_total)
             log.debug(
                 "üíæ Successfully saved %d features for layer %s to %s",
                 features_written_total,
@@ -362,16 +368,19 @@
                     self.src.name,
                 )
                 if not isinstance(configured_layer_ids_from_yaml, list):
-                    configured_layer_ids_from_yaml = [configured_layer_ids_from_yaml]
+                    configured_layer_ids_from_yaml = [
+                        configured_layer_ids_from_yaml]
 
                 for lid_val in configured_layer_ids_from_yaml:
                     lid_str = str(lid_val)
                     layer_detail = metadata_layers_details.get(lid_str)
 
                     if layer_detail:
-                        layer_name = layer_detail.get("name", f"layer_{lid_str}")
+                        layer_name = layer_detail.get(
+                            "name", f"layer_{lid_str}")
                         layers_to_iterate_final.append(
-                            {"id": lid_str, "name": layer_name, "metadata": layer_detail}
+                            {"id": lid_str, "name": layer_name,
+                                "metadata": layer_detail}
                         )
                     else:
                         log.warning(
@@ -423,9 +432,11 @@
                     else service_meta.get("id", "0")
                 )
                 fs_layer_id_str = str(fs_layer_id)
-                fs_layer_name = service_meta.get("name", f"feature_layer_{fs_layer_id_str}")
+                fs_layer_name = service_meta.get(
+                    "name", f"feature_layer_{fs_layer_id_str}")
                 layers_to_iterate_final.append(
-                    {"id": fs_layer_id_str, "name": fs_layer_name, "metadata": service_meta}
+                    {"id": fs_layer_id_str, "name": fs_layer_name,
+                        "metadata": service_meta}
                 )
 
             if not layers_to_iterate_final:
@@ -436,7 +447,8 @@
                 )
                 return
 
-            log_layer_ids_to_query = [layer["id"] for layer in layers_to_iterate_final]
+            log_layer_ids_to_query = [layer["id"]
+                                      for layer in layers_to_iterate_final]
             log.info(
                 "Source '%s': Will attempt to query %d layer(s): %s",
                 self.src.name,
@@ -446,27 +458,31 @@
 
             # Use concurrent downloads for multiple layers
             if len(layers_to_iterate_final) > 1:
-                self._fetch_layers_concurrent(layers_to_iterate_final, rollback_mgr)
+                self._fetch_layers_concurrent(
+                    layers_to_iterate_final, rollback_mgr)
             else:
                 # Single layer - use original sequential approach
                 for layer_info_to_query in layers_to_iterate_final:
                     self._fetch_layer_data(
                         layer_info=layer_info_to_query,
-                        layer_metadata_from_service=layer_info_to_query.get("metadata"),
+                        layer_metadata_from_service=layer_info_to_query.get(
+                            "metadata"),
                         rollback_mgr=rollback_mgr,
                     )
 
     def _fetch_layers_concurrent(self, layers_to_iterate: List[Dict[str, Any]], rollback_mgr: FileOperationTransaction = None) -> None:
         """Fetch multiple layers concurrently for improved performance."""
-        log.info("üöÄ Starting concurrent download of %d layers", len(layers_to_iterate))
-        
+        log.info("üöÄ Starting concurrent download of %d layers",
+                 len(layers_to_iterate))
+
         # Get concurrent downloader
         downloader = get_layer_downloader()
-        
+
         # Enable parallel processing based on configuration
-        use_concurrent = self.global_config.get("enable_concurrent_downloads", True)
+        use_concurrent = self.global_config.get(
+            "enable_concurrent_downloads", True)
         max_workers = self.global_config.get("concurrent_download_workers", 5)
-        
+
         if not use_concurrent:
             log.info("‚ö†Ô∏è Concurrent downloads disabled, falling back to sequential")
             for layer_info in layers_to_iterate:
@@ -476,42 +492,45 @@
                     rollback_mgr=rollback_mgr,
                 )
             return
-        
+
         # Update worker count if specified
         if max_workers != downloader.manager.max_workers:
             downloader.manager.max_workers = max_workers
-        
+
         # Add rollback actions for potential output files from concurrent downloads
         if rollback_mgr:
             for layer_info in layers_to_iterate:
-                layer_name_original = layer_info.get("name", f"layer_{layer_info.get('id')}")
-                layer_name_sanitized = sanitize_for_filename(layer_name_original)
+                layer_name_original = layer_info.get(
+                    "name", f"layer_{layer_info.get('id')}")
+                layer_name_sanitized = sanitize_for_filename(
+                    layer_name_original)
                 source_name_sanitized = sanitize_for_filename(self.src.name)
                 staging_dir = paths.STAGING / self.src.authority / source_name_sanitized
                 output_format = self.src.raw.get("format", "geojson")
                 output_filename = f"{layer_name_sanitized}.{output_format}"
                 output_path = staging_dir / output_filename
                 rollback_mgr.add_file_deletion(output_path)
-        
+
         # Execute concurrent downloads
         results = downloader.download_layers_concurrent(
             handler=self,
             layers_info=layers_to_iterate,
             fail_fast=self.global_config.get("fail_fast_downloads", False)
         )
-        
+
         # Process results and log statistics
         successful_downloads = sum(1 for r in results if r.success)
         failed_downloads = len(results) - successful_downloads
-        
-        log.info("üèÅ Concurrent downloads completed: %d successful, %d failed", 
-                successful_downloads, failed_downloads)
-        
+
+        log.info("üèÅ Concurrent downloads completed: %d successful, %d failed",
+                 successful_downloads, failed_downloads)
+
         # Log any failures
         for result in results:
             if not result.success:
                 layer_name = result.metadata.get("task_name", "unknown")
-                log.error("‚ùå Layer download failed: %s - %s", layer_name, result.error)
+                log.error("‚ùå Layer download failed: %s - %s",
+                          layer_name, result.error)
 
     def _determine_max_record_count(
         self,
@@ -523,7 +542,8 @@
         if max_record_count_from_config is not None:
             try:
                 max_record_count = int(max_record_count_from_config)
-                log.debug("Using max_record_count from config: %d", max_record_count)
+                log.debug("Using max_record_count from config: %d",
+                          max_record_count)
                 return max_record_count, layer_meta
             except ValueError:
                 log.warning(
@@ -542,7 +562,8 @@
         if layer_meta:
             if layer_meta.get("maxRecordCount") is not None:
                 max_record_count = layer_meta["maxRecordCount"]
-                log.debug("Service metadata maxRecordCount: %d", max_record_count)
+                log.debug("Service metadata maxRecordCount: %d",
+                          max_record_count)
             elif layer_meta.get("standardMaxRecordCount") is not None:
                 max_record_count = layer_meta["standardMaxRecordCount"]
                 log.debug(
@@ -676,10 +697,10 @@
         if not layer_id:
             log.error("‚ùå Layer ID is missing from layer_info: %s", layer_info)
             return
-            
+
         layer_name_original = layer_info.get("name", f"layer_{layer_id}")
         layer_name_sanitized = sanitize_for_filename(layer_name_original)
-        
+
         query_url = f"{self.src.url.rstrip('/')}/{layer_id}/query"
         log.info("üöö %s", layer_name_sanitized)
         log.debug(
@@ -703,7 +724,7 @@
 
         output_filename = f"{layer_name_sanitized}.{params['f']}"
         output_path = staging_dir / output_filename
-        
+
         # Add rollback action for the output file
         if rollback_mgr:
             rollback_mgr.add_file_deletion(output_path)
@@ -714,7 +735,7 @@
             layer_name_sanitized=layer_name_sanitized,
             max_record_count=max_record_count,
         )
-        
+
         if not all_features:
             if features_written_total == 0:
                 log.info("‚ÑπÔ∏è %s: no features", layer_name_sanitized)
--- original/./etl/handlers/atom_feed.py
+++ fixed/./etl/handlers/atom_feed.py
@@ -32,18 +32,22 @@
 
     def fetch(self) -> None:
         if not self.src.enabled:
-            log.info("‚è≠Ô∏è Source '%s' (Atom Feed) is disabled, skipping fetch.", self.src.name)
+            log.info(
+                "‚è≠Ô∏è Source '%s' (Atom Feed) is disabled, skipping fetch.", self.src.name)
             return
 
-        log.info("üåê Reading Atom feed for source '%s' from URL: %s", self.src.name, self.src.url)
+        log.info("üåê Reading Atom feed for source '%s' from URL: %s",
+                 self.src.name, self.src.url)
 
         sanitized_source_name = sanitize_for_filename(self.src.name)
         feed_xml_filename = sanitized_source_name + "_feed.xml"
         feed_xml_download_path = paths.DOWNLOADS / feed_xml_filename
 
         try:
-            downloaded_feed_xml_path = download(self.src.url, feed_xml_download_path)
-            feed_xml_content = downloaded_feed_xml_path.read_text(encoding="utf-8")
+            downloaded_feed_xml_path = download(
+                self.src.url, feed_xml_download_path)
+            feed_xml_content = downloaded_feed_xml_path.read_text(
+                encoding="utf-8")
         except Exception as e:
             log.error("‚ùå Failed to download or read Atom feed XML for '%s' from %s: %s",
                       self.src.name, self.src.url, e, exc_info=True)
@@ -52,17 +56,19 @@
         try:
             root = ET.fromstring(feed_xml_content)
         except ET.ParseError as exc:
-            log.error("‚ùå Malformed Atom XML for source '%s': %s", self.src.name, exc, exc_info=True)
+            log.error("‚ùå Malformed Atom XML for source '%s': %s",
+                      self.src.name, exc, exc_info=True)
             return
 
-        source_specific_staging_dir = paths.STAGING / self.src.authority / sanitized_source_name
+        source_specific_staging_dir = paths.STAGING / \
+            self.src.authority / sanitized_source_name
         source_specific_staging_dir.mkdir(parents=True, exist_ok=True)
         log.info("Target staging directory for all content from Atom source '%s': %s",
                  self.src.name, source_specific_staging_dir.relative_to(paths.ROOT))
 
         urls_seen: Set[str] = set()
         processed_any_links = False
-        
+
         for entry in root.findall("atom:entry", _ATOM_NS):
             link_element = (
                 entry.find("atom:link[@rel='enclosure']", _ATOM_NS)
@@ -82,35 +88,39 @@
                 continue
             urls_seen.add(dl_url)
 
-            log.info("Found linked resource in Atom feed for '%s': %s", self.src.name, dl_url)
-            self._download_and_stage_linked_resource(dl_url, source_specific_staging_dir, sanitized_source_name)
+            log.info("Found linked resource in Atom feed for '%s': %s",
+                     self.src.name, dl_url)
+            self._download_and_stage_linked_resource(
+                dl_url, source_specific_staging_dir, sanitized_source_name)
             processed_any_links = True
 
         if not processed_any_links:
-            log.info("No downloadable resources found or processed in Atom feed for '%s'.", self.src.name)
+            log.info(
+                "No downloadable resources found or processed in Atom feed for '%s'.", self.src.name)
 
     def _download_and_stage_linked_resource(
-            self,
-            dl_url: str,
-            target_staging_dir: Path,
-            sanitized_atom_source_name: str
-        ) -> None:
+        self,
+        dl_url: str,
+        target_staging_dir: Path,
+        sanitized_atom_source_name: str
+    ) -> None:
         """
         Downloads a resource linked from an Atom feed entry and stages it into the
         provided target_staging_dir.
         """
         try:
-            true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(dl_url)
+            true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(
+                dl_url)
             sanitized_true_stem = sanitize_for_filename(true_stem_from_web)
 
             final_ext_for_download = true_ext_from_web
             if not final_ext_for_download or len(final_ext_for_download) > 5:
                 log.warning("Suspicious or missing extension ('%s') for %s. Checking source.download_format or defaulting.",
-                           final_ext_for_download, dl_url)
+                            final_ext_for_download, dl_url)
                 if self.src.download_format:
                     final_ext_for_download = f".{self.src.download_format.lower().lstrip('.')}"
                     log.info("Using source.download_format for extension: '%s' for %s",
-                            final_ext_for_download, dl_url)
+                             final_ext_for_download, dl_url)
                 else:
                     path_ext = Path(unquote(dl_url)).suffix.lower()
                     if path_ext in [".zip", ".gpkg"]:
@@ -118,7 +128,7 @@
                     else:
                         final_ext_for_download = ".zip"  # Default to .zip if truly unknown
                     log.info("Inferred or defaulted extension to '%s' for %s",
-                            final_ext_for_download, dl_url)
+                             final_ext_for_download, dl_url)
 
             download_filename = sanitized_true_stem + final_ext_for_download
             download_target_path = paths.DOWNLOADS / download_filename
@@ -132,7 +142,8 @@
             is_downloaded_gpkg = downloaded_file_path.suffix.lower() == ".gpkg"
 
             if is_downloaded_zip:
-                log.info("Downloaded file '%s' is a ZIP archive. Extracting...", downloaded_file_path.name)
+                log.info(
+                    "Downloaded file '%s' is a ZIP archive. Extracting...", downloaded_file_path.name)
                 try:
                     extract_zip(downloaded_file_path, target_staging_dir)
                     log.info("‚ûï Extracted and staged archive %s into %s",
@@ -141,11 +152,12 @@
                     if self.src.staged_data_type == "gpkg":
                         log.info("Source '%s' expects a GPKG. Searching in extracted contents of %s...",
                                  self.src.name, downloaded_file_path.name)
-                        extracted_gpkgs = list(target_staging_dir.rglob("*.gpkg"))
+                        extracted_gpkgs = list(
+                            target_staging_dir.rglob("*.gpkg"))
                         if extracted_gpkgs:
                             if len(extracted_gpkgs) > 1:
                                 log.warning("Found multiple GPKGs in extracted archive, using the first: %s",
-                                           extracted_gpkgs[0].name)
+                                            extracted_gpkgs[0].name)
 
                             expected_staged_gpkg_name = sanitized_atom_source_name + ".gpkg"
                             final_staged_gpkg_path = target_staging_dir / expected_staged_gpkg_name
@@ -153,7 +165,8 @@
                             if extracted_gpkgs[0].resolve() != final_staged_gpkg_path.resolve():
                                 log.info("Renaming/moving extracted GPKG '%s' to '%s'",
                                          extracted_gpkgs[0].name, final_staged_gpkg_path.name)
-                                shutil.move(str(extracted_gpkgs[0]), str(final_staged_gpkg_path))
+                                shutil.move(str(extracted_gpkgs[0]), str(
+                                    final_staged_gpkg_path))
                             else:
                                 log.info("Extracted GPKG '%s' already has the expected name and location.",
                                          final_staged_gpkg_path.name)
@@ -161,13 +174,13 @@
                                      final_staged_gpkg_path.name, self.src.name)
                         else:
                             log.warning("‚ö†Ô∏è Source '%s' expected a GPKG, but no .gpkg file found after extracting %s.",
-                                      self.src.name, downloaded_file_path.name)
+                                        self.src.name, downloaded_file_path.name)
                 except zipfile.BadZipFile:
                     log.error("‚ùå File '%s' from Atom link is not a valid ZIP file. Cannot extract.",
-                             downloaded_file_path.name)
+                              downloaded_file_path.name)
                 except Exception as e:
                     log.error("‚ùå Failed to extract archive %s to %s: %s",
-                             downloaded_file_path.name, target_staging_dir, e, exc_info=True)
+                              downloaded_file_path.name, target_staging_dir, e, exc_info=True)
 
             elif is_downloaded_gpkg:
                 staged_gpkg_filename = sanitized_atom_source_name + ".gpkg"
@@ -179,10 +192,10 @@
                                  downloaded_file_path.name, staged_gpkg_path.relative_to(paths.ROOT))
                     else:
                         log.info("‚ÑπÔ∏è Downloaded GPKG '%s' is already in the target staging location.",
-                                downloaded_file_path.name)
+                                 downloaded_file_path.name)
                 except Exception as e:
                     log.error("‚ùå Failed to copy downloaded GPKG %s to staging location %s: %s",
-                             downloaded_file_path.name, staged_gpkg_path, e, exc_info=True)
+                              downloaded_file_path.name, staged_gpkg_path, e, exc_info=True)
 
             else:
                 log.warning(
@@ -192,7 +205,8 @@
                 )
 
         except Exception as e:
-            log.error("‚ùå Failed to download/stage resource from Atom link %s: %s", dl_url, e, exc_info=True)
+            log.error("‚ùå Failed to download/stage resource from Atom link %s: %s",
+                      dl_url, e, exc_info=True)
 
     def __enter__(self) -> 'AtomFeedDownloadHandler':
         """Enter the context manager for use with 'with' statements."""
@@ -200,4 +214,4 @@
 
     def __exit__(self, exc_type, exc_val, exc_tb) -> None:
         """Exit the context manager. No cleanup needed for Atom feed downloads."""
-        pass
\ No newline at end of file
+        pass
--- original/./tests/test_runner.py
+++ fixed/./tests/test_runner.py
@@ -11,10 +11,11 @@
 import subprocess
 from pathlib import Path
 
+
 def run_tests(test_type="all"):
     """Run tests based on type."""
     cmd = ["python", "-m", "pytest"]
-    
+
     if test_type == "unit":
         cmd.extend(["-m", "unit", "tests/unit/"])
     elif test_type == "integration":
@@ -26,18 +27,20 @@
     else:
         print(f"Unknown test type: {test_type}")
         sys.exit(1)
-    
+
     # Add coverage if available
     try:
-        subprocess.run(["python", "-c", "import coverage"], check=True, capture_output=True)
+        subprocess.run(["python", "-c", "import coverage"],
+                       check=True, capture_output=True)
         cmd.extend(["--cov=etl", "--cov-report=html", "--cov-report=term"])
     except subprocess.CalledProcessError:
         pass  # Coverage not available
-    
+
     print(f"Running {test_type} tests...")
     result = subprocess.run(cmd)
     sys.exit(result.returncode)
 
+
 if __name__ == "__main__":
     test_type = sys.argv[1] if len(sys.argv) > 1 else "all"
-    run_tests(test_type)
\ No newline at end of file
+    run_tests(test_type)
--- original/./tests/unit/__init__.py
+++ fixed/./tests/unit/__init__.py
@@ -1 +1 @@
-# Unit tests package
\ No newline at end of file
+# Unit tests package
--- original/./tests/unit/test_utils_sanitize.py
+++ fixed/./tests/unit/test_utils_sanitize.py
@@ -6,64 +6,67 @@
 
 class TestSlugify:
     """Test the slugify function."""
-    
+
     @pytest.mark.unit
     def test_slugify_basic(self):
         assert slugify("Hello World") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_slugify_swedish_characters(self):
         assert slugify("√Öland √Ñpplen √ñppna") == "aland_applen_oppna"
         assert slugify("√ÖLAND √ÑPPLEN √ñPPNA") == "aland_applen_oppna"
-    
+
     @pytest.mark.unit
     def test_slugify_special_characters(self):
         assert slugify("Hello-World!@#$%") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_slugify_multiple_spaces(self):
         assert slugify("Hello   World    Test") == "hello_world_test"
-    
+
     @pytest.mark.unit
     def test_slugify_leading_trailing_spaces(self):
         assert slugify("  Hello World  ") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_slugify_numbers(self):
         assert slugify("Test123 Data456") == "test123_data456"
-    
+
     @pytest.mark.unit
     def test_slugify_hyphens_preserved_then_converted(self):
         # Based on the code, hyphens are kept in regex but then converted later
         assert slugify("Hello-World-Test") == "hello_world_test"
-    
+
     @pytest.mark.unit
     def test_slugify_underscores_preserved(self):
         assert slugify("Hello_World_Test") == "hello_world_test"
-    
+
     @pytest.mark.unit
     def test_slugify_empty_string(self):
         assert slugify("") == "unnamed"
-    
+
     @pytest.mark.unit
     def test_slugify_whitespace_only(self):
         assert slugify("   ") == "unnamed"
-    
+
     @pytest.mark.unit
     def test_slugify_special_chars_only(self):
         assert slugify("!@#$%^&*()") == "unnamed"
-    
+
     @pytest.mark.unit
     def test_slugify_consecutive_underscores_collapsed(self):
         assert slugify("Hello____World") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_slugify_mixed_case_with_swedish(self):
         assert slugify("TeSt √Ö√§√ñ DaTa") == "test_aao_data"
-    
+
     @pytest.mark.unit
     def test_slugify_real_world_examples(self):
         # Test with realistic source names
-        assert slugify("Naturv√•rdsverket - Naturv√•rdsregistret") == "naturvardsverket_naturvardsregistret"
-        assert slugify("F√∂rsvarsmakten - Rikst√§ckande geodata") == "forsvarsmakten_rikstackande_geodata"
-        assert slugify("SGU - Berggrundskarta 1:50 000") == "sgu_berggrundskarta_1_50_000"
\ No newline at end of file
+        assert slugify(
+            "Naturv√•rdsverket - Naturv√•rdsregistret") == "naturvardsverket_naturvardsregistret"
+        assert slugify(
+            "F√∂rsvarsmakten - Rikst√§ckande geodata") == "forsvarsmakten_rikstackande_geodata"
+        assert slugify(
+            "SGU - Berggrundskarta 1:50 000") == "sgu_berggrundskarta_1_50_000"
--- original/./tests/unit/test_resource_compatibility.py
+++ fixed/./tests/unit/test_resource_compatibility.py
@@ -3,23 +3,24 @@
 import sys
 import os
 
+
 class TestResourceModuleCompatibility(unittest.TestCase):
     """Test that performance_optimizer works when resource module is not available (e.g., on Windows)."""
-    
+
     def test_no_resource_import_in_performance_optimizer(self):
         """Test that performance_optimizer.py does not import the resource module."""
         perf_opt_path = os.path.join(
-            os.path.dirname(__file__), '..', '..', 
+            os.path.dirname(__file__), '..', '..',
             'etl', 'utils', 'performance_optimizer.py'
         )
-        
+
         with open(perf_opt_path, 'r', encoding='utf-8') as f:
             content = f.read()
-        
+
         # Check that resource import has been removed
-        self.assertNotIn("import resource", content, 
+        self.assertNotIn("import resource", content,
                          "performance_optimizer.py should not import the resource module")
-        
+
         # Verify that the critical classes still exist
         self.assertIn("class SystemResources:", content,
                       "SystemResources class should still be present")
@@ -27,22 +28,22 @@
                       "is_under_pressure method should still be present")
         self.assertIn("pressure_level", content,
                       "pressure_level method should still be present")
-    
+
     def test_system_resources_functionality(self):
         """Test that SystemResources class works correctly without resource module."""
         # Import the SystemResources class definition directly
         sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
-        
+
         # Define SystemResources locally to avoid import issues with concurrent.py conflicts
         class SystemResources:
-            def __init__(self, cpu_percent: float, memory_percent: float, 
+            def __init__(self, cpu_percent: float, memory_percent: float,
                          memory_available_gb: float, disk_free_gb: float, network_connections: int):
                 self.cpu_percent = cpu_percent
                 self.memory_percent = memory_percent
                 self.memory_available_gb = memory_available_gb
                 self.disk_free_gb = disk_free_gb
                 self.network_connections = network_connections
-            
+
             @property
             def is_under_pressure(self) -> bool:
                 return (
@@ -50,7 +51,7 @@
                     self.memory_percent > 85 or
                     self.memory_available_gb < 0.5
                 )
-            
+
             @property
             def pressure_level(self) -> str:
                 if self.cpu_percent > 90 or self.memory_percent > 95:
@@ -60,7 +61,7 @@
                 elif self.cpu_percent > 60 or self.memory_percent > 70:
                     return "moderate"
                 return "low"
-        
+
         # Test normal conditions (no pressure)
         normal_resources = SystemResources(
             cpu_percent=50.0,
@@ -71,7 +72,7 @@
         )
         self.assertFalse(normal_resources.is_under_pressure)
         self.assertEqual(normal_resources.pressure_level, "low")
-        
+
         # Test high pressure conditions
         high_pressure = SystemResources(
             cpu_percent=85.0,
@@ -82,7 +83,7 @@
         )
         self.assertTrue(high_pressure.is_under_pressure)
         self.assertEqual(high_pressure.pressure_level, "high")
-        
+
         # Test critical pressure conditions
         critical_pressure = SystemResources(
             cpu_percent=95.0,
@@ -94,5 +95,6 @@
         self.assertTrue(critical_pressure.is_under_pressure)
         self.assertEqual(critical_pressure.pressure_level, "critical")
 
+
 if __name__ == '__main__':
-    unittest.main()
\ No newline at end of file
+    unittest.main()
--- original/./tests/unit/test_exceptions.py
+++ fixed/./tests/unit/test_exceptions.py
@@ -43,7 +43,7 @@
 
 class TestETLError:
     """Test base ETLError functionality."""
-    
+
     @pytest.mark.unit
     def test_basic_etl_error(self):
         error = ETLError("Test error message")
@@ -53,7 +53,7 @@
         assert error.context == {}
         assert error.recoverable is True
         assert error.retry_after is None
-    
+
     @pytest.mark.unit
     def test_etl_error_with_context(self):
         error = ETLError(
@@ -67,7 +67,7 @@
         assert error.context["key"] == "value"
         assert error.recoverable is False
         assert error.retry_after == 60
-    
+
     @pytest.mark.unit
     def test_etl_error_string_representation(self):
         error = ETLError("Test error", source_name="test_source")
@@ -76,13 +76,13 @@
 
 class TestConfigurationErrors:
     """Test configuration-related errors."""
-    
+
     @pytest.mark.unit
     def test_configuration_error(self):
         error = ConfigurationError("Invalid config", config_file="config.yaml")
         assert error.config_file == "config.yaml"
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_validation_error(self):
         error = ValidationError("Invalid field", field_name="test_field")
@@ -91,29 +91,29 @@
 
 class TestSourceErrors:
     """Test source-related errors."""
-    
+
     @pytest.mark.unit
     def test_source_unavailable_error(self):
         error = SourceUnavailableError("Service temporarily down")
         assert error.retry_after == 300  # Default 5 minutes
         assert error.recoverable is True
-    
+
     @pytest.mark.unit
     def test_source_not_found_error(self):
         error = SourceNotFoundError("Source does not exist")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_authentication_error(self):
         error = AuthenticationError("Invalid credentials")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_rate_limit_error(self):
         error = RateLimitError("Rate limit exceeded", retry_after=120)
         assert error.retry_after == 120
         assert error.recoverable is True
-    
+
     @pytest.mark.unit
     def test_rate_limit_error_default_retry(self):
         error = RateLimitError("Rate limit exceeded")
@@ -122,35 +122,36 @@
 
 class TestNetworkErrors:
     """Test network-related errors."""
-    
+
     @pytest.mark.unit
     def test_network_error(self):
         error = NetworkError("Network issue")
         assert error.retry_after == 30  # Default
         assert error.recoverable is True
-    
+
     @pytest.mark.unit
     def test_connection_error(self):
         error = ConnectionError("Connection failed")
         assert isinstance(error, NetworkError)
-    
+
     @pytest.mark.unit
     def test_timeout_error(self):
         error = TimeoutError("Request timed out")
         assert isinstance(error, NetworkError)
-    
+
     @pytest.mark.unit
     def test_http_error_recoverable(self):
         error = HTTPError("Server error", status_code=500)
         assert error.status_code == 500
         assert error.recoverable is True  # 5xx errors are recoverable
-    
+
     @pytest.mark.unit
     def test_http_error_non_recoverable(self):
         error = HTTPError("Bad request", status_code=400)
         assert error.status_code == 400
-        assert error.recoverable is False  # 4xx errors (except 429) are not recoverable
-    
+        # 4xx errors (except 429) are not recoverable
+        assert error.recoverable is False
+
     @pytest.mark.unit
     def test_http_error_rate_limit_recoverable(self):
         error = HTTPError("Too many requests", status_code=429)
@@ -160,23 +161,24 @@
 
 class TestDataErrors:
     """Test data processing errors."""
-    
+
     @pytest.mark.unit
     def test_data_format_error(self):
         error = DataFormatError("Invalid JSON", format_type="json")
         assert error.format_type == "json"
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_data_quality_error(self):
-        error = DataQualityError("Invalid geometry", quality_check="geometry_validation")
+        error = DataQualityError(
+            "Invalid geometry", quality_check="geometry_validation")
         assert error.quality_check == "geometry_validation"
-    
+
     @pytest.mark.unit
     def test_transformation_error(self):
         error = TransformationError("Projection failed")
         assert isinstance(error, DataError)
-    
+
     @pytest.mark.unit
     def test_geospatial_error(self):
         error = GeospatialError("Buffer operation failed", operation="buffer")
@@ -185,17 +187,17 @@
 
 class TestStorageErrors:
     """Test storage-related errors."""
-    
+
     @pytest.mark.unit
     def test_disk_space_error(self):
         error = DiskSpaceError("Insufficient disk space")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_permission_error(self):
         error = PermissionError("Access denied")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_file_not_found_error(self):
         error = FileNotFoundError("File missing", file_path="/path/to/file")
@@ -205,18 +207,18 @@
 
 class TestDatabaseErrors:
     """Test database-related errors."""
-    
+
     @pytest.mark.unit
     def test_connection_pool_error(self):
         error = ConnectionPoolError("Pool exhausted")
         assert error.retry_after == 60
         assert error.recoverable is True
-    
+
     @pytest.mark.unit
     def test_schema_error(self):
         error = SchemaError("Invalid schema")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_load_error(self):
         error = LoadError("Load failed")
@@ -225,17 +227,17 @@
 
 class TestArcGISErrors:
     """Test ArcGIS-related errors."""
-    
+
     @pytest.mark.unit
     def test_geoprocessing_error(self):
         error = GeoprocessingError("Tool failed", tool_name="Buffer")
         assert error.tool_name == "Buffer"
-    
+
     @pytest.mark.unit
     def test_license_error(self):
         error = LicenseError("License unavailable")
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_workspace_error(self):
         error = WorkspaceError("Cannot access workspace")
@@ -244,50 +246,51 @@
 
 class TestPipelineErrors:
     """Test pipeline workflow errors."""
-    
+
     @pytest.mark.unit
     def test_dependency_error(self):
         error = DependencyError("Missing dependency", dependency="arcpy")
         assert error.dependency == "arcpy"
         assert error.recoverable is False
-    
+
     @pytest.mark.unit
     def test_resource_error(self):
         error = ResourceError("Insufficient memory", resource_type="memory")
         assert error.resource_type == "memory"
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_error(self):
-        error = CircuitBreakerError("Circuit breaker open", service_name="api_service")
+        error = CircuitBreakerError(
+            "Circuit breaker open", service_name="api_service")
         assert error.service_name == "api_service"
         assert error.retry_after == 300
 
 
 class TestUtilityFunctions:
     """Test utility functions for error handling."""
-    
+
     @pytest.mark.unit
     def test_is_recoverable_error_with_etl_error(self):
         recoverable_error = SourceUnavailableError("Temporary issue")
         non_recoverable_error = SourceNotFoundError("Not found")
-        
+
         assert is_recoverable_error(recoverable_error) is True
         assert is_recoverable_error(non_recoverable_error) is False
-    
+
     @pytest.mark.unit
     def test_is_recoverable_error_with_standard_exceptions(self):
         assert is_recoverable_error(ConnectionRefusedError()) is True
         assert is_recoverable_error(TimeoutError()) is True
         assert is_recoverable_error(ValueError()) is False
-    
+
     @pytest.mark.unit
     def test_get_retry_delay(self):
         error_with_delay = RateLimitError("Rate limited", retry_after=120)
         error_without_delay = ValueError("Standard error")
-        
+
         assert get_retry_delay(error_with_delay) == 120
         assert get_retry_delay(error_without_delay) is None
-    
+
     @pytest.mark.unit
     def test_format_error_context(self):
         error = ETLError(
@@ -296,15 +299,15 @@
             context={"url": "https://example.com", "status": 500},
             recoverable=False
         )
-        
+
         formatted = format_error_context(error)
         assert "Test error (source: test_source)" in formatted
         assert "url=https://example.com" in formatted
         assert "status=500" in formatted
         assert "(non-recoverable)" in formatted
-    
+
     @pytest.mark.unit
     def test_format_error_context_with_retry_after(self):
         error = ETLError("Test error", retry_after=60)
         formatted = format_error_context(error)
-        assert "(retry after 60s)" in formatted
\ No newline at end of file
+        assert "(retry after 60s)" in formatted
--- original/./tests/unit/test_models.py
+++ fixed/./tests/unit/test_models.py
@@ -9,44 +9,44 @@
 
 class TestParseInclude:
     """Test the _parse_include helper function."""
-    
+
     @pytest.mark.unit
     def test_parse_include_none(self):
         assert _parse_include(None) == []
-    
+
     @pytest.mark.unit
     def test_parse_include_empty_list(self):
         assert _parse_include([]) == []
-    
+
     @pytest.mark.unit
     def test_parse_include_string(self):
         assert _parse_include("item1") == ["item1"]
-    
+
     @pytest.mark.unit
     def test_parse_include_semicolon_separated(self):
         result = _parse_include("item1;item2;item3")
         assert result == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_parse_include_comma_separated(self):
         result = _parse_include("item1,item2,item3")
         assert result == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_parse_include_mixed_separators(self):
         result = _parse_include("item1;item2,item3")
         assert result == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_parse_include_with_trailing_periods(self):
         result = _parse_include("item1.;item2.,item3.")
         assert result == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_parse_include_with_whitespace(self):
         result = _parse_include("  item1  ; item2  ,  item3  ")
         assert result == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_parse_include_list_input(self):
         result = _parse_include(["item1", "item2;item3", "item4,item5"])
@@ -55,7 +55,7 @@
 
 class TestSource:
     """Test the Source dataclass."""
-    
+
     @pytest.mark.unit
     def test_source_defaults(self):
         source = Source(name="Test", authority="TEST")
@@ -68,7 +68,7 @@
         assert source.staged_data_type is None
         assert source.include == []
         assert source.raw == {}
-    
+
     @pytest.mark.unit
     def test_source_from_dict_basic(self):
         data = {
@@ -84,7 +84,7 @@
         assert source.type == "rest_api"
         assert source.url == "https://example.com"
         assert source.enabled is False
-    
+
     @pytest.mark.unit
     def test_source_from_dict_with_include(self):
         data = {
@@ -94,7 +94,7 @@
         }
         source = Source.from_dict(data)
         assert source.include == ["item1", "item2", "item3"]
-    
+
     @pytest.mark.unit
     def test_source_from_dict_with_raw_dict(self):
         data = {
@@ -104,7 +104,7 @@
         }
         source = Source.from_dict(data)
         assert source.raw == {"custom_param": "value", "timeout": 30}
-    
+
     @pytest.mark.unit
     def test_source_from_dict_with_unknown_fields(self):
         data = {
@@ -114,8 +114,9 @@
             "another_field": 42
         }
         source = Source.from_dict(data)
-        assert source.raw == {"custom_field": "custom_value", "another_field": 42}
-    
+        assert source.raw == {
+            "custom_field": "custom_value", "another_field": 42}
+
     @pytest.mark.unit
     def test_source_from_dict_combined_raw_and_unknown(self):
         data = {
@@ -125,36 +126,37 @@
             "unknown_field": "unknown_value"
         }
         source = Source.from_dict(data)
-        expected_raw = {"explicit_raw": "value", "unknown_field": "unknown_value"}
+        expected_raw = {"explicit_raw": "value",
+                        "unknown_field": "unknown_value"}
         assert source.raw == expected_raw
-    
+
     @pytest.mark.unit
     def test_load_all_empty_file(self, temp_dir):
         """Test loading from empty YAML file."""
         yaml_file = temp_dir / "empty.yaml"
         yaml_file.write_text("")
-        
-        sources = Source.load_all(yaml_file)
-        assert sources == []
-    
+
+        sources = Source.load_all(yaml_file)
+        assert sources == []
+
     @pytest.mark.unit
     def test_load_all_missing_sources_key(self, temp_dir):
         """Test loading from YAML without 'sources' key."""
         yaml_file = temp_dir / "no_sources.yaml"
         yaml_file.write_text("other_key: value")
-        
-        sources = Source.load_all(yaml_file)
-        assert sources == []
-    
+
+        sources = Source.load_all(yaml_file)
+        assert sources == []
+
     @pytest.mark.unit
     def test_load_all_invalid_sources_type(self, temp_dir):
         """Test loading when sources is not a list."""
         yaml_file = temp_dir / "invalid_sources.yaml"
         yaml_file.write_text("sources: not_a_list")
-        
-        sources = Source.load_all(yaml_file)
-        assert sources == []
-    
+
+        sources = Source.load_all(yaml_file)
+        assert sources == []
+
     @pytest.mark.unit
     def test_load_all_valid_sources(self, temp_dir):
         """Test loading valid sources."""
@@ -174,32 +176,32 @@
         """
         yaml_file = temp_dir / "valid_sources.yaml"
         yaml_file.write_text(yaml_content)
-        
+
         sources = Source.load_all(yaml_file)
         assert len(sources) == 2
-        
+
         assert sources[0].name == "Source 1"
         assert sources[0].authority == "AUTH1"
         assert sources[0].type == "rest_api"
         assert sources[0].enabled is True
-        
+
         assert sources[1].name == "Source 2"
         assert sources[1].authority == "AUTH2"
         assert sources[1].type == "file"
         assert sources[1].enabled is False
         assert sources[1].include == ["item1", "item2"]
-    
+
     @pytest.mark.unit
     def test_load_all_nonexistent_file(self):
         """Test loading from nonexistent file."""
         sources = Source.load_all("nonexistent.yaml")
         assert sources == []
-    
+
     @pytest.mark.unit
     def test_load_all_invalid_yaml(self, temp_dir):
         """Test loading from invalid YAML."""
         yaml_file = temp_dir / "invalid.yaml"
         yaml_file.write_text("sources: [\ninvalid yaml")
-        
-        sources = Source.load_all(yaml_file)
-        assert sources == []
\ No newline at end of file
+
+        sources = Source.load_all(yaml_file)
+        assert sources == []
--- original/./tests/unit/test_retry.py
+++ fixed/./tests/unit/test_retry.py
@@ -20,7 +20,7 @@
 
 class TestRetryConfig:
     """Test RetryConfig functionality."""
-    
+
     @pytest.mark.unit
     def test_default_retry_config(self):
         config = RetryConfig()
@@ -30,7 +30,7 @@
         assert config.max_delay == 300.0
         assert config.jitter is True
         assert config.exponential is True
-    
+
     @pytest.mark.unit
     def test_custom_retry_config(self):
         config = RetryConfig(
@@ -47,46 +47,49 @@
         assert config.max_delay == 600.0
         assert config.jitter is False
         assert config.exponential is False
-    
+
     @pytest.mark.unit
     def test_should_retry_max_attempts(self):
         config = RetryConfig(max_attempts=3)
         assert config.should_retry(NetworkError("test"), 1) is True
         assert config.should_retry(NetworkError("test"), 3) is False
         assert config.should_retry(NetworkError("test"), 4) is False
-    
+
     @pytest.mark.unit
     def test_should_retry_recoverable_exceptions(self):
         config = RetryConfig()
         assert config.should_retry(NetworkError("test"), 1) is True
         assert config.should_retry(SourceUnavailableError("test"), 1) is True
         assert config.should_retry(ValueError("test"), 1) is False
-    
+
     @pytest.mark.unit
     def test_get_delay_exponential(self):
-        config = RetryConfig(base_delay=1.0, backoff_factor=2.0, exponential=True, jitter=False)
+        config = RetryConfig(base_delay=1.0, backoff_factor=2.0,
+                             exponential=True, jitter=False)
         assert config.get_delay(1) == 1.0
         assert config.get_delay(2) == 2.0
         assert config.get_delay(3) == 4.0
-    
+
     @pytest.mark.unit
     def test_get_delay_linear(self):
         config = RetryConfig(base_delay=2.0, exponential=False, jitter=False)
         assert config.get_delay(1) == 2.0
         assert config.get_delay(2) == 2.0
         assert config.get_delay(3) == 2.0
-    
+
     @pytest.mark.unit
     def test_get_delay_max_delay(self):
-        config = RetryConfig(base_delay=100.0, backoff_factor=10.0, max_delay=200.0, jitter=False)
-        assert config.get_delay(3) == 200.0  # Would be 1000.0, but capped at max_delay
-    
+        config = RetryConfig(
+            base_delay=100.0, backoff_factor=10.0, max_delay=200.0, jitter=False)
+        # Would be 1000.0, but capped at max_delay
+        assert config.get_delay(3) == 200.0
+
     @pytest.mark.unit
     def test_get_delay_with_exception_retry_after(self):
         config = RetryConfig()
         error = RateLimitError("Rate limited", retry_after=120)
         assert config.get_delay(1, error) == 120.0
-    
+
     @pytest.mark.unit
     def test_get_delay_with_jitter(self):
         config = RetryConfig(base_delay=10.0, jitter=True, exponential=False)
@@ -97,93 +100,93 @@
 
 class TestCircuitBreaker:
     """Test CircuitBreaker functionality."""
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_closed_state(self):
         cb = CircuitBreaker(failure_threshold=3)
         assert cb.state == "CLOSED"
         assert cb.failure_count == 0
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_success(self):
         cb = CircuitBreaker(failure_threshold=3)
-        
-        @cb
-        def test_function():
-            return "success"
-        
+
+        @cb
+        def test_function():
+            return "success"
+
         result = test_function()
         assert result == "success"
         assert cb.state == "CLOSED"
         assert cb.failure_count == 0
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_single_failure(self):
         cb = CircuitBreaker(failure_threshold=3)
-        
+
         @cb
         def test_function():
             raise Exception("test error")
-        
+
         with pytest.raises(Exception, match="test error"):
             test_function()
-        
+
         assert cb.state == "CLOSED"  # Still closed, below threshold
         assert cb.failure_count == 1
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_opens_after_threshold(self):
         cb = CircuitBreaker(failure_threshold=2)
-        
+
         @cb
         def test_function():
             raise Exception("test error")
-        
+
         # First failure
         with pytest.raises(Exception):
             test_function()
         assert cb.state == "CLOSED"
-        
+
         # Second failure - should open circuit
         with pytest.raises(Exception):
             test_function()
         assert cb.state == "OPEN"
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_open_state_blocks_calls(self):
         cb = CircuitBreaker(failure_threshold=1)
-        
+
         @cb
         def test_function():
             raise Exception("test error")
-        
+
         # Trigger circuit breaker
         with pytest.raises(Exception):
             test_function()
         assert cb.state == "OPEN"
-        
+
         # Next call should raise CircuitBreakerError
         with pytest.raises(CircuitBreakerError):
             test_function()
-    
+
     @pytest.mark.unit
     def test_circuit_breaker_half_open_recovery(self):
         cb = CircuitBreaker(failure_threshold=1, recovery_timeout=0.1)
-        
+
         @cb
         def test_function(should_fail=True):
             if should_fail:
                 raise Exception("test error")
             return "success"
-        
+
         # Trigger circuit breaker
         with pytest.raises(Exception):
             test_function()
         assert cb.state == "OPEN"
-        
+
         # Wait for recovery timeout
         time.sleep(0.15)
-        
+
         # Next call should attempt half-open
         result = test_function(should_fail=False)
         assert result == "success"
@@ -192,25 +195,25 @@
 
 class TestRetryWithBackoff:
     """Test retry_with_backoff decorator."""
-    
+
     @pytest.mark.unit
     def test_successful_function_no_retry(self):
         call_count = 0
-        
+
         @retry_with_backoff(max_attempts=3, base_delay=0.01)
         def test_function():
             nonlocal call_count
             call_count += 1
             return "success"
-        
+
         result = test_function()
         assert result == "success"
         assert call_count == 1
-    
+
     @pytest.mark.unit
     def test_function_succeeds_after_retries(self):
         call_count = 0
-        
+
         @retry_with_backoff(max_attempts=3, base_delay=0.01)
         def test_function():
             nonlocal call_count
@@ -218,44 +221,44 @@
             if call_count < 3:
                 raise NetworkError("temporary failure")
             return "success"
-        
+
         result = test_function()
         assert result == "success"
         assert call_count == 3
-    
+
     @pytest.mark.unit
     def test_function_fails_after_max_attempts(self):
         call_count = 0
-        
+
         @retry_with_backoff(max_attempts=2, base_delay=0.01)
         def test_function():
             nonlocal call_count
             call_count += 1
             raise NetworkError("persistent failure")
-        
+
         with pytest.raises(NetworkError, match="persistent failure"):
             test_function()
         assert call_count == 2
-    
+
     @pytest.mark.unit
     def test_non_recoverable_error_no_retry(self):
         call_count = 0
-        
+
         @retry_with_backoff(max_attempts=3, base_delay=0.01)
         def test_function():
             nonlocal call_count
             call_count += 1
             raise ValueError("non-recoverable error")
-        
+
         with pytest.raises(ValueError, match="non-recoverable error"):
             test_function()
         assert call_count == 1
-    
+
     @pytest.mark.unit
     def test_retry_with_custom_config(self):
         config = RetryConfig(max_attempts=5, base_delay=0.01)
         call_count = 0
-        
+
         @retry_with_backoff(config=config)
         def test_function():
             nonlocal call_count
@@ -263,7 +266,7 @@
             if call_count < 4:
                 raise NetworkError("temporary failure")
             return "success"
-        
+
         result = test_function()
         assert result == "success"
         assert call_count == 4
@@ -271,11 +274,11 @@
 
 class TestRetryOnExceptions:
     """Test retry_on_exceptions decorator."""
-    
+
     @pytest.mark.unit
     def test_retry_on_specific_exception(self):
         call_count = 0
-        
+
         @retry_on_exceptions(ValueError, max_attempts=3, delay=0.01)
         def test_function():
             nonlocal call_count
@@ -283,29 +286,29 @@
             if call_count < 3:
                 raise ValueError("retryable error")
             return "success"
-        
+
         result = test_function()
         assert result == "success"
         assert call_count == 3
-    
+
     @pytest.mark.unit
     def test_no_retry_on_different_exception(self):
         call_count = 0
-        
+
         @retry_on_exceptions(ValueError, max_attempts=3, delay=0.01)
         def test_function():
             nonlocal call_count
             call_count += 1
             raise TypeError("different error")
-        
+
         with pytest.raises(TypeError, match="different error"):
             test_function()
         assert call_count == 1
-    
+
     @pytest.mark.unit
     def test_retry_on_multiple_exceptions(self):
         call_count = 0
-        
+
         @retry_on_exceptions([ValueError, TypeError], max_attempts=3, delay=0.01)
         def test_function():
             nonlocal call_count
@@ -315,7 +318,7 @@
             elif call_count == 2:
                 raise TypeError("second error")
             return "success"
-        
+
         result = test_function()
         assert result == "success"
         assert call_count == 3
@@ -323,45 +326,45 @@
 
 class TestRetryableOperation:
     """Test RetryableOperation context manager."""
-    
+
     @pytest.mark.unit
     def test_successful_operation(self):
         operation = RetryableOperation("test_operation")
-        
+
         with operation:
             pass  # Successful operation
-        
+
         assert operation.attempt == 1
-    
+
     @pytest.mark.unit
     def test_failed_operation(self):
         operation = RetryableOperation("test_operation")
-        
+
         with pytest.raises(ValueError):
             with operation:
                 raise ValueError("operation failed")
-        
+
         assert operation.attempt == 1
-    
+
     @pytest.mark.unit
     def test_should_retry_decision(self):
         config = RetryConfig(max_attempts=3)
         operation = RetryableOperation("test_operation", config=config)
-        
+
         # Simulate first attempt
         operation.attempt = 1
         assert operation.should_retry(NetworkError("recoverable")) is True
         assert operation.should_retry(ValueError("non-recoverable")) is False
-        
+
         # Simulate max attempts reached
         operation.attempt = 3
         assert operation.should_retry(NetworkError("recoverable")) is False
-    
+
     @pytest.mark.unit
     def test_get_retry_delay(self):
         config = RetryConfig(base_delay=1.0, jitter=False)
         operation = RetryableOperation("test_operation", config=config)
         operation.attempt = 1
-        
+
         delay = operation.get_retry_delay(NetworkError("test"))
-        assert delay == 1.0
\ No newline at end of file
+        assert delay == 1.0
--- original/./tests/unit/test_mapping.py
+++ fixed/./tests/unit/test_mapping.py
@@ -16,7 +16,7 @@
 
 class TestOutputMapping:
     """Test OutputMapping dataclass."""
-    
+
     @pytest.mark.unit
     def test_valid_mapping(self):
         mapping = OutputMapping(
@@ -25,14 +25,14 @@
             sde_dataset="Underlag_MSB",
             description="Test mapping"
         )
-        
+
         assert mapping.staging_fc == "MSB_Stamnat"
         assert mapping.sde_fc == "Stamnat_sodermanland"
         assert mapping.sde_dataset == "Underlag_MSB"
         assert mapping.description == "Test mapping"
         assert mapping.enabled is True
         assert mapping.schema is None
-    
+
     @pytest.mark.unit
     def test_empty_staging_fc(self):
         with pytest.raises(ValidationError, match="staging_fc cannot be empty"):
@@ -41,7 +41,7 @@
                 sde_fc="test_fc",
                 sde_dataset="test_dataset"
             )
-    
+
     @pytest.mark.unit
     def test_empty_sde_fc(self):
         with pytest.raises(ValidationError, match="sde_fc cannot be empty"):
@@ -50,7 +50,7 @@
                 sde_fc="",
                 sde_dataset="test_dataset"
             )
-    
+
     @pytest.mark.unit
     def test_empty_sde_dataset(self):
         with pytest.raises(ValidationError, match="sde_dataset cannot be empty"):
@@ -63,11 +63,11 @@
 
 class TestMappingSettings:
     """Test MappingSettings dataclass."""
-    
+
     @pytest.mark.unit
     def test_default_settings(self):
         settings = MappingSettings()
-        
+
         assert settings.default_schema == "GNG"
         assert settings.default_dataset_pattern == "Underlag_{authority}"
         assert settings.default_fc_pattern == "{authority}_{source_name}"
@@ -78,7 +78,7 @@
 
 class TestMappingManager:
     """Test MappingManager functionality."""
-    
+
     @pytest.fixture
     def temp_mappings_file(self):
         """Create temporary mappings file."""
@@ -113,80 +113,80 @@
                 }
             ]
         }
-        
+
         temp_file = Path(tempfile.mktemp(suffix='.yaml'))
         with temp_file.open('w') as f:
             yaml.dump(mappings_content, f)
-        
+
         yield temp_file
-        
+
         # Cleanup
         if temp_file.exists():
             temp_file.unlink()
-    
+
     @pytest.mark.unit
     def test_initialization_without_file(self):
         manager = MappingManager()
         assert len(manager.mappings) == 0
         assert isinstance(manager.settings, MappingSettings)
-    
+
     @pytest.mark.unit
     def test_load_mappings_from_file(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         assert len(manager.mappings) == 3
         assert 'MSB_Stamnat' in manager.mappings
         assert 'NVV_Naturreservat' in manager.mappings
         assert 'DISABLED_Source' in manager.mappings
-        
+
         msb_mapping = manager.mappings['MSB_Stamnat']
         assert msb_mapping.sde_fc == 'Stamnat_sodermanland'
         assert msb_mapping.sde_dataset == 'Underlag_MSB'
         assert msb_mapping.enabled is True
-    
+
     @pytest.mark.unit
     def test_load_nonexistent_file(self):
         nonexistent_file = Path("nonexistent_mappings.yaml")
         manager = MappingManager(nonexistent_file)
-        
+
         # Should not raise error, just log warning
         assert len(manager.mappings) == 0
-    
+
     @pytest.mark.unit
     def test_get_explicit_mapping(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
         source = Source(name="Test Source", authority="MSB")
-        
+
         mapping = manager.get_output_mapping(source, "MSB_Stamnat")
-        
+
         assert mapping.staging_fc == "MSB_Stamnat"
         assert mapping.sde_fc == "Stamnat_sodermanland"
         assert mapping.sde_dataset == "Underlag_MSB"
-    
+
     @pytest.mark.unit
     def test_get_disabled_mapping_fallback(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
         source = Source(name="Disabled Source", authority="TEST")
-        
+
         # Should fall back to default logic for disabled mapping
         mapping = manager.get_output_mapping(source, "DISABLED_Source")
-        
+
         assert mapping.staging_fc == "DISABLED_Source"
         assert mapping.sde_dataset == "Underlag_TEST"  # Default pattern
         assert mapping.description.startswith("Auto-generated")
-    
+
     @pytest.mark.unit
     def test_create_default_mapping(self):
         manager = MappingManager()
         source = Source(name="Test Source", authority="TEST")
-        
+
         mapping = manager._create_default_mapping(source, "TEST_Source")
-        
+
         assert mapping.staging_fc == "TEST_Source"
         assert mapping.sde_dataset == "Underlag_TEST"
         assert mapping.sde_fc == "TEST_test_source"
         assert mapping.schema == "GNG"
-    
+
     @pytest.mark.unit
     def test_get_full_sde_path(self):
         manager = MappingManager()
@@ -196,10 +196,10 @@
             sde_dataset="test_dataset",
             schema="GNG"
         )
-        
+
         path = manager.get_full_sde_path(mapping, "connection.sde")
         assert path == "connection.sde\\GNG.test_dataset\\output_fc"
-    
+
     @pytest.mark.unit
     def test_get_full_sde_path_no_schema(self):
         manager = MappingManager()
@@ -208,10 +208,10 @@
             sde_fc="output_fc",
             sde_dataset="test_dataset"
         )
-        
+
         path = manager.get_full_sde_path(mapping, "connection.sde")
         assert path == "connection.sde\\test_dataset\\output_fc"
-    
+
     @pytest.mark.unit
     def test_get_dataset_path(self):
         manager = MappingManager()
@@ -221,30 +221,30 @@
             sde_dataset="test_dataset",
             schema="GNG"
         )
-        
+
         path = manager.get_dataset_path(mapping, "connection.sde")
         assert path == "connection.sde\\GNG.test_dataset"
-    
+
     @pytest.mark.unit
     def test_get_mappings_for_dataset(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         mappings = manager.get_mappings_for_dataset("Underlag_MSB")
         assert len(mappings) == 1
         assert mappings[0].staging_fc == "MSB_Stamnat"
-    
+
     @pytest.mark.unit
     def test_get_all_target_datasets(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         datasets = manager.get_all_target_datasets()
-        
+
         # Should include enabled mappings only
         assert "GNG.Underlag_MSB" in datasets
         assert "GNG.Underlag_Naturvard" in datasets
         # Disabled mapping should not be included
         assert "GNG.Test_Dataset" not in datasets
-    
+
     @pytest.mark.unit
     def test_validate_mapping_valid(self):
         manager = MappingManager()
@@ -253,10 +253,10 @@
             sde_fc="Valid_Output",
             sde_dataset="Valid_Dataset"
         )
-        
+
         issues = manager.validate_mapping(mapping)
         assert len(issues) == 0
-    
+
     @pytest.mark.unit
     def test_validate_mapping_invalid_characters(self):
         manager = MappingManager()
@@ -265,26 +265,26 @@
             sde_fc="Invalid#Output",
             sde_dataset="Invalid$Dataset"
         )
-        
+
         issues = manager.validate_mapping(mapping)
         assert len(issues) == 3
         assert any("invalid characters" in issue for issue in issues)
-    
+
     @pytest.mark.unit
     def test_validate_mapping_too_long(self):
         manager = MappingManager()
         long_name = "a" * 150  # Exceeds 128 character limit
-        
+
         mapping = OutputMapping(
             staging_fc="Valid_FC",
             sde_fc=long_name,
             sde_dataset=long_name
         )
-        
+
         issues = manager.validate_mapping(mapping)
         assert len(issues) == 2
         assert any("exceeds 128 character limit" in issue for issue in issues)
-    
+
     @pytest.mark.unit
     def test_add_mapping(self):
         manager = MappingManager()
@@ -293,12 +293,12 @@
             sde_fc="New_Output",
             sde_dataset="New_Dataset"
         )
-        
+
         manager.add_mapping(mapping)
-        
+
         assert "New_FC" in manager.mappings
         assert manager.mappings["New_FC"] == mapping
-    
+
     @pytest.mark.unit
     def test_add_invalid_mapping(self):
         manager = MappingManager()
@@ -307,40 +307,40 @@
             sde_fc="Valid_Output",
             sde_dataset="Valid_Dataset"
         )
-        
+
         with pytest.raises(ValidationError):
             manager.add_mapping(mapping)
-    
+
     @pytest.mark.unit
     def test_remove_mapping(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         assert "MSB_Stamnat" in manager.mappings
-        
+
         result = manager.remove_mapping("MSB_Stamnat")
         assert result is True
         assert "MSB_Stamnat" not in manager.mappings
-        
+
         # Try removing non-existent mapping
         result = manager.remove_mapping("NonExistent")
         assert result is False
-    
+
     @pytest.mark.unit
     def test_get_mapping_statistics(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         stats = manager.get_mapping_statistics()
-        
+
         assert stats['total_mappings'] == 3
         assert stats['enabled_mappings'] == 2  # One is disabled
         assert stats['disabled_mappings'] == 1
         assert 'Underlag_MSB' in stats['datasets']
         assert 'Underlag_Naturvard' in stats['datasets']
-    
+
     @pytest.mark.unit
     def test_save_mappings(self, temp_mappings_file):
         manager = MappingManager(temp_mappings_file)
-        
+
         # Add a new mapping
         new_mapping = OutputMapping(
             staging_fc="Test_FC",
@@ -349,31 +349,31 @@
             description="Test mapping"
         )
         manager.add_mapping(new_mapping)
-        
+
         # Save to new file
         output_file = Path(tempfile.mktemp(suffix='.yaml'))
         try:
             manager.save_mappings(output_file)
-            
+
             # Verify file was created and contains expected data
             assert output_file.exists()
-            
+
             with output_file.open('r') as f:
                 saved_data = yaml.safe_load(f)
-            
+
             assert 'settings' in saved_data
             assert 'mappings' in saved_data
             assert len(saved_data['mappings']) == 4  # 3 original + 1 new
-            
+
         finally:
             if output_file.exists():
                 output_file.unlink()
-    
+
     @pytest.mark.unit
     def test_load_invalid_yaml(self):
         invalid_file = Path(tempfile.mktemp(suffix='.yaml'))
         invalid_file.write_text("invalid: yaml: content: [")
-        
+
         try:
             with pytest.raises(ConfigurationError, match="Invalid YAML"):
                 MappingManager(invalid_file)
@@ -384,25 +384,25 @@
 
 class TestGlobalMappingManager:
     """Test global mapping manager functions."""
-    
+
     @pytest.mark.unit
     def test_get_mapping_manager_singleton(self):
         # Clear global instance
         import etl.mapping
         etl.mapping._mapping_manager = None
-        
+
         manager1 = get_mapping_manager()
         manager2 = get_mapping_manager()
-        
+
         assert manager1 is manager2  # Should be same instance
-    
+
     @pytest.mark.unit
     def test_get_mapping_manager_with_file(self, temp_mappings_file):
         # Clear global instance
         import etl.mapping
         etl.mapping._mapping_manager = None
-        
+
         manager = get_mapping_manager(temp_mappings_file)
-        
+
         assert len(manager.mappings) > 0
-        assert manager.mappings_file == temp_mappings_file
\ No newline at end of file
+        assert manager.mappings_file == temp_mappings_file
--- original/./tests/unit/test_config.py
+++ fixed/./tests/unit/test_config.py
@@ -22,18 +22,18 @@
 
 class TestLoggingConfig:
     """Test LoggingConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_logging_config(self):
         config = LoggingConfig(level="INFO", console_level="WARNING")
         assert config.level == "INFO"
         assert config.console_level == "WARNING"
-    
+
     @pytest.mark.unit
     def test_invalid_log_level(self):
         with pytest.raises(ValidationError, match="Invalid log level"):
             LoggingConfig(level="INVALID")
-    
+
     @pytest.mark.unit
     def test_invalid_console_level(self):
         with pytest.raises(ValidationError, match="Invalid console log level"):
@@ -42,28 +42,28 @@
 
 class TestRetryConfig:
     """Test RetryConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_retry_config(self):
         config = RetryConfig(max_attempts=5, base_delay=2.0)
         assert config.max_attempts == 5
         assert config.base_delay == 2.0
-    
+
     @pytest.mark.unit
     def test_invalid_max_attempts(self):
         with pytest.raises(ValidationError, match="max_attempts must be at least 1"):
             RetryConfig(max_attempts=0)
-    
+
     @pytest.mark.unit
     def test_invalid_base_delay(self):
         with pytest.raises(ValidationError, match="base_delay must be non-negative"):
             RetryConfig(base_delay=-1.0)
-    
+
     @pytest.mark.unit
     def test_invalid_backoff_factor(self):
         with pytest.raises(ValidationError, match="backoff_factor must be at least 1"):
             RetryConfig(backoff_factor=0.5)
-    
+
     @pytest.mark.unit
     def test_invalid_timeout(self):
         with pytest.raises(ValidationError, match="timeout must be at least 1 second"):
@@ -72,7 +72,7 @@
 
 class TestPathsConfig:
     """Test PathsConfig validation and normalization."""
-    
+
     @pytest.mark.unit
     def test_paths_normalization(self):
         config = PathsConfig(download="./downloads", staging="../staging")
@@ -83,23 +83,23 @@
 
 class TestProcessingConfig:
     """Test ProcessingConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_processing_config(self):
         config = ProcessingConfig(chunk_size=500, parallel_workers=4)
         assert config.chunk_size == 500
         assert config.parallel_workers == 4
-    
+
     @pytest.mark.unit
     def test_invalid_chunk_size(self):
         with pytest.raises(ValidationError, match="chunk_size must be at least 1"):
             ProcessingConfig(chunk_size=0)
-    
+
     @pytest.mark.unit
     def test_invalid_parallel_workers(self):
         with pytest.raises(ValidationError, match="parallel_workers must be at least 1"):
             ProcessingConfig(parallel_workers=0)
-    
+
     @pytest.mark.unit
     def test_invalid_memory_limit(self):
         with pytest.raises(ValidationError, match="memory_limit_mb must be at least 128MB"):
@@ -108,13 +108,13 @@
 
 class TestValidationConfig:
     """Test ValidationConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_validation_config(self):
         config = ValidationConfig(strict_mode=True, max_validation_errors=50)
         assert config.strict_mode is True
         assert config.max_validation_errors == 50
-    
+
     @pytest.mark.unit
     def test_invalid_max_validation_errors(self):
         with pytest.raises(ValidationError, match="max_validation_errors must be at least 1"):
@@ -123,7 +123,7 @@
 
 class TestSecurityConfig:
     """Test SecurityConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_security_config(self):
         config = SecurityConfig(
@@ -134,7 +134,7 @@
         assert config.enable_ssl_verification is True
         assert "example.com" in config.trusted_hosts
         assert config.max_file_size_mb == 512
-    
+
     @pytest.mark.unit
     def test_invalid_max_file_size(self):
         with pytest.raises(ValidationError, match="max_file_size_mb must be at least 1MB"):
@@ -143,18 +143,18 @@
 
 class TestDatabaseConfig:
     """Test DatabaseConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_database_config(self):
         config = DatabaseConfig(pool_size=10, max_overflow=5)
         assert config.pool_size == 10
         assert config.max_overflow == 5
-    
+
     @pytest.mark.unit
     def test_invalid_pool_size(self):
         with pytest.raises(ValidationError, match="pool_size must be at least 1"):
             DatabaseConfig(pool_size=0)
-    
+
     @pytest.mark.unit
     def test_invalid_max_overflow(self):
         with pytest.raises(ValidationError, match="max_overflow must be non-negative"):
@@ -163,13 +163,13 @@
 
 class TestGlobalConfig:
     """Test GlobalConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_global_config(self):
         config = GlobalConfig(environment="production", debug=False)
         assert config.environment == "production"
         assert config.debug is False
-    
+
     @pytest.mark.unit
     def test_invalid_environment(self):
         with pytest.raises(ValidationError, match="Invalid environment"):
@@ -178,7 +178,7 @@
 
 class TestSourceConfig:
     """Test SourceConfig validation."""
-    
+
     @pytest.mark.unit
     def test_valid_source_config(self):
         config = SourceConfig(
@@ -192,37 +192,37 @@
         assert config.authority == "TEST"
         assert config.type == "rest_api"
         assert config.enabled is True
-    
+
     @pytest.mark.unit
     def test_empty_name(self):
         with pytest.raises(ValidationError, match="Source name cannot be empty"):
             SourceConfig(name="", authority="TEST")
-    
+
     @pytest.mark.unit
     def test_empty_authority(self):
         with pytest.raises(ValidationError, match="Source authority cannot be empty"):
             SourceConfig(name="Test", authority="")
-    
+
     @pytest.mark.unit
     def test_invalid_type(self):
         with pytest.raises(ValidationError, match="Invalid source type"):
             SourceConfig(name="Test", authority="TEST", type="invalid_type")
-    
+
     @pytest.mark.unit
     def test_enabled_source_without_url(self):
         with pytest.raises(ValidationError, match="Enabled source .* must have a URL"):
             SourceConfig(name="Test", authority="TEST", enabled=True, url="")
-    
+
     @pytest.mark.unit
     def test_invalid_priority(self):
         with pytest.raises(ValidationError, match="Source priority must be between 1 and 100"):
             SourceConfig(name="Test", authority="TEST", priority=0)
-    
+
     @pytest.mark.unit
     def test_invalid_timeout(self):
         with pytest.raises(ValidationError, match="Source timeout must be at least 1 second"):
             SourceConfig(name="Test", authority="TEST", timeout=0)
-    
+
     @pytest.mark.unit
     def test_invalid_retry_attempts(self):
         with pytest.raises(ValidationError, match="Source retry_attempts must be non-negative"):
@@ -231,16 +231,16 @@
 
 class TestConfigManager:
     """Test ConfigManager functionality."""
-    
+
     @pytest.fixture
     def temp_config_dir(self):
         """Create temporary directory with config files."""
         temp_dir = Path(tempfile.mkdtemp())
-        
+
         # Create config directory structure
         config_dir = temp_dir / "config"
         config_dir.mkdir()
-        
+
         # Create basic config file
         config_file = config_dir / "config.yaml"
         config_content = """
@@ -251,7 +251,7 @@
   max_attempts: 2
 """
         config_file.write_text(config_content)
-        
+
         # Create sources file
         sources_file = config_dir / "sources.yaml"
         sources_content = """
@@ -263,96 +263,96 @@
     enabled: true
 """
         sources_file.write_text(sources_content)
-        
+
         yield temp_dir
-        
+
         # Cleanup
         import shutil
         shutil.rmtree(temp_dir, ignore_errors=True)
-    
+
     @pytest.mark.unit
     def test_config_manager_initialization(self):
         manager = ConfigManager(environment="development")
         assert manager.environment == "development"
-    
+
     @pytest.mark.unit
     def test_environment_from_env_var(self):
         with patch.dict(os.environ, {"ETL_ENVIRONMENT": "production"}):
             manager = ConfigManager()
             assert manager.environment == "production"
-    
+
     @pytest.mark.unit
     def test_load_global_config(self, temp_config_dir):
         config_file = temp_config_dir / "config" / "config.yaml"
         manager = ConfigManager()
-        
+
         config = manager.load_global_config(config_file)
         assert isinstance(config, GlobalConfig)
         assert config.environment == "development"
         assert config.logging.level == "DEBUG"
         assert config.retry.max_attempts == 2
-    
+
     @pytest.mark.unit
     def test_load_sources_config(self, temp_config_dir):
         sources_file = temp_config_dir / "config" / "sources.yaml"
         manager = ConfigManager()
-        
+
         sources = manager.load_sources_config(sources_file)
         assert len(sources) == 1
         assert isinstance(sources[0], SourceConfig)
         assert sources[0].name == "Test Source"
         assert sources[0].authority == "TEST"
-    
+
     @pytest.mark.unit
     def test_load_nonexistent_config(self):
         manager = ConfigManager()
-        
+
         with pytest.raises(ConfigurationError, match="Configuration file .* not found"):
             manager.load_global_config(Path("nonexistent.yaml"))
-    
+
     @pytest.mark.unit
     def test_load_invalid_yaml(self, temp_config_dir):
         invalid_file = temp_config_dir / "invalid.yaml"
         invalid_file.write_text("invalid: yaml: content: [")
-        
-        manager = ConfigManager()
-        
+
+        manager = ConfigManager()
+
         with pytest.raises(ConfigurationError, match="Invalid YAML"):
             manager.load_global_config(invalid_file)
-    
+
     @pytest.mark.unit
     def test_sources_missing_sources_key(self, temp_config_dir):
         invalid_sources = temp_config_dir / "invalid_sources.yaml"
         invalid_sources.write_text("other_key: value")
-        
-        manager = ConfigManager()
-        
+
+        manager = ConfigManager()
+
         with pytest.raises(ConfigurationError, match="missing 'sources' key"):
             manager.load_sources_config(invalid_sources)
-    
+
     @pytest.mark.unit
     def test_validate_configuration_warnings(self, temp_config_dir):
         config_file = temp_config_dir / "config" / "config.yaml"
         manager = ConfigManager()
-        
+
         config = manager.load_global_config(config_file)
         warnings = manager.validate_configuration(config)
-        
+
         # Should return a list (may be empty)
         assert isinstance(warnings, list)
-    
+
     @pytest.mark.unit
     def test_environment_variable_overrides(self, temp_config_dir):
         config_file = temp_config_dir / "config" / "config.yaml"
         manager = ConfigManager()
-        
+
         with patch.dict(os.environ, {
             "ETL_LOG_LEVEL": "ERROR",
             "ETL_DEBUG": "true",
             "ETL_MAX_WORKERS": "8"
         }):
             config = manager.load_global_config(config_file)
-            
+
             assert config.logging.level == "ERROR"
             assert config.debug is True
-            assert config.processing.parallel_workers == 8
\ No newline at end of file
+            assert config.processing.parallel_workers == 8
--- original/./tests/unit/test_utils_naming.py
+++ fixed/./tests/unit/test_utils_naming.py
@@ -10,15 +10,15 @@
 
 class TestSanitizeForFilename:
     """Test sanitize_for_filename function."""
-    
+
     @pytest.mark.unit
     def test_basic_filename_sanitization(self):
         assert sanitize_for_filename("Hello World") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_filename_with_swedish_chars(self):
         assert sanitize_for_filename("√Öland √Ñpplen") == "aland_applen"
-    
+
     @pytest.mark.unit
     def test_filename_with_special_chars(self):
         assert sanitize_for_filename("Test@#$Data") == "test_data"
@@ -26,98 +26,105 @@
 
 class TestSanitizeForArcgisName:
     """Test sanitize_for_arcgis_name function."""
-    
+
     @pytest.mark.unit
     def test_basic_arcgis_sanitization(self):
         assert sanitize_for_arcgis_name("Hello World") == "Hello_World"
-    
+
     @pytest.mark.unit
     def test_arcgis_hyphens_to_underscores(self):
-        assert sanitize_for_arcgis_name("Hello-World-Test") == "Hello_World_Test"
-    
+        assert sanitize_for_arcgis_name(
+            "Hello-World-Test") == "Hello_World_Test"
+
     @pytest.mark.unit
     def test_arcgis_swedish_characters(self):
         assert sanitize_for_arcgis_name("√Öland √Ñpplen") == "aland_applen"
-    
+
     @pytest.mark.unit
     def test_arcgis_starts_with_digit(self):
         assert sanitize_for_arcgis_name("123Test") == "_123test"
-    
+
     @pytest.mark.unit
     def test_arcgis_consecutive_underscores(self):
         assert sanitize_for_arcgis_name("Hello___World") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_arcgis_leading_trailing_underscores(self):
         assert sanitize_for_arcgis_name("_Hello_World_") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_arcgis_special_characters_removed(self):
         assert sanitize_for_arcgis_name("Hello@#$World!") == "hello_world"
-    
+
     @pytest.mark.unit
     def test_arcgis_empty_string(self):
         assert sanitize_for_arcgis_name("") == "unnamed"
-    
+
     @pytest.mark.unit
     def test_arcgis_max_length_truncation(self):
         long_name = "a" * 150  # Longer than 128 chars
         result = sanitize_for_arcgis_name(long_name)
         assert len(result) <= 128
         assert result == "a" * 128
-    
+
     @pytest.mark.unit
     def test_arcgis_real_world_examples(self):
-        assert sanitize_for_arcgis_name("Naturv√•rdsverket") == "naturvardsverket"
-        assert sanitize_for_arcgis_name("F√∂rsvarsmakten - Geodata") == "forsvarsmakten_geodata"
-        assert sanitize_for_arcgis_name("SGU-Berggrund 1:50 000") == "sgu_berggrund_1_50_000"
+        assert sanitize_for_arcgis_name(
+            "Naturv√•rdsverket") == "naturvardsverket"
+        assert sanitize_for_arcgis_name(
+            "F√∂rsvarsmakten - Geodata") == "forsvarsmakten_geodata"
+        assert sanitize_for_arcgis_name(
+            "SGU-Berggrund 1:50 000") == "sgu_berggrund_1_50_000"
 
 
 class TestGenerateFcName:
     """Test generate_fc_name function."""
-    
+
     @pytest.mark.unit
     def test_basic_fc_name_generation(self):
         result = generate_fc_name("TEST", "Sample Data")
         assert result == "TEST_sample_data"
-    
+
     @pytest.mark.unit
     def test_fc_name_with_swedish_chars(self):
         result = generate_fc_name("NVV", "Naturv√•rdsomr√•den")
         assert result == "NVV_naturvardsomraden"
-    
+
     @pytest.mark.unit
     def test_fc_name_with_special_chars(self):
         result = generate_fc_name("SGU", "Berggrund 1:50 000")
         assert result == "SGU_berggrund_1_50_000"
-    
+
     @pytest.mark.unit
     def test_fc_name_max_length(self):
         long_source = "Very Long Source Name That Exceeds Normal Limits"
         result = generate_fc_name("AUTHORITY", long_source)
         assert len(result) <= 128
         assert not result.endswith("_")
-    
+
     @pytest.mark.unit
     def test_fc_name_empty_source(self):
         result = generate_fc_name("TEST", "")
         assert result == "TEST_unnamed"
-    
+
     @pytest.mark.unit
     def test_fc_name_strips_trailing_underscore(self):
         # Test that trailing underscores are properly stripped
         result = generate_fc_name("TEST", "Source___")
         assert not result.endswith("_")
         assert result == "TEST_source"
-    
+
     @pytest.mark.unit
     def test_fc_name_real_world_examples(self):
         # Test with actual authority/source combinations
-        assert generate_fc_name("FM", "Rikst√§ckande geodata") == "FM_rikstackande_geodata"
-        assert generate_fc_name("NVV", "Naturv√•rdsregistret") == "NVV_naturvardsregistret"
-        assert generate_fc_name("SGU", "Berggrundskarta 1:50 000") == "SGU_berggrundskarta_1_50_000"
-    
+        assert generate_fc_name(
+            "FM", "Rikst√§ckande geodata") == "FM_rikstackande_geodata"
+        assert generate_fc_name(
+            "NVV", "Naturv√•rdsregistret") == "NVV_naturvardsregistret"
+        assert generate_fc_name(
+            "SGU", "Berggrundskarta 1:50 000") == "SGU_berggrundskarta_1_50_000"
+
     @pytest.mark.unit
     def test_fc_name_authority_with_numbers(self):
         result = generate_fc_name("AUTH123", "Test Data")
-        assert result == "AUTH123_test_data"
\ No newline at end of file
+        assert result == "AUTH123_test_data"
--- original/./tests/integration/__init__.py
+++ fixed/./tests/integration/__init__.py
@@ -1 +1 @@
-# Integration tests package
\ No newline at end of file
+# Integration tests package
--- original/./tests/integration/test_handlers_file.py
+++ fixed/./tests/integration/test_handlers_file.py
@@ -11,7 +11,7 @@
 
 class TestFileDownloadHandler:
     """Test FileDownloadHandler with mocked external dependencies."""
-    
+
     @pytest.fixture
     def sample_file_source(self):
         """Create a sample file source for testing."""
@@ -23,7 +23,7 @@
             enabled=True,
             download_format="zip"
         )
-    
+
     @pytest.fixture
     def sample_gpkg_source(self):
         """Create a sample GPKG file source for testing."""
@@ -35,12 +35,12 @@
             enabled=True,
             download_format="gpkg"
         )
-    
+
     @pytest.fixture
     def sample_multi_file_source(self):
         """Create a sample source with multiple files to download."""
         return Source(
-            name="Test Multi File Source", 
+            name="Test Multi File Source",
             authority="TEST",
             type="file",
             url="https://example.com/base/",
@@ -48,7 +48,7 @@
             download_format="zip",
             include=["file1", "file2", "file3"]
         )
-    
+
     @pytest.fixture
     def sample_global_config(self):
         """Sample global configuration."""
@@ -59,17 +59,17 @@
                 "staging": "test_staging"
             }
         }
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_handler_initialization(self, mock_ensure_dirs, sample_file_source, sample_global_config):
         """Test handler initialization."""
         handler = FileDownloadHandler(sample_file_source, sample_global_config)
-        
+
         assert handler.src == sample_file_source
         assert handler.global_config == sample_global_config
         mock_ensure_dirs.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_fetch_disabled_source(self, mock_ensure_dirs, sample_global_config):
@@ -81,53 +81,54 @@
             url="https://example.com/data.zip",
             enabled=False
         )
-        
+
         handler = FileDownloadHandler(disabled_source, sample_global_config)
-        
+
         # Should return early without attempting download
         handler.fetch()  # Should not raise any exception
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     @patch('etl.handlers.file.download')
     @patch('etl.handlers.file.extract_zip')
     def test_fetch_single_zip_file(self, mock_extract_zip, mock_download, mock_ensure_dirs,
-                                  sample_file_source, sample_global_config, temp_dir):
+                                   sample_file_source, sample_global_config, temp_dir):
         """Test fetching a single ZIP file."""
         # Mock download to return a fake zip file path
         mock_zip_path = temp_dir / "downloaded.zip"
         mock_zip_path.touch()
         mock_download.return_value = mock_zip_path
-        
-        handler = FileDownloadHandler(sample_file_source, sample_global_config)
-        
+
+        handler = FileDownloadHandler(sample_file_source, sample_global_config)
+
         # Mock the private method that would be called
         with patch.object(handler, '_download_single_resource') as mock_single:
             handler.fetch()
             mock_single.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_fetch_gpkg_source_detection(self, mock_ensure_dirs, sample_gpkg_source, sample_global_config):
         """Test detection of GPKG sources."""
         handler = FileDownloadHandler(sample_gpkg_source, sample_global_config)
-        
+
         # Mock the private method to verify correct path is taken
         with patch.object(handler, '_download_single_resource') as mock_single:
             handler.fetch()
             mock_single.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_fetch_multi_file_source(self, mock_ensure_dirs, sample_multi_file_source, sample_global_config):
         """Test fetching multiple files from include list."""
-        handler = FileDownloadHandler(sample_multi_file_source, sample_global_config)
-        
+        handler = FileDownloadHandler(
+            sample_multi_file_source, sample_global_config)
+
         # Mock the private method to verify correct path is taken
         with patch.object(handler, '_download_multiple_files') as mock_multiple:
             handler.fetch()
             mock_multiple.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_fetch_source_with_no_url(self, mock_ensure_dirs, sample_global_config):
@@ -139,72 +140,73 @@
             url="",
             enabled=True
         )
-        
+
         handler = FileDownloadHandler(no_url_source, sample_global_config)
-        
+
         # Should handle gracefully without crashing
         handler.fetch()  # Should not raise exception
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     @patch('etl.handlers.file.fetch_true_filename_parts')
-    def test_filename_detection(self, mock_fetch_filename, mock_ensure_dirs, 
-                              sample_file_source, sample_global_config):
+    def test_filename_detection(self, mock_fetch_filename, mock_ensure_dirs,
+                                sample_file_source, sample_global_config):
         """Test filename detection from HTTP headers."""
         mock_fetch_filename.return_value = ("test_data", ".zip")
-        
-        handler = FileDownloadHandler(sample_file_source, sample_global_config)
-        
+
+        handler = FileDownloadHandler(sample_file_source, sample_global_config)
+
         # This would be called during download process
         # Just verify the mocking works correctly
-        assert mock_fetch_filename("https://example.com/data.zip") == ("test_data", ".zip")
-    
+        assert mock_fetch_filename(
+            "https://example.com/data.zip") == ("test_data", ".zip")
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     @patch('etl.handlers.file.sanitize_for_filename')
     def test_filename_sanitization(self, mock_sanitize, mock_ensure_dirs,
-                                 sample_file_source, sample_global_config):
+                                   sample_file_source, sample_global_config):
         """Test that filenames are properly sanitized."""
         mock_sanitize.return_value = "sanitized_filename"
-        
-        handler = FileDownloadHandler(sample_file_source, sample_global_config)
-        
+
+        handler = FileDownloadHandler(sample_file_source, sample_global_config)
+
         # Verify sanitization function is available
         from etl.utils.naming import sanitize_for_filename
         assert sanitize_for_filename("Test File Name") == "test_file_name"
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_handler_with_empty_global_config(self, mock_ensure_dirs, sample_file_source):
         """Test handler behavior with empty global config."""
         handler = FileDownloadHandler(sample_file_source, {})
-        
+
         assert handler.global_config == {}
         # Should not crash during initialization
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_handler_with_none_global_config(self, mock_ensure_dirs, sample_file_source):
         """Test handler behavior with None global config."""
         handler = FileDownloadHandler(sample_file_source, None)
-        
+
         assert handler.global_config == {}
         # Should handle None config gracefully
 
-    @pytest.mark.integration 
+    @pytest.mark.integration
     @patch('etl.handlers.file.ensure_dirs')
     def test_context_manager_support(self, mock_ensure_dirs, sample_file_source):
         """Test that FileDownloadHandler supports context manager protocol."""
         handler = FileDownloadHandler(sample_file_source)
-        
+
         # Test context manager methods exist
         assert hasattr(handler, '__enter__')
         assert hasattr(handler, '__exit__')
-        
+
         # Test context manager usage
         with FileDownloadHandler(sample_file_source) as context_handler:
             assert context_handler is not None
             assert isinstance(context_handler, FileDownloadHandler)
             assert hasattr(context_handler, 'fetch')
-        
-        # Should exit without errors
\ No newline at end of file
+
+        # Should exit without errors
--- original/./tests/integration/test_handlers_rest_api.py
+++ fixed/./tests/integration/test_handlers_rest_api.py
@@ -10,7 +10,7 @@
 
 class TestRestApiDownloadHandler:
     """Test RestApiDownloadHandler with mocked external dependencies."""
-    
+
     @pytest.fixture
     def sample_rest_api_source(self):
         """Create a sample REST API source for testing."""
@@ -21,7 +21,7 @@
             url="https://services.arcgis.com/test/arcgis/rest/services/TestService/MapServer/0/query",
             enabled=True
         )
-    
+
     @pytest.fixture
     def sample_global_config(self):
         """Sample global configuration."""
@@ -33,22 +33,23 @@
                 "staging": "test_staging"
             }
         }
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.ensure_dirs')
     def test_handler_initialization(self, mock_ensure_dirs, sample_rest_api_source, sample_global_config):
         """Test handler initialization."""
-        handler = RestApiDownloadHandler(sample_rest_api_source, sample_global_config)
-        
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, sample_global_config)
+
         assert handler.src == sample_rest_api_source
         assert handler.global_config == sample_global_config
         mock_ensure_dirs.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.rest_api.ensure_dirs')
-    def test_get_service_metadata_successful(self, mock_ensure_dirs, mock_requests, 
-                                           sample_rest_api_source, sample_global_config):
+    def test_get_service_metadata_successful(self, mock_ensure_dirs, mock_requests,
+                                             sample_rest_api_source, sample_global_config):
         """Test successful service metadata retrieval."""
         # Mock successful response
         mock_response = Mock()
@@ -60,73 +61,80 @@
             "type": "Feature Layer"
         }
         mock_requests.get.return_value = mock_response
-        
-        handler = RestApiDownloadHandler(sample_rest_api_source, sample_global_config)
+
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, sample_global_config)
         result = handler._get_service_metadata("https://test.com/MapServer")
-        
+
         assert result is not None
         assert result["name"] == "TestLayer"
         assert result["type"] == "Feature Layer"
         mock_requests.get.assert_called_once()
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.rest_api.ensure_dirs')
     def test_get_service_metadata_with_retries(self, mock_ensure_dirs, mock_requests,
-                                             sample_rest_api_source, sample_global_config):
+                                               sample_rest_api_source, sample_global_config):
         """Test service metadata retrieval with retries on failure."""
         # Mock first call to fail, second to succeed
         mock_response_fail = Mock()
         mock_response_fail.status_code = 500
-        mock_response_fail.raise_for_status.side_effect = Exception("Server Error")
-        
+        mock_response_fail.raise_for_status.side_effect = Exception(
+            "Server Error")
+
         mock_response_success = Mock()
         mock_response_success.status_code = 200
         mock_response_success.json.return_value = {"name": "TestLayer"}
-        
-        mock_requests.get.side_effect = [mock_response_fail, mock_response_success]
-        
-        handler = RestApiDownloadHandler(sample_rest_api_source, sample_global_config)
+
+        mock_requests.get.side_effect = [
+            mock_response_fail, mock_response_success]
+
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, sample_global_config)
         result = handler._get_service_metadata("https://test.com/MapServer")
-        
+
         assert result is not None
         assert result["name"] == "TestLayer"
         assert mock_requests.get.call_count == 2
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.rest_api.ensure_dirs')
     def test_get_service_metadata_max_retries_exceeded(self, mock_ensure_dirs, mock_requests,
-                                                     sample_rest_api_source, sample_global_config):
+                                                       sample_rest_api_source, sample_global_config):
         """Test service metadata retrieval when max retries exceeded."""
         # Mock all calls to fail
         mock_response = Mock()
         mock_response.status_code = 500
         mock_response.raise_for_status.side_effect = Exception("Server Error")
         mock_requests.get.return_value = mock_response
-        
-        handler = RestApiDownloadHandler(sample_rest_api_source, sample_global_config)
+
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, sample_global_config)
         result = handler._get_service_metadata("https://test.com/MapServer")
-        
+
         assert result is None
         assert mock_requests.get.call_count == 3  # max_retries from config
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.rest_api.ensure_dirs')
     def test_get_service_metadata_invalid_json(self, mock_ensure_dirs, mock_requests,
-                                             sample_rest_api_source, sample_global_config):
+                                               sample_rest_api_source, sample_global_config):
         """Test handling of invalid JSON response."""
         mock_response = Mock()
         mock_response.status_code = 200
-        mock_response.json.side_effect = json.JSONDecodeError("Invalid JSON", "", 0)
+        mock_response.json.side_effect = json.JSONDecodeError(
+            "Invalid JSON", "", 0)
         mock_requests.get.return_value = mock_response
-        
-        handler = RestApiDownloadHandler(sample_rest_api_source, sample_global_config)
+
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, sample_global_config)
         result = handler._get_service_metadata("https://test.com/MapServer")
-        
+
         assert result is None
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.ensure_dirs')
     def test_handler_with_disabled_source(self, mock_ensure_dirs):
@@ -134,15 +142,15 @@
         disabled_source = Source(
             name="Disabled Source",
             authority="TEST",
-            type="rest_api", 
+            type="rest_api",
             url="https://test.com",
             enabled=False
         )
-        
+
         handler = RestApiDownloadHandler(disabled_source)
         # This should not raise any exceptions
         assert handler.src.enabled is False
-    
+
     @pytest.mark.integration
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.rest_api.ensure_dirs')
@@ -152,17 +160,18 @@
             "max_retries": "invalid",  # Should be int
             "timeout": -1  # Should be positive
         }
-        
-        handler = RestApiDownloadHandler(sample_rest_api_source, invalid_config)
-        
+
+        handler = RestApiDownloadHandler(
+            sample_rest_api_source, invalid_config)
+
         # Handler should handle invalid config gracefully
         assert handler.global_config == invalid_config
-        
+
         # Test that metadata fetch still works with default retry logic
         mock_response = Mock()
         mock_response.status_code = 200
         mock_response.json.return_value = {"name": "TestLayer"}
         mock_requests.get.return_value = mock_response
-        
+
         result = handler._get_service_metadata("https://test.com/MapServer")
-        assert result is not None
\ No newline at end of file
+        assert result is not None
--- original/./tests/e2e/test_pipeline_integration.py
+++ fixed/./tests/e2e/test_pipeline_integration.py
@@ -13,7 +13,7 @@
 
 class TestPipelineIntegration:
     """End-to-end tests for the complete ETL pipeline."""
-    
+
     @pytest.fixture
     def test_pipeline_config(self, temp_dir):
         """Create test configuration files for pipeline testing."""
@@ -23,7 +23,7 @@
                 {
                     "name": "Test REST Source",
                     "authority": "TEST",
-                    "type": "rest_api", 
+                    "type": "rest_api",
                     "url": "https://api.example.com/features",
                     "enabled": True,
                     "output_format": "geojson"
@@ -38,7 +38,7 @@
                 }
             ]
         }
-        
+
         # Create global config
         global_config = {
             "logging": {
@@ -55,22 +55,22 @@
                 "staging": str(temp_dir / "staging")
             }
         }
-        
+
         sources_file = temp_dir / "test_sources.yaml"
         config_file = temp_dir / "test_config.yaml"
-        
+
         with sources_file.open("w") as f:
             yaml.dump(sources_config, f)
-        
+
         with config_file.open("w") as f:
             yaml.dump(global_config, f)
-        
+
         return {
             "sources_file": sources_file,
             "config_file": config_file,
             "temp_dir": temp_dir
         }
-    
+
     @pytest.mark.e2e
     def test_pipeline_initialization(self, test_pipeline_config):
         """Test pipeline initialization with config files."""
@@ -78,11 +78,11 @@
             sources_yaml=test_pipeline_config["sources_file"],
             config_yaml_path=test_pipeline_config["config_file"]
         )
-        
+
         assert pipeline.sources_yaml_path == test_pipeline_config["sources_file"]
         assert isinstance(pipeline.global_cfg, dict)
         assert isinstance(pipeline.summary, Summary)
-    
+
     @pytest.mark.e2e
     def test_pipeline_with_custom_summary(self, test_pipeline_config):
         """Test pipeline initialization with custom summary object."""
@@ -92,53 +92,54 @@
             config_yaml_path=test_pipeline_config["config_file"],
             summary=custom_summary
         )
-        
+
         assert pipeline.summary is custom_summary
-    
+
     @pytest.mark.e2e
     def test_pipeline_with_extra_handlers(self, test_pipeline_config):
         """Test pipeline with additional handler mappings."""
         extra_handlers = {
             "custom_handler": Mock()
         }
-        
+
         pipeline = Pipeline(
             sources_yaml=test_pipeline_config["sources_file"],
             config_yaml_path=test_pipeline_config["config_file"],
             extra_handler_map=extra_handlers
         )
-        
+
         assert "custom_handler" in pipeline.handler_map
-    
+
     @pytest.mark.e2e
     def test_pipeline_with_nonexistent_config(self, test_pipeline_config):
         """Test pipeline behavior with nonexistent config file."""
-        nonexistent_config = test_pipeline_config["temp_dir"] / "nonexistent.yaml"
-        
+        nonexistent_config = test_pipeline_config["temp_dir"] / \
+            "nonexistent.yaml"
+
         pipeline = Pipeline(
             sources_yaml=test_pipeline_config["sources_file"],
             config_yaml_path=nonexistent_config
         )
-        
+
         # Should use default empty config
         assert pipeline.global_cfg == {}
-    
+
     @pytest.mark.e2e
     @patch('etl.pipeline.Source.load_all')
     def test_pipeline_with_no_sources(self, mock_load_all, test_pipeline_config):
         """Test pipeline behavior when no sources are loaded."""
         mock_load_all.return_value = []
-        
-        pipeline = Pipeline(
-            sources_yaml=test_pipeline_config["sources_file"],
-            config_yaml_path=test_pipeline_config["config_file"]
-        )
-        
+
+        pipeline = Pipeline(
+            sources_yaml=test_pipeline_config["sources_file"],
+            config_yaml_path=test_pipeline_config["config_file"]
+        )
+
         # Pipeline should handle empty source list gracefully
         with patch.object(pipeline, 'run') as mock_run:
             mock_run.return_value = None
             pipeline.run()
-    
+
     @pytest.mark.e2e
     @patch('etl.pipeline.HANDLER_MAP')
     @patch('etl.pipeline.Source.load_all')
@@ -151,38 +152,38 @@
         mock_source.type = "rest_api"
         mock_source.enabled = True
         mock_load_all.return_value = [mock_source]
-        
+
         # Mock handler
         mock_handler_class = Mock()
         mock_handler_instance = Mock()
         mock_handler_class.return_value = mock_handler_instance
         mock_handler_map.__getitem__ = Mock(return_value=mock_handler_class)
-        
-        pipeline = Pipeline(
-            sources_yaml=test_pipeline_config["sources_file"],
-            config_yaml_path=test_pipeline_config["config_file"]
-        )
-        
+
+        pipeline = Pipeline(
+            sources_yaml=test_pipeline_config["sources_file"],
+            config_yaml_path=test_pipeline_config["config_file"]
+        )
+
         # Verify pipeline can be created
         assert pipeline is not None
-    
+
     @pytest.mark.e2e
     def test_pipeline_invalid_yaml_handling(self, temp_dir):
         """Test pipeline behavior with invalid YAML files."""
         invalid_sources_file = temp_dir / "invalid_sources.yaml"
         invalid_sources_file.write_text("sources: [\ninvalid yaml")
-        
+
         valid_config_file = temp_dir / "valid_config.yaml"
         valid_config_file.write_text("logging:\n  level: INFO")
-        
+
         # Should not crash during initialization
         pipeline = Pipeline(
             sources_yaml=invalid_sources_file,
             config_yaml_path=valid_config_file
         )
-        
+
         assert pipeline is not None
-    
+
     @pytest.mark.e2e
     @patch('etl.pipeline.ArcPyFileGDBLoader')
     @patch('etl.pipeline.geoprocess')
@@ -192,12 +193,12 @@
             sources_yaml=test_pipeline_config["sources_file"],
             config_yaml_path=test_pipeline_config["config_file"]
         )
-        
+
         # Verify components are accessible
         assert hasattr(pipeline, 'handler_map')
         assert hasattr(pipeline, 'global_cfg')
         assert hasattr(pipeline, 'summary')
-    
+
     @pytest.mark.e2e
     def test_pipeline_logging_configuration(self, test_pipeline_config, caplog):
         """Test that pipeline respects logging configuration."""
@@ -205,11 +206,11 @@
             sources_yaml=test_pipeline_config["sources_file"],
             config_yaml_path=test_pipeline_config["config_file"]
         )
-        
+
         # Pipeline should log initialization messages
         # Note: This test may need adjustment based on actual logging behavior
         assert pipeline is not None
-    
+
     @pytest.mark.e2e
     def test_pipeline_summary_integration(self, test_pipeline_config):
         """Test pipeline integration with summary reporting."""
@@ -219,10 +220,10 @@
             config_yaml_path=test_pipeline_config["config_file"],
             summary=summary
         )
-        
+
         # Verify summary is properly integrated
         assert pipeline.summary is summary
-        
+
         # Summary should be usable
         summary.add_success("Test", "Test success message")
-        assert len(summary.successes) > 0
\ No newline at end of file
+        assert len(summary.successes) > 0
--- original/./tests/e2e/test_full_workflow.py
+++ fixed/./tests/e2e/test_full_workflow.py
@@ -12,29 +12,29 @@
 
 class TestFullWorkflow:
     """Test complete ETL workflow scenarios."""
-    
+
     @pytest.fixture
     def mock_arcpy_environment(self):
         """Set up comprehensive ArcPy mocking for full workflow tests."""
         with patch('arcpy.env') as mock_env, \
-             patch('arcpy.Describe') as mock_describe, \
-             patch('arcpy.management') as mock_mgmt:
-            
+                patch('arcpy.Describe') as mock_describe, \
+                patch('arcpy.management') as mock_mgmt:
+
             # Configure ArcPy environment
             mock_env.workspace = ""
             mock_env.overwriteOutput = True
-            
+
             # Configure Describe mock
             mock_spatial_ref = Mock()
             mock_spatial_ref.factoryCode = 4326
             mock_describe.return_value.spatialReference = mock_spatial_ref
-            
+
             yield {
                 'env': mock_env,
                 'describe': mock_describe,
                 'management': mock_mgmt
             }
-    
+
     @pytest.fixture
     def complete_test_config(self, temp_dir):
         """Create complete test configuration simulating real workflow."""
@@ -69,7 +69,7 @@
                 }
             ]
         }
-        
+
         global_config = {
             "logging": {
                 "level": "INFO",
@@ -93,16 +93,16 @@
                 "memory_limit_mb": 512
             }
         }
-        
+
         sources_file = temp_dir / "workflow_sources.yaml"
         config_file = temp_dir / "workflow_config.yaml"
-        
+
         with sources_file.open("w") as f:
             yaml.dump(sources_config, f)
-        
+
         with config_file.open("w") as f:
             yaml.dump(global_config, f)
-        
+
         return {
             "sources_file": sources_file,
             "config_file": config_file,
@@ -110,34 +110,34 @@
             "sources_config": sources_config,
             "global_config": global_config
         }
-    
+
     @pytest.mark.e2e
     @pytest.mark.slow
     def test_complete_workflow_initialization(self, complete_test_config, mock_arcpy_environment):
         """Test complete workflow initialization with realistic configuration."""
         summary = Summary()
-        
+
         pipeline = Pipeline(
             sources_yaml=complete_test_config["sources_file"],
             config_yaml_path=complete_test_config["config_file"],
             summary=summary
         )
-        
+
         # Verify pipeline is properly configured
         assert pipeline.sources_yaml_path == complete_test_config["sources_file"]
         assert "logging" in pipeline.global_cfg
         assert "retry" in pipeline.global_cfg
         assert "paths" in pipeline.global_cfg
         assert isinstance(pipeline.summary, Summary)
-    
+
     @pytest.mark.e2e
     @pytest.mark.slow
     @patch('etl.handlers.rest_api.requests')
     @patch('etl.handlers.file.download')
     @patch('etl.pipeline.ensure_dirs')
-    def test_workflow_with_mocked_external_services(self, mock_ensure_dirs, mock_download, 
-                                                   mock_requests, complete_test_config, 
-                                                   mock_arcpy_environment):
+    def test_workflow_with_mocked_external_services(self, mock_ensure_dirs, mock_download,
+                                                    mock_requests, complete_test_config,
+                                                    mock_arcpy_environment):
         """Test workflow with external services mocked."""
         # Mock HTTP responses
         mock_response = Mock()
@@ -152,38 +152,40 @@
             ]
         }
         mock_requests.get.return_value = mock_response
-        
+
         # Mock file download
-        mock_download.return_value = complete_test_config["temp_dir"] / "downloaded.zip"
-        
+        mock_download.return_value = complete_test_config["temp_dir"] / \
+            "downloaded.zip"
+
         pipeline = Pipeline(
             sources_yaml=complete_test_config["sources_file"],
             config_yaml_path=complete_test_config["config_file"]
         )
-        
+
         # Verify pipeline can handle mocked services
         assert pipeline is not None
         assert len(pipeline.handler_map) > 0
-    
+
     @pytest.mark.e2e
     def test_workflow_error_handling_scenarios(self, complete_test_config, mock_arcpy_environment):
         """Test workflow behavior under various error conditions."""
         # Test with corrupted sources file
-        corrupted_sources = complete_test_config["temp_dir"] / "corrupted_sources.yaml"
+        corrupted_sources = complete_test_config["temp_dir"] / \
+            "corrupted_sources.yaml"
         corrupted_sources.write_text("sources: invalid yaml [\n")
-        
+
         # Should handle gracefully
         pipeline = Pipeline(
             sources_yaml=corrupted_sources,
             config_yaml_path=complete_test_config["config_file"]
         )
-        
-        assert pipeline is not None
-    
+
+        assert pipeline is not None
+
     @pytest.mark.e2e
     @patch('etl.pipeline.Source.load_all')
-    def test_workflow_with_mixed_source_types(self, mock_load_all, complete_test_config, 
-                                            mock_arcpy_environment):
+    def test_workflow_with_mixed_source_types(self, mock_load_all, complete_test_config,
+                                              mock_arcpy_environment):
         """Test workflow handling different source types together."""
         # Create mock sources of different types
         mock_sources = [
@@ -193,44 +195,46 @@
             Mock(name="Atom Source", type="atom_feed", enabled=True)
         ]
         mock_load_all.return_value = mock_sources
-        
+
         pipeline = Pipeline(
             sources_yaml=complete_test_config["sources_file"],
             config_yaml_path=complete_test_config["config_file"]
         )
-        
+
         # Verify all handler types are available
         expected_handlers = ["rest_api", "file", "ogc_api", "atom_feed"]
         for handler_type in expected_handlers:
             assert handler_type in pipeline.handler_map
-    
+
     @pytest.mark.e2e
     def test_workflow_summary_generation(self, complete_test_config, mock_arcpy_environment):
         """Test complete workflow summary generation."""
         summary = Summary()
-        
+
         # Simulate workflow results
-        summary.add_success("KOMMUN", "Successfully processed municipality data")
-        summary.add_failure("MILJO", "Failed to download environmental data", "Network timeout")
+        summary.add_success(
+            "KOMMUN", "Successfully processed municipality data")
+        summary.add_failure(
+            "MILJO", "Failed to download environmental data", "Network timeout")
         summary.add_skip("SGU", "Source disabled in configuration")
-        
+
         pipeline = Pipeline(
             sources_yaml=complete_test_config["sources_file"],
             config_yaml_path=complete_test_config["config_file"],
             summary=summary
         )
-        
+
         # Verify summary contains expected data
         assert len(summary.successes) == 1
         assert len(summary.failures) == 1
         assert len(summary.skips) == 1
-        
+
         # Test summary output (would normally be written to log)
         summary_output = summary._generate_report()
         assert "KOMMUN" in summary_output
         assert "MILJO" in summary_output
         assert "SGU" in summary_output
-    
+
     @pytest.mark.e2e
     def test_workflow_configuration_validation(self, temp_dir, mock_arcpy_environment):
         """Test workflow with various configuration scenarios."""
@@ -239,11 +243,11 @@
         minimal_file = temp_dir / "minimal.yaml"
         with minimal_file.open("w") as f:
             yaml.dump(minimal_config, f)
-        
+
         # Should handle minimal config
         pipeline = Pipeline(sources_yaml=minimal_file)
         assert pipeline is not None
-        
+
         # Test configuration with unknown keys
         extended_config = {
             "sources": [],
@@ -253,29 +257,31 @@
         extended_file = temp_dir / "extended.yaml"
         with extended_file.open("w") as f:
             yaml.dump(extended_config, f)
-        
+
         # Should handle unknown configuration gracefully
         pipeline = Pipeline(sources_yaml=extended_file)
         assert pipeline is not None
-    
+
     @pytest.mark.e2e
     @pytest.mark.slow
     def test_workflow_resource_management(self, complete_test_config, mock_arcpy_environment):
         """Test workflow resource management and cleanup."""
         # Create directories that would be used
-        download_dir = Path(complete_test_config["global_config"]["paths"]["download"])
-        staging_dir = Path(complete_test_config["global_config"]["paths"]["staging"])
-        
+        download_dir = Path(
+            complete_test_config["global_config"]["paths"]["download"])
+        staging_dir = Path(
+            complete_test_config["global_config"]["paths"]["staging"])
+
         with patch('etl.pipeline.ensure_dirs') as mock_ensure_dirs:
             pipeline = Pipeline(
                 sources_yaml=complete_test_config["sources_file"],
                 config_yaml_path=complete_test_config["config_file"]
             )
-            
+
             # Verify directories would be created
             mock_ensure_dirs.assert_called()
-            
+
             # Verify configuration paths are accessible
             paths_config = pipeline.global_cfg.get("paths", {})
             assert "download" in paths_config
-            assert "staging" in paths_config
\ No newline at end of file
+            assert "staging" in paths_config
--- original/./scripts/create_sde_datasets.py
+++ fixed/./scripts/create_sde_datasets.py
@@ -96,11 +96,13 @@
 
         except arcpy.ExecuteError:
             log.error(
-                "‚ùå Failed to create dataset %s: %s", dataset_name, arcpy.GetMessages(2)
+                "‚ùå Failed to create dataset %s: %s", dataset_name, arcpy.GetMessages(
+                    2)
             )
             error_count += 1
         except Exception as exc:
-            log.error("‚ùå Unexpected error creating dataset %s: %s", dataset_name, exc)
+            log.error("‚ùå Unexpected error creating dataset %s: %s",
+                      dataset_name, exc)
             error_count += 1
 
     # Summary
--- original/./scripts/list_tree.py
+++ fixed/./scripts/list_tree.py
@@ -109,7 +109,8 @@
 
 # ‚îÄ‚îÄ CLI entry point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 def main() -> None:
-    parser = argparse.ArgumentParser(description="List workspace directory tree.")
+    parser = argparse.ArgumentParser(
+        description="List workspace directory tree.")
     parser.add_argument(
         "path",
         nargs="?",
--- original/./scripts/cleanup_downloads.py
+++ fixed/./scripts/cleanup_downloads.py
@@ -1,5 +1,6 @@
 import shutil
 import os
+
 
 def delete_directory_contents(directory_path):
     """Delete contents of a directory without removing the directory itself."""
@@ -15,6 +16,7 @@
     else:
         print(f"Directory does not exist: {directory_path}")
 
+
 def delete_directory(directory_path):
     """Delete a directory and its contents."""
     if os.path.exists(directory_path):
@@ -23,13 +25,15 @@
     else:
         print(f"Directory does not exist: {directory_path}")
 
+
 def main():
     # Define paths relative to the script's working directory
     clear_contents = [
-        os.path.join("data", "downloads"),  # Note: you mentioned downloads (plural)
+        # Note: you mentioned downloads (plural)
+        os.path.join("data", "downloads"),
         os.path.join("data", "staging")
     ]
-    
+
     delete_completely = [
         os.path.join("data", "staging.gdb")
     ]
@@ -37,10 +41,11 @@
     # Clear contents but keep directories
     for directory in clear_contents:
         delete_directory_contents(directory)
-    
+
     # Delete directories completely
     for directory in delete_completely:
         delete_directory(directory)
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
