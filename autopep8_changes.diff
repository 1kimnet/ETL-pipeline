--- original/./etl/pipeline.py
+++ fixed/./etl/pipeline.py
@@ -162,8 +162,10 @@
 
         # ---------- 0. PRE-PIPELINE CLEANUP -------------------------------
         # Clean downloads and staging folders for fresh data
-        cleanup_downloads = self.global_cfg.get("cleanup_downloads_before_run", True)
-        cleanup_staging = self.global_cfg.get("cleanup_staging_before_run", True)
+        cleanup_downloads = self.global_cfg.get(
+            "cleanup_downloads_before_run", True)
+        cleanup_staging = self.global_cfg.get(
+            "cleanup_staging_before_run", True)
 
         if cleanup_downloads or cleanup_staging:
             lg_sum.info("🧹 Starting pre-pipeline cleanup...")
@@ -185,8 +187,10 @@
 
         # Log concurrent download configuration
         if self.global_cfg.get("enable_concurrent_downloads", True):
-            rest_workers = self.global_cfg.get("concurrent_download_workers", 5)
-            ogc_workers = self.global_cfg.get("concurrent_collection_workers", 3)
+            rest_workers = self.global_cfg.get(
+                "concurrent_download_workers", 5)
+            ogc_workers = self.global_cfg.get(
+                "concurrent_collection_workers", 3)
             file_workers = self.global_cfg.get("concurrent_file_workers", 4)
             self.logger.info(
                 f"🚀 Concurrent downloads enabled: REST={rest_workers}, OGC={ogc_workers}, Files={file_workers} workers"
@@ -230,7 +234,8 @@
                             "source": src.name,
                             "type": src.type,
                             "concurrent": str(
-                                self.global_cfg.get("enable_concurrent_downloads", True)
+                                self.global_cfg.get(
+                                    "enable_concurrent_downloads", True)
                             ),
                         },
                     )
@@ -342,14 +347,16 @@
                     self.summary.log_staging("error")
                     self.summary.log_error("GDB loader", str(exc))
 
-                    self.logger.error(f"❌ GDB load failed and recovery failed: {exc}")
+                    self.logger.error(
+                        f"❌ GDB load failed and recovery failed: {exc}")
                     self.metrics.increment_counter("staging.error")
 
                     if not self.global_cfg.get("continue_on_failure", True):
                         self.monitor.end_run("failed")
                         raise
                     else:
-                        self.logger.warning("⚠️ Continuing despite staging failures")
+                        self.logger.warning(
+                            "⚠️ Continuing despite staging failures")
 
         # ---------- 3. GEOPROCESS staging.gdb IN-PLACE -------------------
         if staging_success or self.global_cfg.get("continue_on_failure", True):
@@ -417,7 +424,8 @@
         if not aoi_boundary.exists():
             self.logger.error(f"❌ AOI boundary not found: {aoi_boundary}")
             if not self.global_cfg.get("continue_on_failure", True):
-                raise FileNotFoundError(f"AOI boundary not found: {aoi_boundary}")
+                raise FileNotFoundError(
+                    f"AOI boundary not found: {aoi_boundary}")
             return
 
         try:
@@ -432,7 +440,8 @@
                 staging_gdb=Path(str(paths.GDB)),
                 aoi_fc=aoi_boundary,
                 target_srid=target_srid,
-                pp_factor=geoprocessing_config.get("parallel_processing_factor", "100"),
+                pp_factor=geoprocessing_config.get(
+                    "parallel_processing_factor", "100"),
             )
 
             geoprocessing_duration = time.time() - start_time
@@ -478,7 +487,8 @@
             self.logger.warning(f"⚠️ No feature classes found in {source_gdb}")
             return
 
-        self.logger.info(f"📋 Feature classes discovered: {len(all_feature_classes)}")
+        self.logger.info(
+            f"📋 Feature classes discovered: {len(all_feature_classes)}")
 
         # Check if parallel loading is enabled
         use_parallel = self.global_cfg.get("parallel_sde_loading", True)
@@ -534,7 +544,8 @@
                     self.logger.warning(
                         "⚠️ ParallelProcessor method not found, falling back to sequential"
                     )
-                    self._load_to_sde_sequential(feature_classes, sde_connection)
+                    self._load_to_sde_sequential(
+                        feature_classes, sde_connection)
                     return
 
         successful = sum(1 for r in results if r is not None)
@@ -605,7 +616,8 @@
         )
 
         # Get load strategy from config (default: truncate_and_load)
-        load_strategy = self.global_cfg.get("sde_load_strategy", "truncate_and_load")
+        load_strategy = self.global_cfg.get(
+            "sde_load_strategy", "truncate_and_load")
 
         try:
             # Check if target dataset exists in SDE
@@ -671,15 +683,19 @@
 
         if arcpy.Exists(target_path):
             if load_strategy == "truncate_and_load":
-                lg_sum.info(f"🗑️ Truncating existing FC: {dataset}\\{sde_fc_name}")
+                lg_sum.info(
+                    f"🗑️ Truncating existing FC: {dataset}\\{sde_fc_name}")
                 arcpy.management.TruncateTable(target_path)
-                lg_sum.info(f"📄 Loading fresh data to: {dataset}\\{sde_fc_name}")
+                lg_sum.info(
+                    f"📄 Loading fresh data to: {dataset}\\{sde_fc_name}")
                 arcpy.management.Append(
                     inputs=source_fc_path, target=target_path, schema_type="NO_TEST"
                 )
-                lg_sum.info(f"🚚→  {dataset}\\{sde_fc_name} (truncated + loaded)")
+                lg_sum.info(
+                    f"🚚→  {dataset}\\{sde_fc_name} (truncated + loaded)")
             elif load_strategy == "replace":
-                self.logger.info(f"🗑️ Deleting existing FC: {dataset}\\{sde_fc_name}")
+                self.logger.info(
+                    f"🗑️ Deleting existing FC: {dataset}\\{sde_fc_name}")
                 arcpy.management.Delete(target_path)
                 self.logger.info(
                     f"🆕 Creating replacement FC: {dataset}\\{sde_fc_name} ({record_count} records)"
@@ -691,7 +707,8 @@
                 )
 
                 duration = time.time() - start_time
-                self.metrics.record_timing("sde.replace.duration_ms", duration * 1000)
+                self.metrics.record_timing(
+                    "sde.replace.duration_ms", duration * 1000)
                 self.logger.info(
                     f"🚚→ Replaced: {dataset}\\{sde_fc_name} in {duration:.2f}s"
                 )
@@ -704,7 +721,8 @@
                 )
                 lg_sum.info(f"🚚→  {dataset}\\{sde_fc_name} (appended)")
             else:
-                self.logger.error(f"❌ Unknown sde_load_strategy: {load_strategy}")
+                self.logger.error(
+                    f"❌ Unknown sde_load_strategy: {load_strategy}")
         else:
             self.logger.info(
                 f"🆕 Creating new FC: {dataset}\\{sde_fc_name} ({record_count} records)"
@@ -717,7 +735,8 @@
             )
 
             duration = time.time() - start_time
-            self.metrics.record_timing("sde.create.duration_ms", duration * 1000)
+            self.metrics.record_timing(
+                "sde.create.duration_ms", duration * 1000)
             self.logger.info(
                 f"🚚→ Created: {dataset}\\{sde_fc_name} in {duration:.2f}s"
             )
--- original/./etl/utils/regression_detector.py
+++ fixed/./etl/utils/regression_detector.py
@@ -384,7 +384,8 @@
             "total_operations_monitored": len(self.performance_data),
             "baselines_established": len(self.baselines),
             "regressions_by_severity": {
-                severity: len([r for r in recent_regressions if r.severity == severity])
+                severity: len(
+                    [r for r in recent_regressions if r.severity == severity])
                 for severity in ["minor", "moderate", "major", "critical"]
             },
             "operation_analysis": operation_analysis,
--- original/./etl/utils/retry.py
+++ fixed/./etl/utils/retry.py
@@ -402,7 +402,8 @@
             "total_successes": self.total_successes,
             "total_failures": self.total_failures,
             "success_rate": (
-                self.total_successes / (self.total_successes + self.total_failures)
+                self.total_successes /
+                (self.total_successes + self.total_failures)
             )
             * 100
             if (self.total_successes + self.total_failures) > 0
--- original/./etl/utils/adaptive_tuning.py
+++ fixed/./etl/utils/adaptive_tuning.py
@@ -43,8 +43,8 @@
             current_metrics: PerformanceMetrics,
             threshold: float = 0.2) -> bool:
         """Check if current performance is degraded compared to baseline."""
-        duration_increase = (current_metrics.duration - \
-                             self.avg_duration) / self.avg_duration # type: ignore
+        duration_increase = (current_metrics.duration -
+                             self.avg_duration) / self.avg_duration  # type: ignore
         throughput_decrease = (
             self.avg_throughput - current_metrics.throughput_items_per_sec) / self.avg_throughput
 
--- original/./etl/handlers/rest_api.py
+++ fixed/./etl/handlers/rest_api.py
@@ -375,7 +375,8 @@
                 if layer_detail:
                     layer_name = layer_detail.get("name", f"layer_{lid_str}")
                     layers_to_iterate_final.append(
-                        {"id": lid_str, "name": layer_name, "metadata": layer_detail}
+                        {"id": lid_str, "name": layer_name,
+                            "metadata": layer_detail}
                     )
                 else:
                     log.warning(
@@ -427,7 +428,8 @@
             fs_layer_name = service_meta.get(
                 "name", f"feature_layer_{fs_layer_id_str}")
             layers_to_iterate_final.append(
-                {"id": fs_layer_id_str, "name": fs_layer_name, "metadata": service_meta}
+                {"id": fs_layer_id_str, "name": fs_layer_name,
+                    "metadata": service_meta}
             )
 
         if not layers_to_iterate_final:
@@ -453,7 +455,8 @@
             for layer_info_to_query in layers_to_iterate_final:
                 self._fetch_layer_data(
                     layer_info=layer_info_to_query,
-                    layer_metadata_from_service=layer_info_to_query.get("metadata"),
+                    layer_metadata_from_service=layer_info_to_query.get(
+                        "metadata"),
                 )
 
     def _fetch_layers_concurrent(
