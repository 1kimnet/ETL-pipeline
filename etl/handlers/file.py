# etl/handlers/file.py
from __future__ import annotations

import logging
import shutil
import zipfile
from pathlib import Path
from typing import Iterable, Optional, Dict, Any, List
from urllib.parse import unquote

from ..models import Source
from ..utils import download, ensure_dirs, extract_zip, paths
from ..utils.naming import sanitize_for_filename
from ..utils.http import fetch_true_filename_parts
from ..utils.retry import smart_retry, enhanced_retry_with_stats, FILE_OPERATION_RETRY_CONFIG
from ..utils.concurrent import get_file_downloader, ConcurrentResult
from ..utils.recovery import recoverable_operation, get_global_recovery_manager
from ..exceptions import (
    NetworkError,
    SystemError,
    DataError,
    ErrorContext
)

log = logging.getLogger(__name__)


class FileDownloadHandler:
    """
    Handles downloading various file types (ZIPs, direct GPKGs, etc.) and staging them.
    Determines 'true' filenames by checking Content-Disposition or unquoting URL parts.
    For ZIPs, extracts contents. For direct GPKGs, copies them to the staging location.
    """
    
    def __init__(self, src: Source, global_config: Optional[Dict[str, Any]] = None):
        self.src = src
        self.global_config = global_config or {}
        
        # Initialize retry configuration
        retry_config = self.global_config.get("retry", {})
        self.retry_config = RetryConfig(
            max_attempts=retry_config.get("max_attempts", 3),
            base_delay=retry_config.get("base_delay", 1.0),
            backoff_factor=retry_config.get("backoff_factor", 2.0),
            max_delay=retry_config.get("max_delay", 300.0)
        )
        
        ensure_dirs()

    def fetch(self) -> None:
        """
        Fetches data for the source.
        Differentiates between downloading a single file (e.g., a GPKG) vs. multiple files
        (e.g., a collection of ZIPs based on an 'include' list).
        """
        if not self.src.enabled:
            log.info("‚è≠Ô∏è Source '%s' is disabled, skipping fetch.", self.src.name)
            return

        if not self.src.url:
            raise ETLFileNotFoundError(
                f"Source '{self.src.name}' has no URL configured",
                source_name=self.src.name
            )

        try:
            is_single_direct_download_gpkg = (
                (self.src.download_format and self.src.download_format.lower() == "gpkg") or
                (self.src.staged_data_type and self.src.staged_data_type.lower() == "gpkg")
            )

            if self.src.include and not is_single_direct_download_gpkg:
                self._download_multiple_files()
            else:
                self._download_single_resource()
        except Exception as e:
            log.error("‚ùå Failed to fetch source '%s': %s", self.src.name, format_error_context(e) if hasattr(e, 'context') else str(e))
            raise

    def _download_multiple_files(self) -> None:
        """Handle multi-part downloads defined by src.include."""
        log.info(
            "üîó Multi-part download mode for '%s' (%d archives from base URL '%s')‚Ä¶",
            self.src.name,
            len(self.src.include),
            self.src.url,
        )

        file_stems = list(self._iter_included_file_stems())
        
        # Use concurrent downloads for multiple files
        if len(file_stems) > 1:
            self._download_files_concurrent(file_stems)
        else:
            # Single file - use original sequential approach
            for included_filename_stem in file_stems:
                self._download_single_file_stem(included_filename_stem)

    def _download_files_concurrent(self, file_stems: List[str]) -> None:
        """Download multiple files concurrently for improved performance."""
        log.info("üöÄ Starting concurrent download of %d files", len(file_stems))
        
        # Get concurrent downloader
        downloader = get_file_downloader()
        
        # Enable parallel processing based on configuration
        use_concurrent = self.global_config.get("enable_concurrent_downloads", True)
        max_workers = self.global_config.get("concurrent_file_workers", 4)
        
        if not use_concurrent:
            log.info("‚ö†Ô∏è Concurrent downloads disabled, falling back to sequential")
            for file_stem in file_stems:
                self._download_single_file_stem(file_stem)
            return
        
        # Update worker count if specified
        if max_workers != downloader.manager.max_workers:
            downloader.manager.max_workers = max_workers
        
        # Execute concurrent downloads
        results = downloader.download_files_concurrent(
            handler=self,
            file_stems=file_stems,
            fail_fast=self.global_config.get("fail_fast_downloads", False)
        )
        
        # Process results and log statistics
        successful_downloads = sum(1 for r in results if r.success)
        failed_downloads = len(results) - successful_downloads
        
        log.info("üèÅ Concurrent file downloads completed: %d successful, %d failed", 
                successful_downloads, failed_downloads)
        
        # Log any failures
        for result in results:
            if not result.success:
                file_name = result.metadata.get("task_name", "unknown")
                log.error("‚ùå File download failed: %s - %s", file_name, result.error)

    def _download_single_file_stem(self, included_filename_stem: str) -> None:
        """Download a single file stem (extracted from original loop)."""
        file_ext_from_format = ".zip"  # Default for collections
        if self.src.download_format:
            fmt = self.src.download_format.lower().lstrip(".")
            if fmt:
                file_ext_from_format = f".{fmt}"

        actual_filename_for_download_part = included_filename_stem + file_ext_from_format
        base_url = self.src.url.rstrip("/") + "/"
        download_url_for_part = base_url + actual_filename_for_download_part

        sanitized_included_stem = sanitize_for_filename(included_filename_stem)
        self._download_and_stage_one(
            download_url=download_url_for_part,
            explicit_local_filename_stem=sanitized_included_stem,
            explicit_local_file_ext=file_ext_from_format,
            staging_subdir_name_override=sanitized_included_stem,
        )

    def _download_single_resource(self) -> None:
        """Handle downloading a single resource from src.url."""
        log.info(
            "üîó Single resource download mode for '%s' from URL: %s",
            self.src.name,
            self.src.url,
        )

        true_stem_from_web, true_ext_from_web = fetch_true_filename_parts(self.src.url)
        consistent_local_stem = sanitize_for_filename(self.src.name)
        final_extension = true_ext_from_web

        if self.src.download_format:
            expected_ext_from_format = f".{self.src.download_format.lower().lstrip('.')}"
            if final_extension and expected_ext_from_format != final_extension:
                log.warning(
                    "For source '%s', download_format ('%s') differs from determined true extension ('%s'). "
                    "Using determined true extension: '%s'.",
                    self.src.name,
                    expected_ext_from_format,
                    final_extension,
                    final_extension,
                )
            elif not final_extension:
                log.info(
                    "Could not determine true extension for '%s', using download_format: '%s'.",
                    self.src.name,
                    expected_ext_from_format,
                )
                final_extension = expected_ext_from_format

        if not final_extension:
            # Try to infer from URL if all else fails
            path_ext = Path(unquote(self.src.url)).suffix.lower()
            if path_ext in [".zip", ".gpkg", ".geojson", ".json"]:
                final_extension = path_ext
                log.info(
                    "Could not determine true extension for '%s', inferred from URL path: '%s'.",
                    self.src.name,
                    final_extension,
                )
            else:
                final_extension = ".data"  # Ultimate fallback
                log.warning(
                    "Could not determine any extension for '%s', defaulting to '%s'.",
                    self.src.name,
                    final_extension,
                )

        self._download_and_stage_one(
            download_url=self.src.url,
            explicit_local_filename_stem=consistent_local_stem,
            explicit_local_file_ext=final_extension,
            staging_subdir_name_override=consistent_local_stem,
        )

    def _iter_included_file_stems(self) -> Iterable[str]:
        """Yields stems from the source's include list."""
        for stem in self.src.include:
            yield stem

    @smart_retry("file_download_and_stage")
    def _download_and_stage_one(
        self,
        download_url: str,
        explicit_local_filename_stem: str,
        explicit_local_file_ext: str,
        staging_subdir_name_override: str
    ) -> None:
        """
        Downloads a single file and stages it.
        Local download filename: explicit_local_filename_stem + explicit_local_file_ext.
        Staging subdirectory name: staging_subdir_name_override (sanitized).
        """
        local_download_filename = explicit_local_filename_stem + explicit_local_file_ext
        download_target_path = paths.DOWNLOADS / local_download_filename

        sanitized_staging_subdir_name = sanitize_for_filename(staging_subdir_name_override)
        final_staging_destination_dir = paths.STAGING / self.src.authority / sanitized_staging_subdir_name
        final_staging_destination_dir.mkdir(parents=True, exist_ok=True)

        log.debug("Attempting to download: %s \n    -> to local file: %s \n    -> staging dir: %s",
                 download_url, download_target_path.name, final_staging_destination_dir.relative_to(paths.ROOT))

        try:
            downloaded_file_path = download(download_url, download_target_path)
        except Exception as e:
            log.error("‚ùå Download failed for %s (Source: %s): %s", download_url, self.src.name, e, exc_info=True)
            return

        effective_staged_data_type = self.src.staged_data_type or ""

        if not effective_staged_data_type:
            if self.src.download_format and self.src.download_format.lower() == "gpkg":
                effective_staged_data_type = "gpkg"
            elif explicit_local_file_ext.lower() == ".gpkg":
                effective_staged_data_type = "gpkg"
            elif explicit_local_file_ext.lower() in [".geojson", ".json"]:
                effective_staged_data_type = "geojson"
            else:  # Default to shapefile_collection for zips or unknown
                effective_staged_data_type = "shapefile_collection"

        log.info("Determined staged_data_type: '%s' for downloaded file '%s'",
                 effective_staged_data_type, downloaded_file_path.name)

        if effective_staged_data_type == "gpkg":
            staged_gpkg_filename = sanitized_staging_subdir_name + explicit_local_file_ext
            staged_gpkg_path = final_staging_destination_dir / staged_gpkg_filename
            try:
                if downloaded_file_path.resolve() != staged_gpkg_path.resolve():
                    shutil.copy(downloaded_file_path, staged_gpkg_path)
                    log.info("‚ûï Staged GPKG %s to %s",
                             downloaded_file_path.name,
                             staged_gpkg_path.relative_to(paths.ROOT))
                else:
                    log.info("‚ÑπÔ∏è Downloaded GPKG '%s' is already in the target staging location.", 
                            downloaded_file_path.name)
            except Exception as e:
                log.error("‚ùå Failed to copy downloaded GPKG %s to staging location %s: %s", 
                         downloaded_file_path.name, staged_gpkg_path, e, exc_info=True)
        
        elif effective_staged_data_type == "geojson":
            staged_json_filename = sanitized_staging_subdir_name + explicit_local_file_ext
            staged_json_path = final_staging_destination_dir / staged_json_filename
            try:
                if downloaded_file_path.resolve() != staged_json_path.resolve():
                    shutil.copy(downloaded_file_path, staged_json_path)
                    log.info("‚ûï Staged GeoJSON/JSON file %s to %s",
                             downloaded_file_path.name,
                             staged_json_path.relative_to(paths.ROOT))
                else:
                    log.info("‚ÑπÔ∏è Downloaded GeoJSON/JSON '%s' is already in the target staging location.", 
                            downloaded_file_path.name)
            except Exception as e:
                log.error("‚ùå Failed to copy downloaded GeoJSON/JSON %s to staging location %s: %s", 
                         downloaded_file_path.name, staged_json_path, e, exc_info=True)

        elif effective_staged_data_type == "shapefile_collection":
            if explicit_local_file_ext.lower() != ".zip":
                log.warning(
                    "‚ö†Ô∏è Expected a ZIP file for source '%s' (staged_data_type='shapefile_collection' or inferred) "
                    "but actual extension is '%s'. Attempting extraction anyway for '%s'.",
                    self.src.name, explicit_local_file_ext, downloaded_file_path.name
                )
            
            if explicit_local_file_ext.lower() == ".zip":
                try:
                    extract_zip(downloaded_file_path, final_staging_destination_dir)
                    log.info("‚ûï Extracted and staged archive %s to %s",
                             downloaded_file_path.name,
                             final_staging_destination_dir.relative_to(paths.ROOT))
                except zipfile.BadZipFile:
                    log.error("‚ùå File '%s' is not a valid ZIP file. Cannot extract for shapefile_collection.", 
                             downloaded_file_path.name)
                except Exception as e:
                    log.error("‚ùå Failed to extract archive %s to %s: %s", 
                             downloaded_file_path.name, final_staging_destination_dir, e, exc_info=True)

        else:
            log.warning(
                "ü§∑ Don't know how to stage data with effective_staged_data_type '%s' "
                "for source '%s'. File downloaded to %s.",
                effective_staged_data_type, self.src.name, downloaded_file_path
            )